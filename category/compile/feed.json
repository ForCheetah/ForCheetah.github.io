{
    "version": "https://jsonfeed.org/version/1",
    "title": "Пусть этот камень будет более крепким, чем человек • All posts by \"compile\" category",
    "description": "有自己的博客很帅，但是我很懒，要命！！！",
    "home_page_url": "https://forcheetah.github.io",
    "items": [
        {
            "id": "https://forcheetah.github.io/2025/02/05/aicompile04/",
            "url": "https://forcheetah.github.io/2025/02/05/aicompile04/",
            "title": "【AI编译】如何进行内存分配",
            "date_published": "2025-02-05T11:34:55.219Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本文讲解神经网络推理过程中的内存分配相关内容。</p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"tensor类型\"><a class=\"anchor\" href=\"#tensor类型\">#</a> Tensor 类型</h1>\n<p>不同的 tensor 有着不同的生命周期，神经网络推理过程中主要有三种 Tensor 类型：<br>\n1. 输入输出 tensor<br>\n 输出 tensor 是下一节点的输入 tensor，它们一体两面，这种类型的 tensor 生命周期起于 “生产节点”，终于最后一个 “消费节点”。<br>\n2. 权重 tensor<br>\n 权重 tensor 和算子绑定在一起，生命周期随着算子开始，也随算子结束。但也可能存在共享权重的情况，例如 tiling 操作产生的并行算子共享 Weight，<a href=\"https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem\">算能 TPU 选择将 weight 常驻内存</a>。<br>\n3. 中间 tensor<br>\n 中间 tensor 指的是算子计算时不得不开辟的临时空间，比如用来存储中间结果等，其大小和算子的具体实现有关。中间 tensor 的生命周期和权重 tensor 类似，随着算子开始，也随算子结束，但没有权重 tensor 的共享问题。</p>\n<h1 id=\"原地操作inplace-operation\"><a class=\"anchor\" href=\"#原地操作inplace-operation\">#</a> 原地操作（Inplace Operation）</h1>\n<p>原地覆盖不再需要的数据，不再开辟新的输出 tensor 空间。例如 Element-wise 算子、Relu 算子等可以在原地进行操作。</p>\n<p><img loading=\"lazy\" data-src=\"1738754981561.jpg\" alt=\"图 原地操作举例\"></p>\n<p>如图所示，B、C、D 三个 tensor 可以共用同一个内存空间。</p>\n<p><img loading=\"lazy\" data-src=\"1738755079647.jpg\" alt=\"图 原地操作举例2\"></p>\n<p>如图，当 B 的生命周期未结束时，C 不能共用 B 的内存空间。</p>\n<h1 id=\"内存共享memory-sharing\"><a class=\"anchor\" href=\"#内存共享memory-sharing\">#</a> 内存共享（Memory Sharing）</h1>\n<p>分析张量的生命周期，生命周期结束的张量及时释放，其所占据的空间可以被重复利用。拿算能 TPU 网站的图来举一个例子。如下图所示，这是一段简单的网络，有三个算子，分别是 Conv、Conv、和 Add。两个黑色的圆圈是权重，其余绿色的圆圈是输入输出 tensor 和中间 tensor。</p>\n<p><img loading=\"lazy\" data-src=\"1738755160616.jpg\" alt=\"图 算能TPU网络举例\"></p>\n<p>图源：<a href=\"https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem\">算能 TPU 网络举例</a></p>\n<p><img loading=\"lazy\" data-src=\"1738755172523.jpg\" alt=\"图 算能TPU网络举例2\"></p>\n<p>图源：<a href=\"https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem\">算能 TPU 内存生命周期</a></p>\n<p>上图展示了所有 tensor 的生命周期，三个算子的周期分别标记为 T2，T5 和 T7。<br>\n1.0 号 tensor 是首个 Conv 节点的输入 tensor，W1 是权重，所以 T2 开始 T2 结束，及时释放；<br>\n2.3 号 tensor 有两个消费节点，所以需要到 T7 才释放；<br>\n释放后的空间可以重复利用。</p>\n<h1 id=\"简单的分析规则\"><a class=\"anchor\" href=\"#简单的分析规则\">#</a> 简单的分析规则</h1>\n<p><img loading=\"lazy\" data-src=\"1738755220468.jpg\" alt=\"图 分析规则\"></p>\n<p>这里给出一个基本的 Tensor 生命周期分析流程，如图所示。可以通过这个流程来确定输入输出的生命周期。</p>\n<p>先初始化 tensor 的生命周期，它等于 tensor 的出度，也就是后面连着几个 “消费者节点”。<br>\n再按照执行顺序遍历网络结构，①如果当前节点的输入 Input 的生命周期大于 1，意味着它不能被释放，也不能被覆盖，需要为 Ouput 开辟新空间，同时 Input 生命周期减一；②如果当前节点的输入 Input 的生命周期等于 1，则进一步判断是否是 Inplace 节点；③如果不是 Inplace 节点，则仍需要为 Output 开辟空间，计算完成后 Input 可以释放；④是 Inplace 节点，则可以原地计算。</p>\n<h1 id=\"内存拼图游戏\"><a class=\"anchor\" href=\"#内存拼图游戏\">#</a> 内存拼图游戏</h1>\n<p>在完成网络的 Tensor 生命周期分析之后，就要尝试进行实际的内存分配，以确定设备内存是否足够放得下。建立一个坐标系，以横轴为时间 (Time)，纵轴为空间 (Addr)，把所有 tensor 作为一个图块拼进去，图块与图块之间不能有交叠，就像一个拼图游戏。<br>\n下图是算能 TPU 给出的内存分配例子，可供参考。不过这个图是以横轴为空间 (Addr)，纵轴为时间 (Time)。</p>\n<p><img loading=\"lazy\" data-src=\"1738755258225.jpg\" alt=\"图 算能TPU内存分配\"></p>\n<p>图源：<a href=\"https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem\">算能 TPU 内存分配</a></p>\n<p>这个拼图游戏并没有看起来那么容易，当网络结构复杂的时候，tensor 大小多样，拼图的方式也多样。不同的拼图方式可能直接决定内存分配的成功与失败。这里举个简单的例子，如图所示：</p>\n<p><img loading=\"lazy\" data-src=\"1738755278680.jpg\" alt=\"图 内存分配例子\"></p>\n<p>假设有三个内存块需要分配，他们所占空间和生命周期都已经确定。设备内存只有 20 大小。假如按照左图的拼图方式，会发现图块③没有空间存放，超出内存限制。而采用右图的拼图方式，内存分配就顺利实现。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "compile",
                "AI"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2025/01/15/aicompile03/",
            "url": "https://forcheetah.github.io/2025/01/15/aicompile03/",
            "title": "【AI编译】layer-group之后如何tiling",
            "date_published": "2025-01-15T13:12:53.246Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇讲解笔者实现 tiling 算法的一些经验。<br>\n前述文章 <a href=\"https://forcheetah.github.io/2025/01/14/aicompile02/\">《如何进行 layer-group》</a>讲解了 layer group 的内容。<a href=\"https://forcheetah.github.io/2024/10/18/aicompile01/\">《Tiling 操作能优化哪些时间》</a>提到 Tiling 的概念和作用。感兴趣的话可以阅读。</p>\n<p>本篇文章参考过 <a href=\"https://baijiahao.baidu.com/s?id=1787593090401250411&amp;wfr=spider&amp;for=pc\">《超强干货！地平线编译器大牛的编译优化实践总结》</a>，<a href=\"https://github.com/Arm-China/Compass_Optimizer\">《Arm 周易编译器工程》</a>，<a href=\"https://tpumlir.org/docs/developer_manual/10_layergroup.html#group\">《算能 TPU layer group 讲解》</a>，<a href=\"https://www.bilibili.com/video/BV1wo4y1z7AG/\">《算能 TPU 视频讲解》</a> 等文章和工程，欢迎大家参考。</p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"回顾\"><a class=\"anchor\" href=\"#回顾\">#</a> 回顾</h1>\n<p><img loading=\"lazy\" data-src=\"1736946396882.jpg\" alt=\"回顾流程\"></p>\n<p>如图所示，AI 编译优化的基本流程是 1. 图优化 (算子融合，常量折叠等) 2. 拆分 (layer group 和 tiling) 3. 并行和调度。最后得到当前编译的时间消耗。</p>\n<h1 id=\"分支结构tiling\"><a class=\"anchor\" href=\"#分支结构tiling\">#</a> 分支结构 Tiling</h1>\n<p><img loading=\"lazy\" data-src=\"1736946520498.jpg\" alt=\"分支结构\"></p>\n<p>假设现在对某个 layer group 做 tiling，需要对当前这个 layer group 按照 tiling 块数构建平行分支。</p>\n<p>上图来自<a href=\"https://tpumlir.org/docs/developer_manual/10_layergroup.html#group\">《算能 TPU layer group 讲解》</a>。如图所示，含有三个卷积算子的 layer group ，将其 tiling 成四块，则 group 的图结构变成右边的形状，图上也表明了 tiling 之后 tensor 的大小。可以使用 crop 算子完成 tensor 的切分，使用 concat 算子实现 tensor 的合并。图中没有画出 crop 算子和 concat 算子。</p>\n<p>神经网络结构复杂多样，分支结构、多前序节点、多后续节点等给 tiling 的实际操作带来很大困难。根据前述的 layer group 动态搜索划分，我们可能需要在各种各样的结构上进行 tiling 操作。下图是一种较为复杂的分支结构在 tiling 前后的对比图，可以给 tiling 结构的创建带来更直观的感受。</p>\n<p><img loading=\"lazy\" data-src=\"1736946566779.jpg\" alt=\"复杂分支结构tiling\"></p>\n<p>这是一个虚构的、较为复杂的分支结构。中间彩色的五个节点是当前要做 tiling 的 layer group，而上方和下方是当前 layer group 的前序节点和后续节点。<br>\n为方便，只 tiling 成两块。</p>\n<p>tiling 后的结构如图右侧所示，原有彩色算子部分复制成两份；layer group 有几个输出端（当前 group 有两个输出端：Conv 和 Fc）就有几个 concat 节点。layer group 的输入端（Add 节点和 Relu 节点）前面要添加 crop 节点，当输入端有多个前序节点时（Add 有两个前序节点），每个都要添加 crop。</p>\n<p>大致的结构重建就是这个样子。在实际操作中可能还有些不同，例如为保持所有节点都只能有一个输出 tensor 的原则，Crop 节点不能输出两个 tensor。</p>\n<h1 id=\"tiling维度\"><a class=\"anchor\" href=\"#tiling维度\">#</a> Tiling 维度</h1>\n<p>现有方案基本都选择在 N H W，也就是批次、高度、宽度这三个维度上做 tiling。</p>\n<p>N（批次）方向分块是最简单的方案，可以随意划分，对整个网络的运行没有影响。但是也许模型在推理的时候批次只有 1，无法 tiling。所以批次方向不是在所有情况下都适用。</p>\n<p>高度和宽度方向 tiling，比批次方向稍复杂，需要重新 infer shape 和调整参数（下一节讲解），但好在原有的算子推理仍然适用。</p>\n<p>基本上不考虑 C（通道）方向 tiling。通道方向分块，不仅要调整权重偏置 tensor，还需要在算子运算后加减 tensor，破坏了原有算子推理。<br>\n现有方案基本支持批次、高度、宽度这三个维度上做 tiling。</p>\n<h1 id=\"tiling逆向推理\"><a class=\"anchor\" href=\"#tiling逆向推理\">#</a> Tiling 逆向推理</h1>\n<p>上面只完成了 layer group 在 tiling 之后的结构重建。想要实现模型的正确推理，还需要更新好多信息。</p>\n<h2 id=\"逆向shape-推理\"><a class=\"anchor\" href=\"#逆向shape-推理\">#</a> 逆向 shape 推理</h2>\n<p>推理 shape，在 tensor 分块后，维度大小自然需要更新。下图是一个简单的分支结构的 tensor shape 推理例子。</p>\n<p><img loading=\"lazy\" data-src=\"1736946730182.jpg\" alt=\"逆向推理\"></p>\n<p>图①是推理之前的网络结构，仅包含两个卷积算子（均为 3*3，pad=1）。图上已经标明了每个算子的输入输出 tensor 的大小。现在将此 group 从 H（高）维度 tiling 成两份，也就是输出端（Add 节点）的输出 tensor 设置为 5*10。</p>\n<p>从输出端（Add 节点）开始逆序推导各个算子的输入输出 shape。如图②，普通算子如 Add，输入 tensor 的 shape 于输出 tensor 相同。而 Conv 这样的算子，需要根据参数、所在位置、输出 shape 来计算输入 tensor 的 shape。例如图②中 Conv_2 的输入变成 [6,10]，参数中 pad 需要修改为 [1,0,1,1]。</p>\n<p>现在出现一个问题：从 Add 节点直接往前推，Conv_1 的输出应该是 [5,10]，而走 Add-&gt;Conv_2 路径，Conv_1 的输出应该是 [6,10]。Conv_1 不可能同时输出 [6,10] 和 [5,10] 两个 tensor。如何处理当前的歧义呢？</p>\n<p>也许增加一个 crop 节点是个可选的方法。如图③所示，增加一个新的 Crop 节点，将 Conv_1 的输出 tensor 从 [6,10] 变成 [5,10]。</p>\n<h2 id=\"逆向深度优先搜索\"><a class=\"anchor\" href=\"#逆向深度优先搜索\">#</a> 逆向深度优先搜索</h2>\n<p>从逆向 shape 推理过程的分析来看，更新 layer group 的 shape，需要从输出端开始，逆向深度优先搜索每个分支，更新 shape 信息，还得判断采用哪些信息（如上一节中 Conv_1 的输出是 [6,10] 还是 [5,10]）。</p>\n<p>逆向深度优先搜索每个分支的意思如下图所示：从输出端搜索到输入端的每一条通路。下图结构中有四条通路。</p>\n<p><img loading=\"lazy\" data-src=\"1736946743244.jpg\" alt=\"深度优先搜索\"></p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "compile",
                "AI"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2025/01/14/aicompile02/",
            "url": "https://forcheetah.github.io/2025/01/14/aicompile02/",
            "title": "【AI编译】如何进行layer-group",
            "date_published": "2025-01-14T12:09:18.353Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇介绍 AI 编译领域 layer-group 算法。</p>\n<p>本篇文章参考过 <a href=\"https://baijiahao.baidu.com/s?id=1787593090401250411&amp;wfr=spider&amp;for=pc\">《超强干货！地平线编译器大牛的编译优化实践总结》</a>，<a href=\"https://github.com/Arm-China/Compass_Optimizer\">《Arm 周易编译器工程》</a>，<a href=\"https://tpumlir.org/docs/developer_manual/10_layergroup.html#group\">《算能 TPU layer group 讲解》</a>，<a href=\"https://www.bilibili.com/video/BV1wo4y1z7AG/\">《算能 TPU 视频讲解》</a> 等文章和工程，欢迎大家参考。</p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"layer-group\"><a class=\"anchor\" href=\"#layer-group\">#</a> Layer group</h1>\n<p>如图所示，AI 编译优化的基本流程是 1. 图优化 (算子融合，常量折叠等) 2. 拆分 (layer group 和 tiling) 3. 并行和调度。最后得到当前编译的时间消耗。</p>\n<p><img loading=\"lazy\" data-src=\"1736856461365.jpg\" alt=\"如图1所示\"></p>\n<p>在 AI 编译领域，LayerGroup 指的是将神经网络中的多个层（layers (Operator) ）组合成一个逻辑单元或模块的过程。</p>\n<p>一般而言设备的运行内存很大，比如电脑的运行内存 16GB，但是它的速度比较慢，我们把它叫做 Global Memory。而做神经网络推理的专用 NPU 芯片，它的高速缓存速度很快，但是空间可能只有几 MB，我们把它叫做 Cache。我们无法将网络模型所有 layer 全部加载到 cache 中，那么意味着每个算子都需要 Cache 和外部 Global Memory 的交互，load 输入数据、store 结果。</p>\n<p>所以需要将网络拆分成小的 layer group。一般默认只有进入和退出 layer group 的时候，才需要和外部的 Global Memory 做 Load/Store 操作去交互。把需要的数据 load 进来，将结果数据 store 出去。layer group 减少了 Load/Store 操作，同时 layer group 也是后续 tiling、调度等操作的基本单元，降低了问题复杂度。</p>\n<h1 id=\"动态规划搜索\"><a class=\"anchor\" href=\"#动态规划搜索\">#</a> 动态规划搜索</h1>\n<p><img loading=\"lazy\" data-src=\"1736856461365.jpg\" alt=\"如图1所示\"></p>\n<p>还是这张图，如何划分 layer group 呢？图里面只有四个算子，可以 1,2 划分一组，3,4 划分一组；也可以 1 划为一组，2,3,4 划分另一组；甚至 1,2,3,4 全部划分为一组。不同的划分方式，最后得到的 Cost 也不同。<br>\n为了找到最优的划分方式，不得不搜索所有划分方案。</p>\n<p>假设网络有 n 层，则 n 层中间有 n-1 个间隔。选取 0 个间隔，也就是 n 层全部划分为一组，是 C (n-1, 0)；选取 1 个间隔，也就是分成两组，是 C (n-1,1)；选取 2 个间隔，也就是划分三组，是 C (n-1,2)；以此类推， C (n-1, 0)+C (n-1,1)+C (n-1,2)+……+C (n-1,n-1)，根据二项式定理，需要 2^(n-1) 次搜索。</p>\n<p>指数式增长计算量太大，是不可接受的。</p>\n<p>可以采用动态规划的思想来减少搜索次数。</p>\n<p>我们以 100 层网络为例。设 <code>f(n)</code>  为从第 0 层到达第 n 层最短用时。令  <code>f(-1) = 0</code>  ， <code>cost(x,y)</code>  为从 x 层到 y 层作为一个 group 的开销。则：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token function\">min</span><span class=\"token punctuation\">(</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token function\">min</span><span class=\"token punctuation\">(</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">11</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">99</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token function\">min</span><span class=\"token punctuation\">(</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">99</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">99</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span> <span class=\"token punctuation\">;</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span><span class=\"token number\">98</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token function\">cost</span><span class=\"token punctuation\">(</span><span class=\"token number\">99</span><span class=\"token punctuation\">,</span><span class=\"token number\">99</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这就需要提前计算一个 cost table。</p>\n<p><img loading=\"lazy\" data-src=\"1736856527679.jpg\" alt=\"如图2所示\"></p>\n<p>cost table 如图所示。我们要做的就是以从 x 层到 y 层作为一个 group，计算开销 <code>cost(x,y)</code>  并保存下来。对于一个 n 层的网络来说，需要搜索的 layer group 数量为： 1+2+…+n = n (n+1)/2 。搜索次数从原来的 2^(n-1) 指数函数，已经降到幂函数。</p>\n<p>但是当网络层数过多时，搜索量还是很大，可以限制最大搜索长度，例如最长支持 50 层，那么 cost table 就变成下面这个样子：</p>\n<p><img loading=\"lazy\" data-src=\"1736856542354.jpg\" alt=\"如图3所示\"></p>\n<p>现在对于 n 层网络，搜索数量降到 50*n 。</p>\n<p>还可以启发式的剪枝优化，优化掉明显不会有收益的分支，进一步降低搜索的时间消耗。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！global_info)</p>\n<p><code>runtime_mod</code></p>\n",
            "tags": [
                "compile",
                "AI"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/10/18/aicompile01/",
            "url": "https://forcheetah.github.io/2024/10/18/aicompile01/",
            "title": "【AI编译】Tiling操作能优化什么时间",
            "date_published": "2024-10-18T13:44:56.351Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇讲解 Tiling 操作为什么能够优化神经网络推理。</p>\n<p>也可以参考 <a href=\"https://www.hiascend.com/developer/techArticles/20240920-1?envFlag=1\">《Ascend C 算子优化实用技巧 04——Tiling 优化》</a></p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"什么是tiling\"><a class=\"anchor\" href=\"#什么是tiling\">#</a> 什么是 tiling</h1>\n<p>无法完整的容纳算子的输入与输出，需要每次搬运一部分输入进行计算然后搬出，再搬运下一部分输入进行计算，直到得到完整的最终结果，这个数据切分、分块计算的过程称之为 Tiling，切分数据的算法称为 Tiling 算法或者 Tiling 策略。</p>\n<h1 id=\"tile算子和tiling的区别\"><a class=\"anchor\" href=\"#tile算子和tiling的区别\">#</a> tile 算子和 tiling 的区别</h1>\n<p>我们先问一问语言大模型两者的区别：</p>\n<h2 id=\"神经网络推理中的tile算子\"><a class=\"anchor\" href=\"#神经网络推理中的tile算子\">#</a> 神经网络推理中的 Tile 算子</h2>\n<p>在神经网络中，会发现 tile 作为一个节点算子出现。Tile 算子（或称为 Tiling 操作）是一种张量操作，它的功能是将输入张量沿着指定的维度重复一定次数。该算子需要指定两个参数：</p>\n<ul>\n<li>1.reps<br>\n（重复次数）：这是一个整数列表，定义了每个维度上的重复次数。列表的长度必须与输入张量的维度相匹配，或者至少与你想要扩展的那些维度相匹配。如果对于某个维度你不希望进行复制，可以设置为 1。</li>\n<li>2.axis<br>\n（轴 / 维度）：虽然某些框架可能不需要显式指定轴，因为它们可以通过 reps 的结构来推断，但有些情况下需要明确指出哪些维度应该被复制。</li>\n</ul>\n<p>例如，假设有一个形状为 (2, 3) 的二维张量，并且你想沿第一个维度（行方向）重复两次，沿第二个维度（列方向）重复三次，那么你可以使用 Tile 算子并设置 reps=[2, 3]。这样操作后，输出张量将会是一个形状为 (4, 9) 的新张量，其中原始张量的内容被按照指定的方式进行了复制。</p>\n<p>在不同的深度学习库中，Tile 算子的实现可能会有所不同。例如，在 TensorFlow 中，它是 tf.tile () 函数；而在 PyTorch 中，则对应的是 torch.tile () 或者 .repeat () 方法。每种实现都有其特定的语法和用法，但核心概念是一致的。</p>\n<h2 id=\"ai编译优化中的tiling操作\"><a class=\"anchor\" href=\"#ai编译优化中的tiling操作\">#</a> AI 编译优化中的 tiling 操作</h2>\n<p>在 AI 编译领域，特别是针对深度学习模型的优化过程中，“tiling”（平铺）操作是指一种将计算任务分解成更小、更易于管理的子任务的技术。这种技术通常用于提高计算效率和内存使用效率，尤其是在处理大规模数据集或高维度张量时。</p>\n<p>Tiling 的主要目的是：</p>\n<ul>\n<li>1. 减少内存访问开销：通过将大块数据划分为较小的 “瓦片”，可以将这些小块数据加载到高速缓存中，从而减少对外部存储器的访问次数。这有助于利用 CPU 或 GPU 的高速缓存来加速计算过程。</li>\n<li>2. 并行化处理：每个 “瓦片” 可以独立处理，这意味着它们可以在多核处理器上并行执行，或者在 GPU 等并行计算架构上高效地分发给不同的线程或流处理器。</li>\n<li>3. 更好地利用硬件资源：通过适当调整瓦片大小，可以确保计算单元能够被充分利用，同时避免因单个任务过大导致的资源浪费。</li>\n<li>4. 改善局部性：合理设置的瓦片尺寸可以帮助保持良好的空间局部性和时间局部性，使得数据在被处理前已经被预取到了更快的存储层次中。</li>\n<li>5. 降低峰值内存需求：对于某些运算来说，一次性加载整个输入可能需要大量的内存。通过 tilling，我们可以仅处理当前所需的那部分数据，从而降低了对系统内存的需求。<br>\n例如，在卷积神经网络 (CNN) 中，一个常见的做法是将图像分割成多个小块，并且只在这些小块上进行卷积运算，而不是在整个图像上进行。这样做不仅可以减少每次计算所需的数据量，还能促进更好的并行化。</li>\n</ul>\n<p>Tiling 策略的选择取决于具体的硬件特性以及所处理数据的特点。在实际应用中，通常需要根据目标平台和具体应用场景来精心设计合适的 tilling 方案。一些自动化的工具和框架如 TensorFlow XLA、TVM (Tensor Virtual Machine) 等提供了高级别的 API 来帮助开发者实现高效的 tilling 优化。</p>\n<h2 id=\"两个tile是完全不同的概念\"><a class=\"anchor\" href=\"#两个tile是完全不同的概念\">#</a> 两个 tile 是完全不同的概念</h2>\n<p>两个 “tile” 概念虽然名字相似，但它们在不同的上下文中使用，并且具有不同的含义和用途：</p>\n<p>1. 神经网络中的 Tile 算子：</p>\n<ul>\n<li>在这个上下文中，Tile 是指一种张量操作，它将输入张量沿着指定维度重复一定次数。</li>\n<li>它主要用于数据处理阶段，例如当你需要复制或扩展张量以匹配某些特定的尺寸要求时。</li>\n<li>例子包括 TensorFlow 中的 tf.tile () 函数或 PyTorch 中的.repeat () 方法。</li>\n</ul>\n<p>2.AI 编译领域的 Tiling（平铺）：</p>\n<ul>\n<li>这里的 Tiling 是一种优化技术，用于将计算任务分解成更小的、可管理的部分，以便于更好地利用硬件资源，如缓存和并行计算能力。</li>\n<li>它通常是在模型优化阶段使用的，目的是提高计算效率、减少内存访问开销、促进并行化处理等。</li>\n<li>Tiling 可以应用于各种类型的运算，比如卷积运算中将图像分割成小块来处理。</li>\n</ul>\n<p>总结来说，尽管两者都涉及到了 “复制” 或 “分块” 的概念，但是神经网络中的 Tile 算子更多地关注于数据结构的操作，而 AI 编译领域的 Tiling 则是一个优化策略，旨在提升程序执行的性能。这两个概念分别属于数据处理和性能优化的不同领域。</p>\n<h1 id=\"案例\"><a class=\"anchor\" href=\"#案例\">#</a> 案例</h1>\n<p>一台电脑的内存很大，现在主流配置 16GB，甚至 32GB，虽然空间很大，但是它的速度比较慢，我们把它叫做 Global Memory。</p>\n<p>专用的 NPU 芯片用来做神经网络推理，它的高速缓存速度很快，但是空间可能只有几 MB，我们把它叫做 Cache。</p>\n<p>为了获取更快的运算速度，tensor 需要加载到 Cache 中进行计算，但是当算子需要占用的空间超过 Cache 的空间时，需要不断的进行数据搬运，导致算子搬入或搬出数据变为算子整个运行过程的性能瓶颈。</p>\n<p><img loading=\"lazy\" data-src=\"1729258859529.jpg\" alt=\"AI\"></p>\n<p>如上图所示：</p>\n<p>假设现在有 300 个数，需要连续经过三个 add 算子进行加操作，分别是 <code>add_1</code> ， <code>add_2</code>  和 <code>add_3</code> ，最终仍然输出 300 个数。</p>\n<p>但是 Cache 只能够存放 100 个数。</p>\n<p>在没有 tiling 操作的情况下：计算 <code>add_1</code>  时，需要将 (0,100) 个数 load 到 Cache，计算完毕后，需要将这 (0,100) store 回 global memory，为下一百个数腾出空间【接下来的计算 Cache 未命中】；然后加载 load (100,200) 的数据，继续计算 <code>add_1</code> 。以此类推，在没有 tiling 操作的情况下，计算完 <code>add_1</code> ， <code>add_2</code>  和 <code>add_3</code>  需要 load 和 store 操作的数据都是 900。</p>\n<p>在 tiling 的情况下，会提前将数据分块，分成 (0,100)，(100,200) 和 (200,300)。加载 (0,100)，接连计算 <code>add_1</code> ， <code>add_2</code>  和 <code>add_3</code> 。计算 <code>add_2</code>  时发现 Cache 中的数据正是所需要的数据【Cache 命中】。计算流程如图所示，整个计算下来，load 和 store 操作的数据都是 300。</p>\n<p><img loading=\"lazy\" data-src=\"a.jpg\" alt=\"图片\"></p>\n<p>tiling 操作提高了 cache 的命中率，避免了频繁搬运带来的时间损耗。<br>\n从图上看，同一 group 中包含的超出 cache 算子越多，tiling 带来的收益越大。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "compile",
                "AI"
            ]
        }
    ]
}
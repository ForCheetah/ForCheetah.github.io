{
    "version": "https://jsonfeed.org/version/1",
    "title": "Пусть этот камень будет более крепким, чем человек • All posts by \"卷积加速\" category",
    "description": "有自己的博客很帅，但是我很懒，要命！！！",
    "home_page_url": "https://forcheetah.github.io",
    "items": [
        {
            "id": "https://forcheetah.github.io/2024/06/27/conv3/",
            "url": "https://forcheetah.github.io/2024/06/27/conv3/",
            "title": "【im2col】昇腾卷积加速算法",
            "date_published": "2024-06-27T15:21:34.031Z",
            "content_html": "<h1 id=\"前置信息\"><a class=\"anchor\" href=\"#前置信息\">#</a> 前置信息</h1>\n<p><strong>（1）本文讲解使用的例子</strong></p>\n<p>以如下的卷积为例，进行昇腾 Im2Col 卷积过程：</p>\n<ul>\n<li>Input 输入维度为 NHWC ：【2，25，25，17】</li>\n<li>外圈蓝色代表 pad</li>\n<li>Kernal 维度为  CCHkWk  ：【34，17，3，3】</li>\n<li>操作为 3*3 卷积 pad=1, Group=1, Stride=1， 2D 卷积</li>\n<li>得到输出的维度 为 NHWC : 【22，25，25，18】</li>\n</ul>\n<p>从图上可以轻易看出相关信息。</p>\n<p><img loading=\"lazy\" data-src=\"1719500181052.jpg\" alt=\"例子\"></p>\n<blockquote>\n<p>现在想起来，光是遇到你这个家伙，就感觉自己赚到了。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<p><strong>（2）矩阵乘运算单元</strong></p>\n<p>昇腾达芬奇架构设计了 16*16 的矩阵乘运算单元，能够提供强大的并行乘加计算能力，可以以一条指令实现两个 16*16 的矩阵相乘的运算。所以昇腾 Im2Col 卷积的目的就是让卷积能够高效地利用 “矩阵乘运算单元” 进行计算。</p>\n<p><img loading=\"lazy\" data-src=\"1719500301302.jpg\" alt=\"davincii\"></p>\n<p>感兴趣的可以阅读昇腾架构介绍书籍。</p>\n<blockquote>\n<p>矩阵计算单元可以⽤⼀条指令完成两个 16×16 矩阵的相乘运算（标记为 16<sup>3，也是 Cube 这⼀名称的来历），等同于在极短时间内进⾏了 16</sup>3＝4096 个乘加运算，并且可以实现 FP16 的运算精度。如图 3-7 所⽰，矩阵计算单元在完成 C＝A×B 的矩阵运算时，会事先将矩阵 A 按⾏存放在输⼊缓冲区中，同时将矩阵 B 按列存放在输⼊缓冲区中，通过矩阵计算单元计算后得到的结果矩阵 C 按⾏存放在输出缓冲区中。在矩阵相乘运算中，矩阵 C 的第⼀元素由矩阵 A 的第⼀⾏的 16 个元素和矩阵 B 的第⼀列的 16 个元素由矩阵计算单元⼦电路进⾏ 16 次乘法和 15 次加法运算得出。矩阵计算单元中共有 256 个矩阵计算⼦电路，可以由⼀条指令并⾏完成矩阵 C 的 256 个元素计算。                                          摘自《昇腾 AI 处理器架构与编程》</p>\n</blockquote>\n<h1 id=\"权重排布\"><a class=\"anchor\" href=\"#权重排布\">#</a> 权重排布</h1>\n<p>昇腾 Im2Col 五维卷积加速算法   基本流程：</p>\n<p>输入为 nhwc 输出为 nhwc</p>\n<p>权重维度变化： 权重的维度变化离线进行，不消耗神经网络推理时间。（神经网络推理大致分为 模型转换 量化 推理三个步骤，权重的维度转换可以在模型转换时进行，不占用推理的时间）。下面是权重变换的分步流程，代码实现可以一步完成，也可以分多步完成（因为不影响推理时间。）</p>\n<p><img loading=\"lazy\" data-src=\"1719500423112.jpg\" alt=\"weight change\"></p>\n<p>上方的变换如果比较抽象的话，可以结合后面的流程来理解。</p>\n<h2 id=\"权重-从kernel-4d变换到kernel-2d\"><a class=\"anchor\" href=\"#权重-从kernel-4d变换到kernel-2d\">#</a> 权重 从 kernel 4D 变换到 kernel 2D</h2>\n<p><img loading=\"lazy\" data-src=\"1719500512598.jpg\" alt=\"weight change\"></p>\n<p><img loading=\"lazy\" data-src=\"1719500556415.jpg\" alt=\"weight change2\"></p>\n<p>上图是 Kernel 2D 的数据排布方式，维度为【2*3*3*16，34】，为了简便，跳过昇腾 5D 结构，直接从 4D 转到 2D。下面介绍 4D 数据和 2D 数据的一一对应关系。</p>\n<ul>\n<li>D 图 ① 覆盖区域表示 一个卷积核【17，3，3】展开成 2D 中的一列。对应于 A 图中一整个卷积核。34 个卷积核将展开为 34 列。因此每列代表一个卷积核。</li>\n<li>B 图，卷积核通道数为 17，需要补零为 16 的倍数 32，并拆分成 2 块（分别是紫色、黄色）。E 图：每一列（每一个卷积核）的紫色部分②是卷积核通道方向拆分的第一块（B 图中的紫色），黄色部分③是拆分的第二块（B 图中的黄色）。</li>\n<li>拆分的每一块（比如紫色部分）又分成 3*3（kernel 行 * 列），F 图: ④覆盖的是 kernel 第一行 (对应于 C 图中的④的部分)，⑤覆盖的是 kernel 第二行（对应于 C 图中⑤的部分），相似的⑥覆盖的是 kernel 第三行（对应于 C 图中⑥的部分）。3*3 卷积核一共就三行</li>\n<li>每一个紫色的小方格代表通道方向的 16 个数。</li>\n<li>至此，kernel 4D 和 kernel 2D 所有的数据都一一对应了。例如 F 图中：⑦代表第 6 个卷积核、通道拆分的第二块、第一行、第二列、通道方向的 16 个数。</li>\n</ul>\n<p>通过上述对应关系，我们不难得到维度为【2*3*3*16，34】的卷积核 2D 形式。由于昇腾卷积算法的 AI 计算核心是 16*16 的矩阵乘运算单元，同时为了取数方便，还需要将卷积核 2D 转换为大 Z 小 N 排布方式。</p>\n<h2 id=\"权重-从kernel2d变换到大z小n\"><a class=\"anchor\" href=\"#权重-从kernel2d变换到大z小n\">#</a> 权重 从 kernel2D 变换到大 Z 小 N</h2>\n<p><img loading=\"lazy\" data-src=\"1719500684744.jpg\" alt=\"2d\"></p>\n<p>第一步，将 2D【2*3*3*16，34】中 34 补零为 16 的倍数，即 48，得到【2*3*3*16，48】。</p>\n<p>第二步，将其按照 16*16 的方格进行划分，得到【2*3*3，3】个【16，16】的小块。（图中画成了 4 个小块，实际应该是 3 个，示意图，见谅）</p>\n<p>第三步，将这些小块按照大 Z 小 N 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube8 这种 “Z” 字形排布；小 N 指的是内部按照列优先，即每个 16*16 的 Cube，先排第一列，然后是第二列...  详见最右边的彩色表示。</p>\n<p>多说一句，之所以专门按照 “小 N” 排布，是因为在矩阵运算中，权重作为矩阵乘的第二个参数，数据是按列取的。这就意味着在实际内存中要跳着取数（内存中都是按照行优先排序），自然效率低。提前将其按照列优先的方式进行排布，那么在矩阵乘运算中可以连续取数。至此，我们得到了 【2*3*3，3，16*16】的权重大 Z 小 N 排布形式，这种形式使得能够一次性取出 256 个数参与计算，效率很高。</p>\n<p>下面的代码一次性完成了 权重 4D nhwc  到权重大 Z 小 N 排布，仅供参考。还是那句话，权重的变换离线进行，不占用宝贵的推理时间，所以无须关心转换的效率。完整代码可以下载 <a href=\"https://github.com/ForCheetah/ConvAccelerate\">加速算法模拟</a>，并运行其中的  <code>TestAscendConvLayer();</code>  函数。可以看到三个测试函数，它们的区别在于不同的输入排布方式。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayer();</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token comment\">//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNCHW();</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token comment\">//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNHWC();</span></pre></td></tr></table></figure><figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">WeightTrans_A</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> filters<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> TensorDim weight_dim<span class=\"token punctuation\">,</span> Ascend5Dim we_5D_dim<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> we_tran5D<span class=\"token punctuation\">,</span> </pre></td></tr><tr><td data-num=\"2\"></td><td><pre>            AscendTransform5Dim we_tran5D_dim<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> CUBE_row<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> CUBE_col<span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token keyword\">int</span> lastdim4 <span class=\"token operator\">=</span> we_tran5D_dim<span class=\"token punctuation\">.</span>move <span class=\"token operator\">*</span> we_tran5D_dim<span class=\"token punctuation\">.</span>channel <span class=\"token operator\">*</span> we_tran5D_dim<span class=\"token punctuation\">.</span>LW <span class=\"token operator\">*</span> we_tran5D_dim<span class=\"token punctuation\">.</span>cube<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">int</span> lastdim3 <span class=\"token operator\">=</span> we_tran5D_dim<span class=\"token punctuation\">.</span>channel <span class=\"token operator\">*</span> we_tran5D_dim<span class=\"token punctuation\">.</span>LW <span class=\"token operator\">*</span> we_tran5D_dim<span class=\"token punctuation\">.</span>cube<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">int</span> lastdim2 <span class=\"token operator\">=</span> we_tran5D_dim<span class=\"token punctuation\">.</span>LW <span class=\"token operator\">*</span> we_tran5D_dim<span class=\"token punctuation\">.</span>cube<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token keyword\">int</span> single_filter_num <span class=\"token operator\">=</span> weight_dim<span class=\"token punctuation\">.</span>c <span class=\"token operator\">*</span> weight_dim<span class=\"token punctuation\">.</span>h <span class=\"token operator\">*</span> weight_dim<span class=\"token punctuation\">.</span>w<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token keyword\">int</span> single_filter_channel <span class=\"token operator\">=</span> weight_dim<span class=\"token punctuation\">.</span>h <span class=\"token operator\">*</span> weight_dim<span class=\"token punctuation\">.</span>w<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> ch_cube<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> ch_cube<span class=\"token operator\">&lt;</span>we_tran5D_dim<span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">;</span> ch_cube<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span>  <span class=\"token comment\">// 通道方向块   ch_cube</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>        <span class=\"token keyword\">int</span> index_1 <span class=\"token operator\">=</span> ch_cube <span class=\"token operator\">*</span> lastdim4<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>        <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> hk<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> hk<span class=\"token operator\">&lt;</span>we_tran5D_dim<span class=\"token punctuation\">.</span>move<span class=\"token punctuation\">;</span> hk<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span>  <span class=\"token comment\">//filter 长  </span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>            <span class=\"token keyword\">int</span> index_2 <span class=\"token operator\">=</span> index_1 <span class=\"token operator\">+</span> hk <span class=\"token operator\">*</span> lastdim3<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>            <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> wk<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> wk<span class=\"token operator\">&lt;</span>we_tran5D_dim<span class=\"token punctuation\">.</span>channel<span class=\"token punctuation\">;</span> wk<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span>  <span class=\"token comment\">//filter 宽</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>                <span class=\"token keyword\">int</span> index_3 <span class=\"token operator\">=</span> index_2 <span class=\"token operator\">+</span> wk <span class=\"token operator\">*</span> lastdim2<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>                <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> cout_cube<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> cout_cube<span class=\"token operator\">&lt;</span>we_tran5D_dim<span class=\"token punctuation\">.</span>LW<span class=\"token punctuation\">;</span> cout_cube<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span> <span class=\"token comment\">//cout 方向块 </span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>                    <span class=\"token keyword\">int</span> index_4 <span class=\"token operator\">=</span> index_3 <span class=\"token operator\">+</span> cout_cube<span class=\"token operator\">*</span>we_tran5D_dim<span class=\"token punctuation\">.</span>cube<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>                    <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> cube_row<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> cube_row<span class=\"token operator\">&lt;</span>CUBE_row<span class=\"token punctuation\">;</span> cube_row<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>                        <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> cube_col<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> cube_col<span class=\"token operator\">&lt;</span>CUBE_col<span class=\"token punctuation\">;</span> cube_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>                            <span class=\"token keyword\">int</span> index <span class=\"token operator\">=</span> index_4 <span class=\"token operator\">+</span> cube_row<span class=\"token operator\">*</span>CUBE_col <span class=\"token operator\">+</span> cube_col<span class=\"token punctuation\">;</span>                       </pre></td></tr><tr><td data-num=\"20\"></td><td><pre>                            <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>cout_cube<span class=\"token operator\">*</span>CUBE_col<span class=\"token operator\">+</span>cube_row<span class=\"token punctuation\">)</span><span class=\"token operator\">>=</span>weight_dim<span class=\"token punctuation\">.</span>n  <span class=\"token operator\">||</span> <span class=\"token punctuation\">(</span>ch_cube<span class=\"token operator\">*</span>CUBE_col<span class=\"token operator\">+</span>cube_col<span class=\"token punctuation\">)</span><span class=\"token operator\">>=</span>weight_dim<span class=\"token punctuation\">.</span>c<span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>                                we_tran5D<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span><span class=\"token keyword\">else</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>                                <span class=\"token comment\">// 第几个 filter  第几个通道  第几行  第几列  还要注意 大 Z 小 N 排布方式     大 Z 小 N 排布方式（行变列，列变行）</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>                                <span class=\"token keyword\">int</span> index_from <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>cout_cube<span class=\"token operator\">*</span>CUBE_col<span class=\"token operator\">+</span>cube_row<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>single_filter_num <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>ch_cube<span class=\"token operator\">*</span>CUBE_col<span class=\"token operator\">+</span>cube_col<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>single_filter_channel <span class=\"token operator\">+</span> hk<span class=\"token operator\">*</span>weight_dim<span class=\"token punctuation\">.</span>w<span class=\"token operator\">+</span> wk<span class=\"token punctuation\">;</span>                                </pre></td></tr><tr><td data-num=\"25\"></td><td><pre>                                we_tran5D<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> filters<span class=\"token punctuation\">[</span>index_from<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span>  </pre></td></tr><tr><td data-num=\"27\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre>                <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"30\"></td><td><pre>            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"31\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"32\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"33\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><h1 id=\"输入排布\"><a class=\"anchor\" href=\"#输入排布\">#</a> 输入排布</h1>\n<p>输入 tensor 的内存排布为 nhwc 输出为 nhwc</p>\n<p>昇腾算法的维度详细变换如图下图所示。这里展示了输入 input 从 4D 维度转换到 昇腾 5D 结构，然后再转换到 2D 结构，最后转换到大 Z 小 Z 维度。写这么详细只是为了方便读者理解，而在实际操作中，由于 Input 的变换是在线进行，消耗宝贵的推理时间，所以如华为昇腾书中所说：input 先是从 4D 维度 通过软件算法转换为 昇腾 5D 维度（在模型推理过程中这一步可能不需要，因为中间层的 tensor 已经处于昇腾 5D 维度了），之后从昇腾 5D 维度通过 硬件直接转换到大 Z 小 Z 排布（模型推理过程肯定是边转换变计算，所以不会将整个 tensor 转换为大 Z 小 Z 之后，才进行矩阵运算阶段的。本博客为方便，将整个 tensor 完全转换到大 Z 小 Z，再进行后面计算。）</p>\n<p>说完这些，就可以介绍一下昇腾算法极致高效的输入的排布转换过程了！</p>\n<p><img loading=\"lazy\" data-src=\"1719500920323.jpg\" alt=\"input\"></p>\n<h2 id=\"输入-从input-4d-到input-5d\"><a class=\"anchor\" href=\"#输入-从input-4d-到input-5d\">#</a> 输入 从 Input 4D 到 Input 5D</h2>\n<p>还是再强调一下，昇腾可以做到整个模型的中间层的 tensor 均保持昇腾 5D 的维度，所以思考一下，可能只有最初输入到模型的 tensor 需要 从 Input 4D 转 到 Input 5D，或者再数据预处理的时候就将数据处理为 5D 排布。</p>\n<p><img loading=\"lazy\" data-src=\"1719500976081.jpg\" alt=\"trans6\"></p>\n<ul>\n<li>G 图是最原始的 Input4D 结构，当然，batch 维度 N=2 没有画，只画了一个。它的维度是【25，25，17】</li>\n<li>H 图为昇腾 5D 结构图，首先要将通道方向的 17 补齐为 16 的倍数 32，同时每 16 个进行一次拆分，拆成两组。</li>\n<li>最后注意一下数据的排布顺序就好了：注意 5D 结构中，K_cube 位于最内层，这些数据是连续的，所以先把 高 h=1, 宽 w=1 位置的 16 个数据排在一起。</li>\n<li>紧接着将宽度方向 25 个 K_cube 排在一起，变成 25*16</li>\n<li>然后再遍历高的方向。变成 25*25*16</li>\n<li>最后是遍历两组，得到昇腾的 5D 结构【2，25，25，16】</li>\n</ul>\n<p>此处数据搬运较为简单，可以参考代码<a href=\"https://github.com/ForCheetah/ConvAccelerate\">加速算法模拟</a></p>\n<h2 id=\"输入-从input-5d-直接搬到-大z小z\"><a class=\"anchor\" href=\"#输入-从input-5d-直接搬到-大z小z\">#</a> 输入 从 Input 5D 直接搬到 大 Z 小 Z</h2>\n<p>昇腾通过专门设计的硬件，将 input 从 5D 格式直接搬到 大 Z 小 Z 排布。想要知道怎么搬以及为什么这么搬，还真不得不把其 2D 排布讲明白。  《昇腾 AI 处理器架构与编程》这本书中直接跳过了 2D 排布，导致晦涩难懂。</p>\n<h3 id=\"input-5d-到-input-2d\"><a class=\"anchor\" href=\"#input-5d-到-input-2d\">#</a> Input 5D 到 Input 2D</h3>\n<p>所以我们直接看 Input2D 与 Weight 2D 的对应情况，如下图所示。</p>\n<p><img loading=\"lazy\" data-src=\"1719501086578.jpg\" alt=\"trans5\"></p>\n<ul>\n<li>J 图为 input2D 【25*25，2*3*3*16】   K 图为 Weight2D 【2*3*3*16，34】。再回忆一下 Weight2D 数据每一行和每一列的数据的意义，它的一列数据 2*3*3*16 代表什么呢？  2*3*3*16 代表一整个卷积核，2 代表该卷积核通道方向拆成两块，那么 3*3*16 就是每一块的 高 * 宽 * K_cube。</li>\n<li>好巧！Input2D 的一行也是 2*3*3*16！（废话，不一样就没法算了）。既然 weight2D 一列数据的意义一清二楚，那么对应的 Input2D 数据一行的意义也就呼之欲出啦！ Input2D 的一行 就是卷积核在某个滑动窗口位置对应的 input 数据。例如，Input2D 的第一行，就对应于 I 图 3*3 的彩色窗口数据（没有 Pad 的情况下）。</li>\n<li>也就可以推知，Input2D 的每一行绿色部分，就是 I 图通道方向拆分的第一块（拆分的绿色部分）；每一行的的蓝色部分，就是 I 图通道防线拆分的第二块（中间深蓝宽度 1，和补齐的浅蓝 15）</li>\n<li>那么，为什么 Input2D 有足足 625 行呢？因为滑动窗口纵向滑动 25 次，每次纵向滑动，都包含横向的 25 次，总共 625 次。</li>\n</ul>\n<p>假如直接计算 Input2D 矩阵乘 Weight2D，卷积计算就得到最终结果啦！这就是普通的 Im2Col 算法，不清楚的小伙伴们还可以去读一下 <a href=\"https://forcheetah.github.io/2024/05/23/conv1/\">Im2Col 算法 NCHW</a> 和 <a href=\"https://forcheetah.github.io/2024/05/23/conv2/\">Im2Col 算法 NHWC</a>。</p>\n<p>从 2D 的角度来看，算法是不是很简单啊。</p>\n<p>不要高兴的太早，还没完呢。</p>\n<h3 id=\"input-2d-到-大z小z\"><a class=\"anchor\" href=\"#input-2d-到-大z小z\">#</a> Input 2D 到 大 Z 小 Z</h3>\n<p><img loading=\"lazy\" data-src=\"1719501234541.jpg\" alt=\"trans4\"></p>\n<p>接下来是将 Input2D 转换到大 Z 小 Z 排布</p>\n<p>第一步，将 Input2D【25*25，2*3*3*16】中 25*25 补零为 16 的倍数，即 640，得到【640，2*3*3*16】  ，如图 L。</p>\n<p>第二步，将其按照 16*16 的方格进行划分，即得到【40，18】个【16，16】的小块，如图 M。</p>\n<p>第三步，将这些小块按照大 Z 小 Z 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube720 这些块按照 “Z” 字形排布；像 N 图上方排成一行；小 Z 指的是内部也按照行优先，即每个 16*16 的 Cube，先排第一行，然后是第二行... 详见 N 图中的颜色表示。</p>\n<p><img loading=\"lazy\" data-src=\"1719501296597.jpg\" alt=\"trans3\"></p>\n<p>上图来自《昇腾 AI 处理器架构与编程》，矩阵 A 的排布为大 Z 小 Z，矩阵 B 的排布为大 Z 小 N，大家可以再理解一下。</p>\n<p>至此，Input 的大 Z 小 Z 排布已经实现，接下来就是 16*16 的矩阵乘了。</p>\n<p><img loading=\"lazy\" data-src=\"1719501324297.jpg\" alt=\"trans2\"></p>\n<ul>\n<li>Input 现在是【40，18】个【16，16】小块，如左图，当然，它现在处于大 Z 小 Z 的一维排布。</li>\n<li>Weight 现在是 【18，3】个【16，16】小块，如中间图，当然，它现在处于大 Z 小 N 的一维排布。</li>\n<li>不知道分块矩阵乘的小伙伴可以再搜索下 《线性代数》中的分块矩阵乘运算。</li>\n<li>内部，进行两个 16*16 块的矩阵乘运算，由于 weight 已经按照列优先进行排布，所以矩阵乘的顺序如上图最右边所示。</li>\n<li>外部，对【40，18】和【18，3】做矩阵乘运算。</li>\n<li>至此，我们得到了【640，18】的矩阵。</li>\n<li>然后将上图两图灰色部分对应的多余数据裁掉，就得到了卷积结果【25，25，34】 ，当然，还得遍历一下 batch，得到【2，25，25，34】</li>\n</ul>\n<h3 id=\"input5d搬到大z小z\"><a class=\"anchor\" href=\"#input5d搬到大z小z\">#</a> Input5D 搬到大 Z 小 Z</h3>\n<p>前两小节介绍了 Input5D 变换到 Input 2D，再变换到 大 Z 小 Z 的过程。而在昇腾芯片中，从 Input5D 到 Input2D 由硬件一步实现。</p>\n<p>如果前面两小节已经看明白了的话，那么搬运的秘密就呼之欲出了。</p>\n<p><img loading=\"lazy\" data-src=\"1719501381618.jpg\" alt=\"trans1\"></p>\n<ul>\n<li>看上图，左图是 Input 的 5D 维度排布【2，25，25，16】，右边是 Input 2D 排布【25*25，2*3*3*16】。中间是个滑动窗口示意图，3*3，因为本文中用的例子就是 3*3 卷积。</li>\n<li>回忆一下右边 2D 排布的数据的意义，每一个小格子是通道方向的 16 个数，每一行是滑动窗口每一个位置对应的 2*3*3*16 个数。滑动窗口纵向滑动 25 次，每次要横向滑动 25 次，所以有 625 行数据，再加上补齐的 15 行，才达到了 640 行数据。</li>\n<li>那么右图红色 1 的位置是滑动窗口 a 在第一个位置所对应的 16 个数字；红色 2 的位置是滑动窗口 a 横向滑动一次对应的 16 个数字；红色 3 的位置是滑动窗口 a 横向滑动第三次对应的 16 个数字；依次类推，红色 16 的位置是滑动窗口横向滑动第 16 次对应的 16 个数字。这 16 次滑动，滑动窗口的 a 在左图从 1 滑倒 16！</li>\n<li>也就是说，右图红色框的 1-16 与左图 1-16 一一对应！</li>\n<li>再来回忆一下，左图中 1-16 这 16*16 的数据是连续的吗？是！（不清楚的再回去看 Input 的维度变换）</li>\n<li>那么右图中的 1-16 这 16*16 个数据是连续的吗？它是！ 根据大 Z 小 Z 排布，这红色框中 16*16 的数据刚好被分到一个小 Cube 中！</li>\n<li>昇腾能够从 Input5D 中一次性拷贝 256 个数据到大 Z 小 Z 排布！</li>\n</ul>\n<h1 id=\"代码模拟\"><a class=\"anchor\" href=\"#代码模拟\">#</a> 代码模拟</h1>\n<p>当然，我猜测昇腾应该是设计了 16 个 DMA 组成的 DAM 队列，来实现一次 256 个数据的搬运。真的是相当高效了！</p>\n<p>我提供了 C 语言代码模拟整个昇腾的卷积运算流程。完整代码可以在 <a href=\"https://github.com/ForCheetah/ConvAccelerate\">加速算法模拟</a>下载，该工程提供了以下三个测试函数，它们的区别在于不同的输入排布方式。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayer();</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token comment\">//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNCHW();</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token comment\">//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNHWC();</span></pre></td></tr></table></figure><p>还要再提一句，该工程中采用 C 语言函数 memcpy () 来模拟昇腾的批量数据拷贝功能。数据搬运中并不是所有的情况都是 256 个数据内存连续的，所以可以看到代码运行中分两次、三次才能拷贝完 256 个数据的情况。昇腾硬件中设计的 DMA 队列不会出现这种问题。此外，硬件肯定设计为边搬运边计算的工作模式，不会像我工程中完全得到 Input 大 Z 小 Z 排布再进行矩阵运算。</p>\n<p>文章好长啊！画了好多图！</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "Linux",
                "openBlas"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/23/conv2/",
            "url": "https://forcheetah.github.io/2024/05/23/conv2/",
            "title": "【Im2Col】卷积加速算法【2】NHWC",
            "date_published": "2024-05-23T12:12:27.275Z",
            "content_html": "<p>本文为最基本的 Im2Col 算法的原理及实现。</p>\n<p><a href=\"https://forcheetah.github.io/2024/05/23/conv1/\">【Im2Col】卷积加速算法 NHWC</a> 【1】中已经讲了在输入和输出都是 nchw 排布下 Im2Col 算法的实现方式。常见的 tensor 输入有 NCHW 和 NHWC 两种内存排布方式，不同的排布方式各有优劣。排布方式不同，Im2Col 也有区别，本篇主要是在 NHWC 内存排布情况下的 Im2Col 算法原理和基本实现。</p>\n<blockquote>\n<p>慌乱的时候全是破绽，冷静下来，能够找到对方的破绽。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"两种内存排布\"><a class=\"anchor\" href=\"#两种内存排布\">#</a> 两种内存排布</h1>\n<p>卷积神经网络（CNN）的输入数据布局主要有两种标准：NCHW（通道、高度、宽度）和 NHWC（高度、宽度、通道）。主要深度学习框架对这两种布局的支持情况如下：</p>\n<p>PyTorch：主要采用 NCHW 格式。这是 PyTorch 在大多数情况下的默认布局，尤其是在涉及 GPU 计算时。</p>\n<p>Caffe：采用 NCHW 格式。Caffe 框架倾向于使用这种通道优先的布局。</p>\n<p>TensorFlow：默认使用 NHWC 格式，特别是在早期版本中，这是由于 TensorFlow 最初设计时主要针对 CPU 进行优化，NHWC 布局在这种场景下有更好的内存访问局部性。<br>\nKeras：Keras 本身是一个高级 API，集成到 Tensorflow 之后跟随 tensorflow 的内存排布方式。</p>\n<h1 id=\"内存排布的优缺点\"><a class=\"anchor\" href=\"#内存排布的优缺点\">#</a> 内存排布的优缺点</h1>\n<p><strong>NCHW 格式的优点：</strong></p>\n<ul>\n<li>\n<p>1. 在 GPU 中计算卷积时，比 NHWC 要快 2.5 倍左右。这是因为在 GPU 中，NCHW 格式的数据布局更符合 GPU 的内存访问模式和计算方式。</p>\n</li>\n<li>\n<p>2.NCHW 格式更适合那些需要对每个通道单独做运算的操作，比如 “MaxPooling”。这是因为 NCHW 格式的同一通道的像素值连续排布，使得对每个通道的数据可以更高效地进行运算。</p>\n</li>\n</ul>\n<p><strong>NCHW 格式的缺点：</strong></p>\n<ul>\n<li>\n<p>1.NCHW 格式需要把所有通道的数据都读取到，才能进行运算，因此在计算时需要的存储更多。这可能会限制其在一些具有限制的硬件环境下的应用。</p>\n</li>\n<li>\n<p>2.NCHW 格式的访存与计算的控制逻辑相对简单，这使得在一些需要精细控制访存和计算的场景下，可能不是最佳的选择。</p>\n</li>\n</ul>\n<p><strong>NHWC 格式的优点：</strong></p>\n<ul>\n<li>\n<p>1.NHWC 格式的访存局部性更好。这意味着每三个输入像素就可以得到一个输出像素，因此在一些特定的计算操作中，可以更高效地利用硬件资源。</p>\n</li>\n<li>\n<p>2.NHWC 格式更适合那些需要对不同通道的同一像素做某种运算的操作，比如 “Conv1x1”。这是因为 NHWC 格式的不同通道中的同一位置元素顺序存储，使得对不同通道的数据可以进行更高效的运算。</p>\n</li>\n<li>\n<p>3.NHWC 格式在早期的 CPU 开发中应用较多，因此对于主要基于 CPU 开发的深度学习框架和算法，NHWC 格式可能更受欢迎。</p>\n</li>\n</ul>\n<p><strong>NHWC 格式的缺点：</strong></p>\n<ul>\n<li>\n<p>1. 在使用 GPU 进行计算加速时，NHWC 格式不如 NCHW 格式高效。这是因为 NCHW 格式更符合 GPU 的内存访问模式和计算方式。</p>\n</li>\n<li>\n<p>2. 对于一些需要精细控制访存和计算的场景，NHWC 格式的控制逻辑可能相对复杂一些。</p>\n</li>\n</ul>\n<h1 id=\"im2col变换\"><a class=\"anchor\" href=\"#im2col变换\">#</a> Im2Col 变换</h1>\n<p>Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 input_2D 放在前面，也就是 (input_2D * kernel_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。</p>\n<p>依然采用下面这个简单的卷积样例，输入 tensor 按照 nhwc 排布，所以是一个 3 通道 5*5 的 input tensor。卷积核有 9 个，pad 为 1，因此输出是【1，5，5，9】。</p>\n<p><img loading=\"lazy\" data-src=\"1716464754440.jpg\" alt=\"卷积样例\"></p>\n<p>直接展开 2D 形式，两个矩阵形式如下。</p>\n<p><img loading=\"lazy\" data-src=\"1716464821580.jpg\" alt=\"Im2Col 2D示意图\"></p>\n<p><strong>上图右边是权重 kernel_2D</strong></p>\n<ul>\n<li>\n<p>1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序展成一列，也就是第一列绿色部分，共有 27 个数。</p>\n</li>\n<li>\n<p>2. 依次将余下 8 个 kernel 按照相同的方式展成一列，就得到了 kernel_2D。</p>\n</li>\n<li>\n<p>3.kernel_2D 维度为【27，9】</p>\n</li>\n</ul>\n<p>从内存排布上来看，需要将原本的 kernel_4D 进行数据重排。</p>\n<p><strong>上图左边是输入 input_2D</strong></p>\n<ul>\n<li>\n<p>1. 矩阵乘是 行 * 列；kernel_2D 一列代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一行是 “一个卷积核滑动窗口” 对应的数据，也就是 27 个。</p>\n</li>\n<li>\n<p>2. 第一行橘黄色部分是第一个滑动窗口对应的数据，未填的数代表 Pad。</p>\n</li>\n<li>\n<p>3. 滑动窗口需要纵移 5 次，每次纵移需要横移 5 次，因此有 5*5 行数据。</p>\n</li>\n<li>\n<p>4. 例如第 5 行蓝色是滑动窗口在图一中向右移动到第 5 格，蓝色格子时对应的数据。由于 Input_2D 的维度是 NHWC，也就是说图一中 “0，25，50” 三个数在内存中是相邻的（分别位于同一个 HW 位置的第一、第二、第三通道）。为了减少数据的搬运，这些连续的数被搬运到一起。即：图一中 3*3 的蓝色滑动窗口中前 3 个都对应 Pad，所以图二第 5 行蓝色行先有 3*3 (channel) 个 Pad 数据。图一蓝色滑动窗口在第 4 和第 5 个格子出现数据，所以图二 9 个 Pad 后面跟着 3，28，53；4，29，54。紧接着又出现了 Pad，以此类推。</p>\n</li>\n<li>\n<p>5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一行 27 个数据。input_2D 的维度为【25，27】</p>\n</li>\n<li>\n<p>6. 总结完 input_2D 的数据排布，那么 kernel_2D 每一列的数据具体怎么排也就清楚了，需要和 input_2D 的每一行一一对应。</p>\n</li>\n</ul>\n<p><strong>输出 output</strong></p>\n<p>input_2D【25，27】* kernel_2D 【27，9】得到结果 output_2D 【25，9】，刚好是输出 output【1，5，5，9】的内存排布方式。因此输出也不需要额外的内存转换。</p>\n<p><strong>具体实现</strong></p>\n<p>这里没有专门写 Im2Col 在 NHWC 排布情况下的代码，可以在卷积加速算法模拟下载完整的测试代码。并参考 “TestIm2FlavorConvLayer ();” 函数及文章<a href=\"https://forcheetah.github.io/2024/05/15/accelerate1/\"> im2col 卷积加速算法 NHWC</a>，完成 NHWC 内存排布情况下的算法。</p>\n<h1 id=\"矩阵乘先后顺序的影响\"><a class=\"anchor\" href=\"#矩阵乘先后顺序的影响\">#</a> 矩阵乘先后顺序的影响</h1>\n<p>以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。</p>\n<p><img loading=\"lazy\" data-src=\"1716466223424.jpg\" alt=\"两种排布情况下矩阵乘先后顺序不同对数据变换的影响\"></p>\n<p>已经讲解了简单的 im2col 算法在 NCHW 排布<a href=\"https://forcheetah.github.io/2024/05/15/accelerate1/\">上一篇文章</a>和 NHWC 排布情况下的 2D 内存排布情况。上图总结了两种排布情况下 矩阵乘先后顺序不同对数据变换的影响。</p>\n<ul>\n<li>NCHW 排布  kernel 在前  只需要对 input 做 im2col 变换</li>\n<li>NCHW 排布  input 在前  多出来对权重的转置变换 和输出的转置变换</li>\n<li>NHWC 排布  kernel 在前  需要对权重数据进行重排 以及输出进行转置</li>\n<li>NHWC 排布  input 在前  需要对权重进行转置</li>\n</ul>\n<p>当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/23/conv1/",
            "url": "https://forcheetah.github.io/2024/05/23/conv1/",
            "title": "【Im2Col】卷积加速算法【1】 NCHW",
            "date_published": "2024-05-23T11:31:49.363Z",
            "content_html": "<p>本文为最基本的 Im2Col 算法的原理及实现。<br>\n加速算法道阻且长，想要选择最优的算法，需要通盘考虑现实需求、软件算法、硬件支持，这就是 “坚持理论联系实际”。 所以这里只是对 Im2Col 算法最基本的原理探讨。<br>\n探索本就是由正确和错误交织而成，还望各位不吝赐教！</p>\n<blockquote>\n<p>现在想起来，光是遇到你这个家伙，就感觉自己赚到了。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"卷积算法\"><a class=\"anchor\" href=\"#卷积算法\">#</a> 卷积算法</h1>\n<p>将 Im2col 算法之前，不得不再提一下卷积。卷积是一种运算，在神经网络中是提取特征的过程，具体的操作过程是在输入特征中不断滑动卷积核大小的窗口，与卷积核做乘加运算，得到输出结果。</p>\n<p>先定义一下维度的符号： 卷积核 ：【C_out, C_in, Hk, Wk】   输入 ： 【B, C_in, H, W】  输出：【B, C_out, Ho, Wo】</p>\n<p><img loading=\"lazy\" data-src=\"1715770544973.jpg\" alt=\"卷积运算流程\"></p>\n<p>上图是维度非常小的一个卷积运算的图示，左边卷积核维度为【9，3，3，3】，即个数为 9，通道数为 3（个数和通道数均未在图中展示出来）的 3*3 卷积核。中间为 Input 的维度，周围方格蓝色代表 tensor 的 pad，中间黄色代表维度为【1，3，5，5】的 Input，即通道为 3，长宽 5*5。</p>\n<p>进行卷积过程中，卷积核（3*3，通道为 3）先横向滑动（5 次），再纵向滑动（5 次）；每到一个位置计算 kernel 与 Input 对应位置的乘积和，因此得到 output 5*5 的结果。9 个卷积核依次进行，得到输出大小【1，9，5，5】</p>\n<p>想要理解卷积乃至实现卷积加速算法，不仅要知道计算过程，还要格外关注数据在内存中的排布顺序。在内存中所有的数据都是一维存储的，例如 Input【1，3，5，5】，在内存中只有一个连续的、大小为 75 的数组，【1，3，5，5】只是它的逻辑维度；最后一个维度的 5 个数字是连续的（0~4），紧接着是下一行（5~9）......</p>\n<p>图中仅画出了平面的大小，通道方向就要靠大家的想象了，上图中每个格子后面还有 2 个格子（通道数为 3）。<br>\n简单的卷积更容易看清 Im2col 算法的流程，本文以及后续系列文章都将以这个例子进行。</p>\n<h1 id=\"im2col算法\"><a class=\"anchor\" href=\"#im2col算法\">#</a> Im2Col 算法</h1>\n<p>为什么要对卷积算法进行加速呢？</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> batch <span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> batch<span class=\"token operator\">&lt;</span>in_n<span class=\"token punctuation\">;</span> batch<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>        <span class=\"token keyword\">while</span> <span class=\"token punctuation\">(</span>s <span class=\"token operator\">&lt;</span> out_c<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> out_row <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> out_row <span class=\"token operator\">&lt;</span> out_h<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>out_row<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>                <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> out_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> out_col <span class=\"token operator\">&lt;</span> out_w<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>out_col<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>                    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> imap <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> imap <span class=\"token operator\">&lt;</span> in_c<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>imap<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kr <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kr <span class=\"token operator\">&lt;</span> ker_size<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>kr<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>                            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kc <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kc <span class=\"token operator\">&lt;</span> ker_size<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>kc<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>                                dosomething<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>                <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>上方是个典型的卷积算法，要实现一个卷积，不仅要遍历 batch、C_out、Ho、Wo，还要遍历整个卷积核的维度，整个循环达到了 7 层之多。当输入数据维度变大时，整个卷积占用的资源让人难以接受。提高神经网络的推理速度是推理引擎和 AI 芯片设计者不断的追求，而卷积又占了神经网络推理的大部分时间，因此卷积的加速是重中之重。<br>\nIm2Col 算法的原理就是将卷积运算<br>\n转化为矩阵运算，这一转换带来以下好处：</p>\n<ul>\n<li>\n<p>易于优化：由于 GEMM 操作在计算库中被广泛研究和优化，开发者可以利用这些库的最新进展提高效率。</p>\n</li>\n<li>\n<p>并行化和硬件支持：矩阵乘法天然适合并行处理；大多硬件平台都会充分利用其硬件资源，对矩阵乘法进行深度优化。通过使用如 CUDA、cuDNN、OpenBLAS 等库，可以实现并行计算，极大地加速卷积运算。</p>\n</li>\n<li>\n<p>内存管理灵活性：虽然 im2col 需要额外的内存来存储展开后的矩阵，但通过调整实现策略（如分块处理），可以在内存使用和计算效率之间找到平衡。</p>\n</li>\n</ul>\n<p>总之，尽管 Im2col 算法没有减少任何计算量，甚至还给内存管理带来挑战，但是矩阵的高效运算、硬件、并行等仍然能够在很多场景下提高卷积计算效率。</p>\n<h1 id=\"im2col变换\"><a class=\"anchor\" href=\"#im2col变换\">#</a> Im2Col 变换</h1>\n<p>Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 kernel_2D 放在前面，也就是 (kernel_2D * input_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。</p>\n<p>再看一遍下面这个简单的卷积样例，输入 tensor 按照 nchw 排布。</p>\n<p><img loading=\"lazy\" data-src=\"1715770588007.jpg\" alt=\"卷积样例\"></p>\n<p>直接展开 2D 形式，两个矩阵形式如下。</p>\n<p><img loading=\"lazy\" data-src=\"1715770625347.jpg\" alt=\"Im2Col 2D示意图\"></p>\n<p><strong>图 “Im2Col 2D 示意图” 左边是 kernel_2D</strong></p>\n<ul>\n<li>\n<p>1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序完全展平，也就是第一行绿色部分，共有 28 个数。</p>\n</li>\n<li>\n<p>2. 依次将余下 8 个 kernel 按照相同的方式展平，就得到了 kernel_2D。</p>\n</li>\n<li>\n<p>3.kernel_2D 维度为【9，27】</p>\n</li>\n</ul>\n<p>惊喜地发现，kernel_2D 的内存排布和 kernel_4D 完全一致，不需要任何内存搬运！</p>\n<p><strong>图 “Im2Col 2D 示意图” 右边是 Input_2D</strong></p>\n<ul>\n<li>\n<p>1. 矩阵乘是 行 * 列；kernel_2D 中，一行代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一列是 “一个卷积核滑动窗口” 对应的数据。</p>\n</li>\n<li>\n<p>2. 右图中，前半部分空数据代表 pad,  后面的大面积空白只是懒得填上数字。</p>\n</li>\n<li>\n<p>3. 例如，右图中蓝色的一列数据，代表图 2 中，卷积核滑动到蓝色窗口时对应的 input 数据。即第一个 channel：(pad,pad,pad;3,4,pad;8,9,pad)； 第二个 channel：(pad,pad,pad;28,29,pad;33,34,pad)；第三个 channel：(pad,pad,pad;53,54,pad;58,59,pad)</p>\n</li>\n<li>\n<p>4. 那么 input_2D 一行数据代表什么呢？代表 kernel 在 input 中窗口的横向移动和纵向移动。在本例子中，窗口需要纵向滑动 5 次，每次纵向滑动都要横向滑动 5 次，一共产生 25 次窗口滑动。</p>\n</li>\n<li>\n<p>5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一列 27 个数据。input_2D 的维度为【27，25】</p>\n</li>\n</ul>\n<p><strong>输出 output</strong></p>\n<p>kernel_2D 【9，27】* input_2D【27，25】得到结果 output_2D 【9，25】，刚好是输出 output【1，9，5，5】的内存排布方式。因此输出也不需要额外的内存转换。</p>\n<p><strong>具体实现</strong></p>\n<p>这里是将 input 由 nchw 转为 2D 排布的代码。如果感兴趣，可以在<a href=\"https://github.com/ForCheetah/ConvAccelerate\">卷积加速算法模拟</a> 下载完整的测试代码。并通过 “TestIm2FlavorConvLayer ();” 函数进行测试。同时函数 “TestIm2ColConvIMW” 是 NCHW 排布下 Input_2D 在前，kernel_2D 在后的 Im2Col 算法实现。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">Im2Col</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>data_im<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> channels<span class=\"token punctuation\">,</span><span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> height<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> width<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> kernel_h<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>        <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> kernel_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> pad_h<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> pad_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> stride_h<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> stride_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>data_col<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> output_h <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>height <span class=\"token operator\">+</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> pad_h <span class=\"token operator\">-</span> kernel_h <span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> stride_h <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> output_w <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>width <span class=\"token operator\">+</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> pad_w <span class=\"token operator\">-</span> kernel_w<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> stride_w <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> channel_size <span class=\"token operator\">=</span> height <span class=\"token operator\">*</span> width<span class=\"token punctuation\">;</span> </pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> channel <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> channel <span class=\"token operator\">&lt;</span> channels<span class=\"token punctuation\">;</span> channel<span class=\"token operator\">++</span><span class=\"token punctuation\">,</span> data_im <span class=\"token operator\">+=</span> channel_size<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kernel_row <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kernel_row <span class=\"token operator\">&lt;</span> kernel_h<span class=\"token punctuation\">;</span> kernel_row<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kernel_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kernel_col <span class=\"token operator\">&lt;</span> kernel_w<span class=\"token punctuation\">;</span> kernel_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>                <span class=\"token keyword\">int</span> input_row <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>pad_h <span class=\"token operator\">+</span> kernel_row<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>                <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_rows <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_rows<span class=\"token operator\">&lt;</span>output_h<span class=\"token punctuation\">;</span> output_rows<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>                    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">!</span><span class=\"token function\">is_a_ge_zero_and_a_lt_b</span><span class=\"token punctuation\">(</span>input_row<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_cols <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_cols<span class=\"token operator\">&lt;</span>output_w<span class=\"token punctuation\">;</span> output_cols<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>                            <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>                        <span class=\"token keyword\">int</span> input_col <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>pad_w <span class=\"token operator\">+</span> kernel_col<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_col<span class=\"token operator\">&lt;</span>output_w<span class=\"token punctuation\">;</span> output_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>                            <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token function\">is_a_ge_zero_and_a_lt_b</span><span class=\"token punctuation\">(</span>input_col<span class=\"token punctuation\">,</span> width<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>                                <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> data_im<span class=\"token punctuation\">[</span>input_row <span class=\"token operator\">*</span> width <span class=\"token operator\">+</span> input_col<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>                                <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>                            input_col <span class=\"token operator\">+=</span> stride_w<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>                    input_row <span class=\"token operator\">+=</span> stride_h<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"27\"></td><td><pre>                <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre>            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"30\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"31\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><h1 id=\"矩阵乘先后顺序的影响\"><a class=\"anchor\" href=\"#矩阵乘先后顺序的影响\">#</a> 矩阵乘先后顺序的影响</h1>\n<p>以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。</p>\n<p><img loading=\"lazy\" data-src=\"1715770640439.jpg\" alt=\"Kernel_2D * Input_2D\"><br>\n 在输入为 nchw 排布，输出也是 nchw 排布情况下，kernel_2D 在前，Input_2D 在后，只需要对 Input 进行 Im2Col 变换。</p>\n<p><img loading=\"lazy\" data-src=\"1715770657999.jpg\" alt=\"Input_2D * Kernel_2D\"><br>\n 在输入为 nchw 排布，输出也是 nchw 排布情况下，Input_2D 在前，kernel_2D 在后，多出了对 kernel 和 output 的转置操作。</p>\n<p>当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        }
    ]
}
<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Пусть этот камень будет более крепким, чем человек • Posts by &#34;卷积加速&#34; category</title>
        <link>https://forcheetah.github.io</link>
        <description>有自己的博客很帅，但是我很懒，要命！！！</description>
        <language>en</language>
        <pubDate>Thu, 23 May 2024 20:12:27 +0800</pubDate>
        <lastBuildDate>Thu, 23 May 2024 20:12:27 +0800</lastBuildDate>
        <category>bar</category>
        <category>baz</category>
        <category>Linux</category>
        <category>openBlas</category>
        <category>lib</category>
        <category>accelerate</category>
        <category>conv</category>
        <item>
            <guid isPermalink="true">https://forcheetah.github.io/2024/05/23/conv2/</guid>
            <title>【Im2Col】卷积加速算法【2】NHWC</title>
            <link>https://forcheetah.github.io/2024/05/23/conv2/</link>
            <category>accelerate</category>
            <category>conv</category>
            <pubDate>Thu, 23 May 2024 20:12:27 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;本文为最基本的 Im2Col 算法的原理及实现。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://forcheetah.github.io/2024/05/23/conv1/&#34;&gt;【Im2Col】卷积加速算法 NHWC&lt;/a&gt; 【1】中已经讲了在输入和输出都是 nchw 排布下 Im2Col 算法的实现方式。常见的 tensor 输入有 NCHW 和 NHWC 两种内存排布方式，不同的排布方式各有优劣。排布方式不同，Im2Col 也有区别，本篇主要是在 NHWC 内存排布情况下的 Im2Col 算法原理和基本实现。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;慌乱的时候全是破绽，冷静下来，能够找到对方的破绽。&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;两种内存排布&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#两种内存排布&#34;&gt;#&lt;/a&gt; 两种内存排布&lt;/h1&gt;
&lt;p&gt;卷积神经网络（CNN）的输入数据布局主要有两种标准：NCHW（通道、高度、宽度）和 NHWC（高度、宽度、通道）。主要深度学习框架对这两种布局的支持情况如下：&lt;/p&gt;
&lt;p&gt;PyTorch：主要采用 NCHW 格式。这是 PyTorch 在大多数情况下的默认布局，尤其是在涉及 GPU 计算时。&lt;/p&gt;
&lt;p&gt;Caffe：采用 NCHW 格式。Caffe 框架倾向于使用这种通道优先的布局。&lt;/p&gt;
&lt;p&gt;TensorFlow：默认使用 NHWC 格式，特别是在早期版本中，这是由于 TensorFlow 最初设计时主要针对 CPU 进行优化，NHWC 布局在这种场景下有更好的内存访问局部性。&lt;br&gt;
Keras：Keras 本身是一个高级 API，集成到 Tensorflow 之后跟随 tensorflow 的内存排布方式。&lt;/p&gt;
&lt;h1 id=&#34;内存排布的优缺点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#内存排布的优缺点&#34;&gt;#&lt;/a&gt; 内存排布的优缺点&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;NCHW 格式的优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 在 GPU 中计算卷积时，比 NHWC 要快 2.5 倍左右。这是因为在 GPU 中，NCHW 格式的数据布局更符合 GPU 的内存访问模式和计算方式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.NCHW 格式更适合那些需要对每个通道单独做运算的操作，比如 “MaxPooling”。这是因为 NCHW 格式的同一通道的像素值连续排布，使得对每个通道的数据可以更高效地进行运算。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NCHW 格式的缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1.NCHW 格式需要把所有通道的数据都读取到，才能进行运算，因此在计算时需要的存储更多。这可能会限制其在一些具有限制的硬件环境下的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.NCHW 格式的访存与计算的控制逻辑相对简单，这使得在一些需要精细控制访存和计算的场景下，可能不是最佳的选择。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NHWC 格式的优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1.NHWC 格式的访存局部性更好。这意味着每三个输入像素就可以得到一个输出像素，因此在一些特定的计算操作中，可以更高效地利用硬件资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.NHWC 格式更适合那些需要对不同通道的同一像素做某种运算的操作，比如 “Conv1x1”。这是因为 NHWC 格式的不同通道中的同一位置元素顺序存储，使得对不同通道的数据可以进行更高效的运算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.NHWC 格式在早期的 CPU 开发中应用较多，因此对于主要基于 CPU 开发的深度学习框架和算法，NHWC 格式可能更受欢迎。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NHWC 格式的缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 在使用 GPU 进行计算加速时，NHWC 格式不如 NCHW 格式高效。这是因为 NCHW 格式更符合 GPU 的内存访问模式和计算方式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 对于一些需要精细控制访存和计算的场景，NHWC 格式的控制逻辑可能相对复杂一些。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;im2col变换&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#im2col变换&#34;&gt;#&lt;/a&gt; Im2Col 变换&lt;/h1&gt;
&lt;p&gt;Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 input_2D 放在前面，也就是 (input_2D * kernel_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。&lt;/p&gt;
&lt;p&gt;依然采用下面这个简单的卷积样例，输入 tensor 按照 nhwc 排布，所以是一个 3 通道 5*5 的 input tensor。卷积核有 9 个，pad 为 1，因此输出是【1，5，5，9】。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1716464754440.jpg&#34; alt=&#34;卷积样例&#34;&gt;&lt;/p&gt;
&lt;p&gt;直接展开 2D 形式，两个矩阵形式如下。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1716464821580.jpg&#34; alt=&#34;Im2Col 2D示意图&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上图右边是权重 kernel_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序展成一列，也就是第一列绿色部分，共有 27 个数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 依次将余下 8 个 kernel 按照相同的方式展成一列，就得到了 kernel_2D。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.kernel_2D 维度为【27，9】&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从内存排布上来看，需要将原本的 kernel_4D 进行数据重排。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上图左边是输入 input_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 矩阵乘是 行 * 列；kernel_2D 一列代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一行是 “一个卷积核滑动窗口” 对应的数据，也就是 27 个。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 第一行橘黄色部分是第一个滑动窗口对应的数据，未填的数代表 Pad。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3. 滑动窗口需要纵移 5 次，每次纵移需要横移 5 次，因此有 5*5 行数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4. 例如第 5 行蓝色是滑动窗口在图一中向右移动到第 5 格，蓝色格子时对应的数据。由于 Input_2D 的维度是 NHWC，也就是说图一中 “0，25，50” 三个数在内存中是相邻的（分别位于同一个 HW 位置的第一、第二、第三通道）。为了减少数据的搬运，这些连续的数被搬运到一起。即：图一中 3*3 的蓝色滑动窗口中前 3 个都对应 Pad，所以图二第 5 行蓝色行先有 3*3 (channel) 个 Pad 数据。图一蓝色滑动窗口在第 4 和第 5 个格子出现数据，所以图二 9 个 Pad 后面跟着 3，28，53；4，29，54。紧接着又出现了 Pad，以此类推。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一行 27 个数据。input_2D 的维度为【25，27】&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;6. 总结完 input_2D 的数据排布，那么 kernel_2D 每一列的数据具体怎么排也就清楚了，需要和 input_2D 的每一行一一对应。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;输出 output&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;input_2D【25，27】* kernel_2D 【27，9】得到结果 output_2D 【25，9】，刚好是输出 output【1，5，5，9】的内存排布方式。因此输出也不需要额外的内存转换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具体实现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这里没有专门写 Im2Col 在 NHWC 排布情况下的代码，可以在卷积加速算法模拟下载完整的测试代码。并参考 “TestIm2FlavorConvLayer ();” 函数及文章&lt;a href=&#34;https://forcheetah.github.io/2024/05/15/accelerate1/&#34;&gt; im2col 卷积加速算法 NHWC&lt;/a&gt;，完成 NHWC 内存排布情况下的算法。&lt;/p&gt;
&lt;h1 id=&#34;矩阵乘先后顺序的影响&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#矩阵乘先后顺序的影响&#34;&gt;#&lt;/a&gt; 矩阵乘先后顺序的影响&lt;/h1&gt;
&lt;p&gt;以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1716466223424.jpg&#34; alt=&#34;两种排布情况下矩阵乘先后顺序不同对数据变换的影响&#34;&gt;&lt;/p&gt;
&lt;p&gt;已经讲解了简单的 im2col 算法在 NCHW 排布&lt;a href=&#34;https://forcheetah.github.io/2024/05/15/accelerate1/&#34;&gt;上一篇文章&lt;/a&gt;和 NHWC 排布情况下的 2D 内存排布情况。上图总结了两种排布情况下 矩阵乘先后顺序不同对数据变换的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NCHW 排布  kernel 在前  只需要对 input 做 im2col 变换&lt;/li&gt;
&lt;li&gt;NCHW 排布  input 在前  多出来对权重的转置变换 和输出的转置变换&lt;/li&gt;
&lt;li&gt;NHWC 排布  kernel 在前  需要对权重数据进行重排 以及输出进行转置&lt;/li&gt;
&lt;li&gt;NHWC 排布  input 在前  需要对权重进行转置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://forcheetah.github.io/2024/05/23/conv1/</guid>
            <title>【Im2Col】卷积加速算法【1】 NCHW</title>
            <link>https://forcheetah.github.io/2024/05/23/conv1/</link>
            <category>accelerate</category>
            <category>conv</category>
            <pubDate>Thu, 23 May 2024 19:31:49 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;本文为最基本的 Im2Col 算法的原理及实现。&lt;br&gt;
加速算法道阻且长，想要选择最优的算法，需要通盘考虑现实需求、软件算法、硬件支持，这就是 “坚持理论联系实际”。 所以这里只是对 Im2Col 算法最基本的原理探讨。&lt;br&gt;
探索本就是由正确和错误交织而成，还望各位不吝赐教！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;现在想起来，光是遇到你这个家伙，就感觉自己赚到了。&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;卷积算法&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#卷积算法&#34;&gt;#&lt;/a&gt; 卷积算法&lt;/h1&gt;
&lt;p&gt;将 Im2col 算法之前，不得不再提一下卷积。卷积是一种运算，在神经网络中是提取特征的过程，具体的操作过程是在输入特征中不断滑动卷积核大小的窗口，与卷积核做乘加运算，得到输出结果。&lt;/p&gt;
&lt;p&gt;先定义一下维度的符号： 卷积核 ：【C_out, C_in, Hk, Wk】   输入 ： 【B, C_in, H, W】  输出：【B, C_out, Ho, Wo】&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770544973.jpg&#34; alt=&#34;卷积运算流程&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是维度非常小的一个卷积运算的图示，左边卷积核维度为【9，3，3，3】，即个数为 9，通道数为 3（个数和通道数均未在图中展示出来）的 3*3 卷积核。中间为 Input 的维度，周围方格蓝色代表 tensor 的 pad，中间黄色代表维度为【1，3，5，5】的 Input，即通道为 3，长宽 5*5。&lt;/p&gt;
&lt;p&gt;进行卷积过程中，卷积核（3*3，通道为 3）先横向滑动（5 次），再纵向滑动（5 次）；每到一个位置计算 kernel 与 Input 对应位置的乘积和，因此得到 output 5*5 的结果。9 个卷积核依次进行，得到输出大小【1，9，5，5】&lt;/p&gt;
&lt;p&gt;想要理解卷积乃至实现卷积加速算法，不仅要知道计算过程，还要格外关注数据在内存中的排布顺序。在内存中所有的数据都是一维存储的，例如 Input【1，3，5，5】，在内存中只有一个连续的、大小为 75 的数组，【1，3，5，5】只是它的逻辑维度；最后一个维度的 5 个数字是连续的（0~4），紧接着是下一行（5~9）......&lt;/p&gt;
&lt;p&gt;图中仅画出了平面的大小，通道方向就要靠大家的想象了，上图中每个格子后面还有 2 个格子（通道数为 3）。&lt;br&gt;
简单的卷积更容易看清 Im2col 算法的流程，本文以及后续系列文章都将以这个例子进行。&lt;/p&gt;
&lt;h1 id=&#34;im2col算法&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#im2col算法&#34;&gt;#&lt;/a&gt; Im2Col 算法&lt;/h1&gt;
&lt;p&gt;为什么要对卷积算法进行加速呢？&lt;/p&gt;
&lt;figure class=&#34;highlight c&#34;&gt;&lt;figcaption data-lang=&#34;c&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; batch &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; batch&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;in_n&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; batch&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token keyword&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;s &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; out_c&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; out_row &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; out_row &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; out_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;out_row&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; out_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; out_col &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; out_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;out_col&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; imap &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; imap &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; in_c&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;imap&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kr &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kr &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; ker_size&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;kr&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kc &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kc &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; ker_size&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;kc&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                                dosomething&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;上方是个典型的卷积算法，要实现一个卷积，不仅要遍历 batch、C_out、Ho、Wo，还要遍历整个卷积核的维度，整个循环达到了 7 层之多。当输入数据维度变大时，整个卷积占用的资源让人难以接受。提高神经网络的推理速度是推理引擎和 AI 芯片设计者不断的追求，而卷积又占了神经网络推理的大部分时间，因此卷积的加速是重中之重。&lt;br&gt;
Im2Col 算法的原理就是将卷积运算&lt;br&gt;
转化为矩阵运算，这一转换带来以下好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;易于优化：由于 GEMM 操作在计算库中被广泛研究和优化，开发者可以利用这些库的最新进展提高效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;并行化和硬件支持：矩阵乘法天然适合并行处理；大多硬件平台都会充分利用其硬件资源，对矩阵乘法进行深度优化。通过使用如 CUDA、cuDNN、OpenBLAS 等库，可以实现并行计算，极大地加速卷积运算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;内存管理灵活性：虽然 im2col 需要额外的内存来存储展开后的矩阵，但通过调整实现策略（如分块处理），可以在内存使用和计算效率之间找到平衡。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总之，尽管 Im2col 算法没有减少任何计算量，甚至还给内存管理带来挑战，但是矩阵的高效运算、硬件、并行等仍然能够在很多场景下提高卷积计算效率。&lt;/p&gt;
&lt;h1 id=&#34;im2col变换&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#im2col变换&#34;&gt;#&lt;/a&gt; Im2Col 变换&lt;/h1&gt;
&lt;p&gt;Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 kernel_2D 放在前面，也就是 (kernel_2D * input_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。&lt;/p&gt;
&lt;p&gt;再看一遍下面这个简单的卷积样例，输入 tensor 按照 nchw 排布。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770588007.jpg&#34; alt=&#34;卷积样例&#34;&gt;&lt;/p&gt;
&lt;p&gt;直接展开 2D 形式，两个矩阵形式如下。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770625347.jpg&#34; alt=&#34;Im2Col 2D示意图&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图 “Im2Col 2D 示意图” 左边是 kernel_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序完全展平，也就是第一行绿色部分，共有 28 个数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 依次将余下 8 个 kernel 按照相同的方式展平，就得到了 kernel_2D。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.kernel_2D 维度为【9，27】&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;惊喜地发现，kernel_2D 的内存排布和 kernel_4D 完全一致，不需要任何内存搬运！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图 “Im2Col 2D 示意图” 右边是 Input_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 矩阵乘是 行 * 列；kernel_2D 中，一行代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一列是 “一个卷积核滑动窗口” 对应的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 右图中，前半部分空数据代表 pad,  后面的大面积空白只是懒得填上数字。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3. 例如，右图中蓝色的一列数据，代表图 2 中，卷积核滑动到蓝色窗口时对应的 input 数据。即第一个 channel：(pad,pad,pad;3,4,pad;8,9,pad)； 第二个 channel：(pad,pad,pad;28,29,pad;33,34,pad)；第三个 channel：(pad,pad,pad;53,54,pad;58,59,pad)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4. 那么 input_2D 一行数据代表什么呢？代表 kernel 在 input 中窗口的横向移动和纵向移动。在本例子中，窗口需要纵向滑动 5 次，每次纵向滑动都要横向滑动 5 次，一共产生 25 次窗口滑动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一列 27 个数据。input_2D 的维度为【27，25】&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;输出 output&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;kernel_2D 【9，27】* input_2D【27，25】得到结果 output_2D 【9，25】，刚好是输出 output【1，9，5，5】的内存排布方式。因此输出也不需要额外的内存转换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具体实现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这里是将 input 由 nchw 转为 2D 排布的代码。如果感兴趣，可以在&lt;a href=&#34;https://github.com/ForCheetah/ConvAccelerate&#34;&gt;卷积加速算法模拟&lt;/a&gt; 下载完整的测试代码。并通过 “TestIm2FlavorConvLayer ();” 函数进行测试。同时函数 “TestIm2ColConvIMW” 是 NCHW 排布下 Input_2D 在前，kernel_2D 在后的 Im2Col 算法实现。&lt;/p&gt;
&lt;figure class=&#34;highlight cpp&#34;&gt;&lt;figcaption data-lang=&#34;C++&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;Im2Col&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;data_im&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; channels&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; height&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; width&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_h&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_w&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; pad_h&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; pad_w&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; stride_h&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; stride_w&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;data_col&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_h &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;height &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; pad_h &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt; kernel_h &lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;/&lt;/span&gt; stride_h &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_w &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;width &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; pad_w &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt; kernel_w&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;/&lt;/span&gt; stride_w &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; channel_size &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; height &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; width&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; channel &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; channel &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; channels&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; channel&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; data_im &lt;span class=&#34;token operator&#34;&gt;+=&lt;/span&gt; channel_size&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_row &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_row &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; kernel_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_row&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_col &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; kernel_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; input_row &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;pad_h &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; kernel_row&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_rows &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_rows&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;output_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_rows&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;is_a_ge_zero_and_a_lt_b&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;input_row&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; height&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_cols &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_cols&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;output_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_cols&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;data_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;16&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; input_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;pad_w &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; kernel_col&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;17&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_col&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;output_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;18&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;is_a_ge_zero_and_a_lt_b&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;input_col&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; width&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;19&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                                &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;data_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; data_im&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;input_row &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; width &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; input_col&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;20&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;21&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                                &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;data_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;22&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;23&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            input_col &lt;span class=&#34;token operator&#34;&gt;+=&lt;/span&gt; stride_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;24&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;25&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;26&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    input_row &lt;span class=&#34;token operator&#34;&gt;+=&lt;/span&gt; stride_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;27&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;28&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;29&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;30&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;31&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&#34;矩阵乘先后顺序的影响&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#矩阵乘先后顺序的影响&#34;&gt;#&lt;/a&gt; 矩阵乘先后顺序的影响&lt;/h1&gt;
&lt;p&gt;以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770640439.jpg&#34; alt=&#34;Kernel_2D * Input_2D&#34;&gt;&lt;br&gt;
 在输入为 nchw 排布，输出也是 nchw 排布情况下，kernel_2D 在前，Input_2D 在后，只需要对 Input 进行 Im2Col 变换。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770657999.jpg&#34; alt=&#34;Input_2D * Kernel_2D&#34;&gt;&lt;br&gt;
 在输入为 nchw 排布，输出也是 nchw 排布情况下，Input_2D 在前，kernel_2D 在后，多出了对 kernel 和 output 的转置操作。&lt;/p&gt;
&lt;p&gt;当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>

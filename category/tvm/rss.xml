<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Пусть этот камень будет более крепким, чем человек • Posts by &#34;tvm&#34; category</title>
        <link>https://forcheetah.github.io</link>
        <description>有自己的博客很帅，但是我很懒，要命！！！</description>
        <language>en</language>
        <pubDate>Tue, 18 Jun 2024 21:01:39 +0800</pubDate>
        <lastBuildDate>Tue, 18 Jun 2024 21:01:39 +0800</lastBuildDate>
        <category>bar</category>
        <category>baz</category>
        <category>Linux</category>
        <category>openBlas</category>
        <category>lib</category>
        <category>accelerate</category>
        <category>conv</category>
        <category>tvm</category>
        <category>tengine</category>
        <category>ncnn</category>
        <category>cmake</category>
        <category>runtime</category>
        <category>tank</category>
        <category>zatan</category>
        <category>systemc</category>
        <item>
            <guid isPermalink="true">https://forcheetah.github.io/2024/06/18/deployTVMPython/</guid>
            <title>【TVM】Python脚本实现模型编译和保存</title>
            <link>https://forcheetah.github.io/2024/06/18/deployTVMPython/</link>
            <category>tvm</category>
            <category>cmake</category>
            <category>runtime</category>
            <pubDate>Tue, 18 Jun 2024 21:01:39 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;本篇博客提供简单的 Python 脚本代码，实现 onnx 模型转换编译，保存为 TVM 的  &lt;code&gt;.so .params .json&lt;/code&gt;  文件 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;望长城内外，惟余莽莽；大河上下，顿失滔滔。&lt;br&gt;
--------------- 教员&lt;br&gt;
 ------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;python脚本实现模型编译和保存&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#python脚本实现模型编译和保存&#34;&gt;#&lt;/a&gt; Python 脚本实现模型编译和保存&lt;/h1&gt;
&lt;p&gt;脚本中需要修改的就一些路径，很容易看明白，就不再过多介绍了。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;
import onnx
from tvm.contrib.download import download_testdata
from PIL import Image
import numpy as np
import tvm.relay as relay
import tvm
from tvm.contrib import graph_executor


# 图片
img_path = &amp;quot;../image/imagenet_cat.png&amp;quot;
# img_url = &amp;quot;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&amp;quot;
# img_path = download_testdata(img_url, &amp;quot;../image/imagenet_cat.png&amp;quot;, module=&amp;quot;data&amp;quot;)

# 重设大小为 224x224
resized_image = Image.open(img_path).resize((224, 224))
img_data = np.array(resized_image).astype(&amp;quot;float32&amp;quot;)

# 输入图像是 HWC 布局，而 ONNX 需要 CHW 输入，所以转换数组
img_data = np.transpose(img_data, (2, 0, 1))

# 根据 ImageNet 输入规范进行归一化
imagenet_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))
imagenet_stddev = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))
norm_img_data = (img_data / 255 - imagenet_mean) / imagenet_stddev


# 添加 batch 维度，期望 4 维输入：NCHW。
img_data = np.expand_dims(norm_img_data, axis=0)
# 保存为 bin 文件  
norm_img_data.astype(&amp;quot;float32&amp;quot;).tofile(&amp;quot;../image/imagenet_cat.bin&amp;quot;)


# 目标设备配置
target = &#39;llvm&#39;  # 以CPU为例

input_name = &amp;quot;data&amp;quot;
shape_dict = &amp;#123;input_name: img_data.shape&amp;#125;

onnx_model = onnx.load(&amp;quot;../model/simple.onnx&amp;quot;)

mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)

with tvm.transform.PassContext(opt_level=3):
    lib = relay.build(mod, target=target, params=params)


# 运行相关
dev = tvm.device(str(target), 0)
module = graph_executor.GraphModule(lib[&amp;quot;default&amp;quot;](dev))

# 保存库文件
lib_fname = &amp;quot;../lib/mod.so&amp;quot;
lib.export_library(lib_fname)

# 保存模型参数
params_fname = &amp;quot;../lib/mod.params&amp;quot;
with open(params_fname, &amp;quot;wb&amp;quot;) as param_file:
    param_file.write(relay.save_param_dict(lib.get_params()))

# 保存JSON格式的计算图
json_fname = &amp;quot;../lib/mod.json&amp;quot;
with open(json_fname, &amp;quot;w&amp;quot;) as json_file:
    json_file.write(lib.get_executor_config())

dtype = &amp;quot;float32&amp;quot;
module.set_input(input_name, img_data)
module.run()
output_shape = (1, 10)
tvm_output = module.get_output(0, tvm.nd.empty(output_shape)).numpy()

from scipy.special import softmax

# 下载标签列表
labels_url = &amp;quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&amp;quot;
labels_path = download_testdata(labels_url, &amp;quot;synset.txt&amp;quot;, module=&amp;quot;data&amp;quot;)

with open(labels_path, &amp;quot;r&amp;quot;) as f:
    labels = [l.rstrip() for l in f]

# 打开输出文件并读取输出张量
scores = softmax(tvm_output)    #   直接输出模型结果
scores = np.squeeze(tvm_output)
ranks = np.argsort(scores)[::-1]
for rank in ranks[0:5]:
    print(&amp;quot;class=&#39;%s&#39; with probability=%f&amp;quot; % (labels[rank], scores[rank]))

&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://forcheetah.github.io/2024/06/10/deployTVM/</guid>
            <title>【TVM】C++部署运行TVM</title>
            <link>https://forcheetah.github.io/2024/06/10/deployTVM/</link>
            <category>tvm</category>
            <category>cmake</category>
            <category>runtime</category>
            <pubDate>Mon, 10 Jun 2024 19:47:15 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;本篇博客主要介绍如何通过 G++ 编译器编译 C++ 代码，部署 TVM。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;总感觉，属于我们的时代还没开始，就要结束了呢。&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;现状&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#现状&#34;&gt;#&lt;/a&gt; 现状&lt;/h1&gt;
&lt;p&gt;TVM 官方文档:&lt;a href=&#34;https://tvm.apache.org/docs&#34;&gt; 英文文档&lt;/a&gt; &lt;a href=&#34;https://tvm.hyper.ai/&#34;&gt;中文文档&lt;/a&gt; 主要介绍了通过 Python 脚本和 Python 命令行 tvmc 来编译和部署 TVM。但是以这两种方式部署，部署设备还需要安装 Python 运行环境，带来额外空间占用和开销。显然不能以这种方式部署。&lt;/p&gt;
&lt;p&gt;TVM 项目的 &lt;a href=&#34;https://github.com/apache/tvm/tree/main/apps/howto_deploy&#34;&gt;howto_deploy&lt;/a&gt; 目录下提供了 G++ 编译 C++ 代码部署 TVM 的方式。遗憾的是给的例子没有包含模型的权重.params 和图结构.json 的加载，也没有输入图片的加载。&lt;/p&gt;
&lt;p&gt;因此本博客提供了一个简单的 C++ 部署 TVM 工程，可以在 &lt;a href=&#34;https://github.com/ForCheetah/TvmCppDeploy&#34;&gt;TvmCppDeploy 项目&lt;/a&gt; 找到并下载，用于你的 TVM 项目部署。&lt;/p&gt;
&lt;p&gt;该项目没有使用 &lt;a href=&#34;https://github.com/apache/tvm/tree/main/apps/howto_deploy&#34;&gt;TVM 项目 howto_deploy&lt;/a&gt; 中的 Makefile，而是重写了 CMakeLists.txt 文件，更方便读懂和修改。&lt;/p&gt;
&lt;h1 id=&#34;使用方式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#使用方式&#34;&gt;#&lt;/a&gt; 使用方式&lt;/h1&gt;
&lt;p&gt;下载 &lt;a href=&#34;https://github.com/ForCheetah/TvmCppDeploy&#34;&gt;TvmCppDeploy 项目&lt;/a&gt; 到你的本地，可以通过下载 zip 文件后解压缩，也可以直接 git：&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;git&lt;/span&gt; clone https://github.com/ForCheetah/TvmCppDeploy.git&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;进入项目根目录，进行必要的路径修改。和 &lt;a href=&#34;https://github.com/apache/tvm/tree/main/apps/howto_deploy&#34;&gt;TVM 项目 howto_deploy&lt;/a&gt; 一样，本项目也提供了两种部署方式，所需要修改的内容也有些不同。&lt;/p&gt;
&lt;h1 id=&#34;重新编译-tvm_runtime&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#重新编译-tvm_runtime&#34;&gt;#&lt;/a&gt; 重新编译 tvm_runtime&lt;/h1&gt;
&lt;p&gt;重新编译 tvm_runtime，和个人的 C++ 文件编译在一起，编译好的可执行文件可独立执行。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第 1 步：打开  &lt;code&gt;src/Resnet50_deploy.cc &lt;/code&gt; 文件，找到 81 行  &lt;code&gt;const std::string artifacts_folder(&amp;quot;/home/xiamu/whs/temp/resnet50-tvm/&amp;quot;);&lt;/code&gt;  ，将其中的 &lt;code&gt;/home/xiamu/whs/temp/resnet50-tvm/&lt;/code&gt;  修改为自己的已经编译好的模型路径，该路径下应该存在有  &lt;code&gt;mod.so, mod.params, mod.json&lt;/code&gt; 。&lt;/li&gt;
&lt;li&gt;第 2 步：还是 &lt;code&gt;src/Resnet50_deploy.cc &lt;/code&gt; 文件， 找到 132 行，将其中的图片路径 &lt;code&gt;/home/xiamu/whs/python/remote_tvm/imagenet_cat.bin&lt;/code&gt;  改为自己的图片路径，该 bin 文件应当是已经转换好的 float 格式文件。&lt;/li&gt;
&lt;li&gt;第 3 步：打开   &lt;code&gt;src/tvm\_runtime\_pack.cc&lt;/code&gt; ， 将文件中所有的路径中的  &lt;code&gt;/home/xiamu/tvm&lt;/code&gt;   修改为你本地 TVM 工程的根目录路径。 修改完一定要检查一下对应的目录中是否有相应的文件。&lt;/li&gt;
&lt;li&gt;第 4 步：打开  &lt;code&gt;CMakeLists.txt&lt;/code&gt; , 找到第 10 行  &lt;code&gt;set(TVM_ROOT /home/xianmu/tvm)&lt;/code&gt; ，将其中的 &lt;code&gt;/home/xianmu/tvm&lt;/code&gt;  改成你本地 TVM 工程的根目录路径。&lt;/li&gt;
&lt;li&gt;为防止编译报错，可以将 &lt;code&gt;部署方式二： tvm_runtime.so 作为动态链接库编译&lt;/code&gt; 对应的代码（43 至 63 行） 注释掉（当前可能还没有对其进行修改）。&lt;/li&gt;
&lt;li&gt;第 5 步：编译和执行，在根目录下：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;mkdir&lt;/span&gt; build &lt;span class=&#34;token operator&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;token builtin class-name&#34;&gt;cd&lt;/span&gt; build&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;cmake &lt;span class=&#34;token punctuation&#34;&gt;..&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;make&lt;/span&gt; &lt;span class=&#34;token parameter variable&#34;&gt;-j4&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;./MyRunnable&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&#34;tvm_runtimeso-作为动态链接库编译&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#tvm_runtimeso-作为动态链接库编译&#34;&gt;#&lt;/a&gt; tvm_runtime.so 作为动态链接库编译&lt;/h1&gt;
&lt;p&gt;tvm_runtime.so 作为动态链接库，仅编译个人的 C++ 文件，运行时需要链接 libtvm_runtime.so&lt;/p&gt;
&lt;p&gt;这种方式的修改与第一种方式略有不同，修改如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第 1 步：打开  &lt;code&gt;src/Resnet50_deploy.cc &lt;/code&gt; 文件，找到 81 行  &lt;code&gt;const std::string artifacts_folder(&amp;quot;/home/xiamu/whs/temp/resnet50-tvm/&amp;quot;);&lt;/code&gt;  ，将其中的 &lt;code&gt;/home/xiamu/whs/temp/resnet50-tvm/&lt;/code&gt;  修改为自己的已经编译好的模型路径，该路径下应该存在有  &lt;code&gt;mod.so, mod.params, mod.json&lt;/code&gt; 。&lt;/li&gt;
&lt;li&gt;第 2 步：还是 &lt;code&gt;src/Resnet50_deploy.cc &lt;/code&gt; 文件， 找到 132 行，将其中的图片路径 &lt;code&gt;/home/xiamu/whs/python/remote_tvm/imagenet_cat.bin&lt;/code&gt;  改为自己的图片路径，该 bin 文件应当是已经转换好的 float 格式文件。&lt;/li&gt;
&lt;li&gt;第 3 步：打开  &lt;code&gt;CMakeLists.txt&lt;/code&gt; , 找到第 10 行  &lt;code&gt;set(TVM_ROOT /home/xianmu/tvm)&lt;/code&gt; ，将其中的 &lt;code&gt;/home/xianmu/tvm&lt;/code&gt;  改成你本地 TVM 工程的根目录路径。将 62 行的 &lt;code&gt;$&amp;#123;TVM_ROOT&amp;#125;/build&lt;/code&gt;  libtvm_runtime.so 路径修改为你存放 libtvm_runtime.so 库的路径。&lt;/li&gt;
&lt;li&gt;为防止编译报错，可以将 &lt;code&gt;部署方式一： 重新编译 tvm_runtime&lt;/code&gt;  对应的代码（17 至 38 行）注释掉（当前可能还没有对其进行修改）。&lt;/li&gt;
&lt;li&gt;第 5 步：编译和执行，在根目录下：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;mkdir&lt;/span&gt; build &lt;span class=&#34;token operator&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;token builtin class-name&#34;&gt;cd&lt;/span&gt; build&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;cmake &lt;span class=&#34;token punctuation&#34;&gt;..&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;make&lt;/span&gt; &lt;span class=&#34;token parameter variable&#34;&gt;-j4&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;./MyExcute&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://forcheetah.github.io/2024/05/24/tvm1/</guid>
            <title>【TVM】根据例子走通代码库</title>
            <link>https://forcheetah.github.io/2024/05/24/tvm1/</link>
            <category>accelerate</category>
            <category>conv</category>
            <category>tvm</category>
            <pubDate>Fri, 24 May 2024 22:49:36 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;最近开始学习 TVM。感觉 TVM 英文文档中 &lt;a href=&#34;https://tvm.apache.org/docs/dev/tutorial/codebase_walkthrough.html&#34;&gt;TVM Codebase Walkthrough by Example&lt;/a&gt;    一节对于理解 TVM 工程非常有用。本篇文章只是翻译，可以直接跳转查看英文全文。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个时代有这么多愿意开源并将技术介绍给我们的行业大牛，真是我们的荣幸，膜拜！&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;codebase-structure-overview&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#codebase-structure-overview&#34;&gt;#&lt;/a&gt; Codebase Structure Overview&lt;/h1&gt;
&lt;p&gt;在 TVM 存储库的根目录中，我们有以下子目录，它们共同构成了大部分代码库。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;src&lt;/strong&gt;&lt;br&gt;
C++ code for operator compilation and deployment runtimes.&lt;br&gt;
 算子编译 、 runtime 部署 的 C++ 代码&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;src/relay&lt;/strong&gt;&lt;br&gt;
Implementation of Relay, a new functional IR for deep learning framework.&lt;br&gt;
Relay IR 的实现      算子的映射关系在 src/relay/op&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;python&lt;/strong&gt;&lt;br&gt;
Python frontend that wraps C++ functions and objects implemented in src.&lt;br&gt;
python 前端&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;src/topi&lt;/strong&gt;&lt;br&gt;
Compute definitions and backend schedules for standard neural network operators.&lt;br&gt;
 计算标准神经网络算子的定义和后端调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TVM 中 Python 和 C++ 的互操作性不是单向的。尽管在 TVM 中 C++ 完成繁重的内部执行工作，Python 完成用户接口， TVM 中也存在 C++ 调用 Python 的情况：For example, the convolution operator is implemented in Python, and its implementation is invoked from C++ code in Relay.（Relay 中的 C++ 调用 Python 实现的卷积算子）&lt;/p&gt;
&lt;h1 id=&#34;vector-add-example&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#vector-add-example&#34;&gt;#&lt;/a&gt; Vector Add Example&lt;/h1&gt;
&lt;p&gt;使用 vector add 的例子来查看底层 TVM API.&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;n &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;1024&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;A &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;placeholder&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#39;A&#39;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;B &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;placeholder&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#39;B&#39;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;C &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;compute&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;shape&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;lambda&lt;/span&gt; i&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt; A&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;C&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这里面 A、B、C 都是  &lt;code&gt;tvm.tensor.Tensor&lt;/code&gt;   其 Python 定义位于 &lt;code&gt;python/tvm/te/tensor.py&lt;/code&gt; . 支撑的 C++ 定义位于 &lt;code&gt;include/tvm/te/tensor.h&lt;/code&gt;  和 &lt;code&gt;src/te/tensor.cc&lt;/code&gt;  所有的 Python 类型定义都能找到对应的相同名字的 C++ 定义。&lt;/p&gt;
&lt;p&gt;Python 对 C++ 的包装位于  &lt;code&gt;python/tvm/_ffi/&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;一个 Tensor 包含一个 Operation 类，定义于 python/tvm/te/tensor.py，对应的 C++ 实现位于 &lt;code&gt;include/tvm/te/operation.h&lt;/code&gt;  和 &lt;code&gt;src/tvm/te/operation&lt;/code&gt;  。 &lt;code&gt;Tensor&lt;/code&gt;  是  &lt;code&gt;Operation&lt;/code&gt;  类的输出。&lt;/p&gt;
&lt;p&gt;我们将输出张量 C 对应的操作传递给 &lt;code&gt;tvm.te.create_schedule()&lt;/code&gt;  函数 （来自于 &lt;code&gt;python/tvm/te/schedule.py&lt;/code&gt; 。）&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;create_schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;op&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这个函数映射到 C++ 函数 &lt;code&gt;include/tvm/schedule.h&lt;/code&gt; 。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;inline Schedule create_schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;Array&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;Operation&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt; ops&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;return&lt;/span&gt; Schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;ops&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;Schedule&lt;/code&gt;  包含 &lt;code&gt;Stage&lt;/code&gt;  输出  &lt;code&gt;Operation&lt;/code&gt;  的集合。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Stage&lt;/code&gt;  对应于一个操作 &lt;code&gt;Operation&lt;/code&gt; 。上面的 vector add 操作中有两个 placeholder ops 和一个 compute op. 所以 &lt;code&gt;Schedule s&lt;/code&gt;  有三个状态  &lt;code&gt;Stage&lt;/code&gt; ，每个 &lt;code&gt;Stage&lt;/code&gt;  持有以下信息： 循环嵌套结构、每个循环的类型（ &lt;code&gt;Parallel，Vectorized，Unrolled&lt;/code&gt; ）、以及在下一个循环嵌套 &lt;code&gt;Stage&lt;/code&gt;  中在哪里执行它自己的计算。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Schedule&lt;/code&gt;  和 &lt;code&gt;Stage&lt;/code&gt;  本身定义在 &lt;code&gt;tvm/python/te/schedule.py&lt;/code&gt; ，  &lt;code&gt;include/tvm/te/schedule.h&lt;/code&gt; ， 和 &lt;code&gt;src/te/schedule/schedule_ops.cc&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;为简单起见，我们使用 &lt;code&gt;tvm.build(...)&lt;/code&gt;  处理上方 &lt;code&gt;create_schedule()&lt;/code&gt;  函数创建的默认 &lt;code&gt;Schedule s&lt;/code&gt;  和 &amp;lt;em&amp;gt;。我们必须添加必要的线程绑定，来使得其能在 GPU 上运行：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;target &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token string&#34;&gt;&#34;cuda&#34;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;bx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tx &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;split&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;op&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;axis&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; factor&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;bind&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;bx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;thread_axis&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;blockIdx.x&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;bind&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;tx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;thread_axis&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;threadIdx.x&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;fadd &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;build&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;s&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; target&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;tvm.build(...)&lt;/code&gt; ，定义在 &lt;code&gt;python/tvm/driver/build_module.py&lt;/code&gt; ， 需要输入一个 &lt;code&gt;Schedule&lt;/code&gt; ;  &lt;code&gt;input&lt;/code&gt; , &lt;code&gt;output Tensor&lt;/code&gt; ; 以及一个 &lt;code&gt;target&lt;/code&gt; 。返回一个 &lt;code&gt;tvm.runtime.Module&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;整个 &lt;code&gt;tvm.build(...)&lt;/code&gt;  过程可以分成两步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;i. 降级 高级的、初始的循环嵌套结构被转换为 最终的、低级的 IR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ii. 代码生成 low level IR 生成目标机器码&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;降级是通过 &lt;code&gt;tvm.lower()&lt;/code&gt;  函数完成的，它定义在 &lt;code&gt;python/tvm/build\_module.py&lt;/code&gt; 。第一，指定绑定推理，一个最初的循环嵌套结构就创建好了。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;default_function&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          binds&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token boolean&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          simple_mode&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token boolean&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   bounds &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; schedule&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;InferBound&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; schedule&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;ScheduleOps&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; bounds&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;边界推断是推断所有循环边界和中间缓冲区大小的过程。如果你的目标是 CUDA，且你用了 share memory，它需要的最小 size 在此处确定。绑定推理时在 &lt;code&gt;src/te/schedule/bound.cc，src/te/schedule/graph.cc &lt;/code&gt;  和  &lt;code&gt;src/te/schedule/message\_passing.cc&lt;/code&gt;  中实现的。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;stmt&lt;/code&gt; ， &lt;code&gt;ScheduleOps()&lt;/code&gt;  的输出，表示一个初识的循环嵌套结构。如果在 schedule 中已经应用了 &lt;code&gt;reorder&lt;/code&gt;  和 &lt;code&gt;split&lt;/code&gt;  原语，那么初始的循环嵌套结构已经反映了这些变化。 &lt;code&gt;ScheduleOps()&lt;/code&gt;  定义在 &lt;code&gt;rc/te/schedule/schedule_ops.cc&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;接下来应用一些 lowering passes to  &lt;code&gt;stmt&lt;/code&gt;  . 这些 passes 在 &lt;code&gt;src/tir/pass&lt;/code&gt;  子文件夹下实现。举个例子，如果在你的 &lt;code&gt;schedule&lt;/code&gt;  中应用了 &lt;code&gt;vectorize&lt;/code&gt;  或者 &lt;code&gt;unroll&lt;/code&gt;  原语，他们会被应用到循环 vectorization 和 unrolling passes。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; ir_pass&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;VectorizeLoop&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;stmt&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; ir_pass&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;UnrollLoop&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    stmt&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_step&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_depth&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_extent&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;unroll_explicit&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;在降级 lowering 结束后， &lt;code&gt;build()&lt;/code&gt;  函数生成目标机器代码。如果你的设备是 X86, 这个代码可能包含 SSE 或者 AVX 指令；如果是 CUDA 设备，将包含 PTX 指令。 此外，除了目标特定的机器代码之外，TVM 还生成负责内存管理、内核启动等的主机端代码。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;build\_module()&lt;/code&gt;  函数完成代码生成，定义在 &lt;code&gt;python/tvm/target/codegen.py&lt;/code&gt; 。在 C++ 端代码生成定义在 &lt;code&gt;src/target/codegen&lt;/code&gt; 。 &lt;code&gt;build\_module()&lt;/code&gt;  Python 函数会搜索在 &lt;code&gt;src/target/codegen/codegen.cc&lt;/code&gt;  中的 &lt;code&gt;build()&lt;/code&gt;  函数。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;build()&lt;/code&gt;  函数 &lt;code&gt;PackedFunc&lt;/code&gt;  注册表中为目标设备查找代码生成器，并调用找到的函数。例如， &lt;code&gt;codegen.build\_cuda&lt;/code&gt;  函数注册在 &lt;code&gt;src/codegen/build_cuda_on.cc&lt;/code&gt; ，就像这样：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;TVM_REGISTER_GLOBAL&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;codegen.build_cuda&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;set_body&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;TVMArgs args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; TVMRetValue&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; rv&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;rv &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; BuildCUDA&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;args&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;上方的 &lt;code&gt;BuildCUDA()&lt;/code&gt;  函数使用定义在 &lt;code&gt;src/codegen/codegen_cuda.cc&lt;/code&gt;  的 &lt;code&gt;CodeGenCUDA&lt;/code&gt;  类，从 lowered IR 生成 CUDA kernel source，并使用 NVRTC 编译 kernel。如果你的目标设备使用 LLVM，包括 X86、ARM、NVPTX 和 AMDGPU，代码可由定义在 &lt;code&gt;src/codegen/llvm/codegen_llvm.cc&lt;/code&gt;  的 &lt;code&gt;CodeGenLLVM&lt;/code&gt;  来生成。 &lt;code&gt;CodeGenLLVM&lt;/code&gt;  将 TVM IR 转换成 LLVM IR，运行一些 LLVM 优化 passes，以及生成目标机器码。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;src/codegen/codegen.cc&lt;/code&gt;  中的 &lt;code&gt;Build()&lt;/code&gt;  函数会返回一个 &lt;code&gt;runtime::Module&lt;/code&gt;  类，它定义在 &lt;code&gt;include/tvm/runtime/module.h&lt;/code&gt;  和 &lt;code&gt;src/runtime/module.cc&lt;/code&gt; 。一个 &lt;code&gt;Module&lt;/code&gt;  类是一个潜在目标 设备的特定 &lt;code&gt;ModuleNode&lt;/code&gt;  的容器。&lt;/p&gt;
&lt;p&gt;每个后端都实现一个 &lt;code&gt;ModuleNode&lt;/code&gt;  的子类，来添加目标特定的 runtime API 调用。 例如，CUDA 后端在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  实现 &lt;code&gt;CUDAModuleNode&lt;/code&gt;  类，来管理 CUDA 驱动 API。上方的 &lt;code&gt;BuildCUDA()&lt;/code&gt;  函数用 &lt;code&gt;runtime::Module&lt;/code&gt;  包装了 &lt;code&gt;CUDAModuleNode&lt;/code&gt; ，并包装到 Python 端。LLVM 后端在 &lt;code&gt;src/codegen/llvm/llvm_module.cc&lt;/code&gt;  实现了 &lt;code&gt;LLVMModuleNode&lt;/code&gt; ，处理了 JIT 执行和编译代码。其他对应各个后端的 &lt;code&gt;ModuleNode&lt;/code&gt;  子类可以在 &lt;code&gt;src/runtime&lt;/code&gt;  子文件夹找到。&lt;br&gt;
返回的 &lt;code&gt;module&lt;/code&gt; ，可以被认作编译函数和设备 API 的组合，可以被 TVM 的 NDArray objects 调用。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;dev &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;device&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;target&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;a &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;random&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;uniform&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;size&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;astype&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;b &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;random&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;uniform&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;size&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;astype&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;B&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;c &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;zeros&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dtype&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;fadd&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;a&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; b&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; c&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;output &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; c&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;numpy&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;在幕后，TVM 会自动分配设备内存并管理内存传输。为了实现这个目标，每个后端都需要继承在 &lt;code&gt;include/tvm/runtime/device_api.h&lt;/code&gt;  定义的 &lt;code&gt;DeviceAPI&lt;/code&gt;  类，使用设备特定的 API 重写里面的内存管理方法。例如，CUDA 后端在 &lt;code&gt;src/runtime/cuda/cuda_device_api.cc&lt;/code&gt;  使用 &lt;code&gt;cudaMalloc&lt;/code&gt; 、 &lt;code&gt;cudaMemcpy&lt;/code&gt;  实现了 &lt;code&gt;CUDADeviceAPI&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;第一次使用 &lt;code&gt;fadd(a, b, c)&lt;/code&gt;  调用编译后的模块时，会调用  &lt;code&gt;ModuleNode&lt;/code&gt;  的  &lt;code&gt;GetFunction()&lt;/code&gt;  方法来获取可用于内核调用的  &lt;code&gt;PackedFunc&lt;/code&gt; 。例如，在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  CUDA 后端实现了 &lt;code&gt;CUDAModuleNode::GetFunction()&lt;/code&gt;  函数如下：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;PackedFunc CUDAModuleNode&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;GetFunction&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      const std&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;string&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; name&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      const std&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;shared_ptr&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;ModuleNode&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; sptr_to_self&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  auto it &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; fmap_&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;find&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;name&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  const FunctionInfo&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; info &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; it&lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;second&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  CUDAWrappedFunc f&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  f&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;Init&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;this&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; sptr_to_self&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;arg_types&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;size&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;launch_param_tags&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;return&lt;/span&gt; PackFuncVoidAddr&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;f&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;arg_types&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;PackedFunc&lt;/code&gt;  的重载函数 &lt;code&gt;operator()&lt;/code&gt;  会被调用。从而会调用定义在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  的 &lt;code&gt;CUDAWrappedFunc&lt;/code&gt;  的 &lt;code&gt;operator()&lt;/code&gt;  函数，最终我们会看到 &lt;code&gt;cuLaunchKernel&lt;/code&gt;  驱动会调用：&lt;/p&gt;
&lt;figure class=&#34;highlight cpp&#34;&gt;&lt;figcaption data-lang=&#34;C++&#34;&gt;&lt;span&gt;p&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;token class-name&#34;&gt;CUDAWrappedFunc&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt; &lt;span class=&#34;token keyword&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;Init&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;operator&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;TVMArgs args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                  TVMRetValue&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; rv&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                  &lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; void_args&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; device_id&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token function&#34;&gt;CUDA_CALL&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cudaGetDevice&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;fcache_&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;nullptr&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      fcache_&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; m_&lt;span class=&#34;token operator&#34;&gt;-&gt;&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;GetFunc&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; func_name_&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    CUstream strm &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token generic-function&#34;&gt;&lt;span class=&#34;token function&#34;&gt;static_cast&lt;/span&gt;&lt;span class=&#34;token generic class-name&#34;&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;CUstream&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token class-name&#34;&gt;CUDAThreadEntry&lt;/span&gt;&lt;span class=&#34;token double-colon punctuation&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;ThreadLocal&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;-&gt;&lt;/span&gt;stream&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    ThreadWorkLoad wl &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; launch_param_config_&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;Extract&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;args&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    CUresult result &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;cuLaunchKernel&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;16&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        fcache_&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;17&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;grid_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;18&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;grid_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;19&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;grid_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;20&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;block_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;21&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;block_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;22&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;block_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;23&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; strm&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; void_args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;24&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;25&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;本文概括了 TVM 如何编译和执行函数。 虽然本文没有详细说明 TOPI 或 Relay，但最终所有神经网络算子都会经历与上述相同的编译过程。&lt;/p&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>

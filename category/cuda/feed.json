{
    "version": "https://jsonfeed.org/version/1",
    "title": "Пусть этот камень будет более крепким, чем человек • All posts by \"cuda\" category",
    "description": "有自己的博客很帅，但是我很懒，要命！！！",
    "home_page_url": "https://forcheetah.github.io",
    "items": [
        {
            "id": "https://forcheetah.github.io/2026/02/01/CUDA03/",
            "url": "https://forcheetah.github.io/2026/02/01/CUDA03/",
            "title": "【CUDA C++】GPU内存使用【3】",
            "date_published": "2026-02-01T13:47:17.043Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇介绍 GPU 的内存使用，主要是全局内存的合并内存访问，和共享内存的 bank 冲突。资料来源于 <a href=\"https://docs.nvidia.com/cuda/cuda-programming-guide/contents.html\">官网 CUDA Programming Guide</a>。本文会比官网教程简洁一些，去掉一些我不太感兴趣的内容（任性）。</p>\n<p>参考  <a href=\"https://docs.nvidia.com/cuda/cuda-programming-guide/contents.html\">官网 CUDA Programming Guide</a>。</p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"gpu合并内存访问\"><a class=\"anchor\" href=\"#gpu合并内存访问\">#</a> GPU 合并内存访问</h1>\n<p>GPU 的全局内存 Global Memory (GPU  Dram, 常见的显卡 8GB、12GB)，是通过 32-byte memory transactions 进行访问的。<br>\n当一个 CUDA 线程从全局内存中请求一个数据字节时，相关的 thread warp 会将该 thread warp 中所有线程的内存请求合并成满足该请求所需的内存交易数量，具体数量取决于每个线程访问的数据字节的大小以及内存地址在各线程中的分布情况。<br>\n例如，如果一个线程请求一个 4 字节的数据字节，那么该 Thread warp（包含 32 个 Thread）向全局内存发出的实际内存交易总量将是 32 字节。如果同一 warp 的其他 31 个线程并不需要这 32 字节中的数据，那么数据的利用率很低。而如果一个线程从全局内存中请求一个 4 字节的数据字节，并且交易大小为 32 字节，如果该 Thread warp 中的其他线程可以从这个 32 字节的请求中使用其他 4 字节的数据字节，它们可以在同一个请求中获取所需的数据（合并内存访问）。<br>\n举一个简单的例子，如果在 warp 请求中连续的线程在内存中请求连续的 4 个字节的数据，那么该 warp 将合并他们的请求，共请求 128 个字节的内存，而这 128 个字节的数据将通过四次 32 字节的内存操作来获取。这就实现了内存系统的 100% 利用率。也就是说，warp 利用了 100% 的内存流量。下图展示了这种完全协同的内存访问示例：</p>\n<p><img loading=\"lazy\" data-src=\"1769953213349.jpg\" alt=\"线程合并访存\"></p>\n<p>与之相反的，最糟糕的情况是，连续的线程（同一个 warp 中的线程）在同一内存位置上访问的数据元素之间相隔至少 32 个字节。在这种情况下，Thread warp 将被迫为每个线程执行一次 32 字节的内存操作，那么内存传输的总字节数将为 32 字节 * 32 Thread = 1024 字节。然而，实际使用的内存量仅为 128 字节（每个 warp 中的每个线程使用 4 字节），因此内存利用率仅为 128 / 1024 = 12.5%。这是对内存系统的极大浪费。下图展示了这种未合并的内存访问示例：</p>\n<p><img loading=\"lazy\" data-src=\"1769953274901.jpg\" alt=\"未能实现线程合并访存\"></p>\n<p>实现合并内存访问最直接的方法是让连续的线程依次访问内存中的连续元素。<br>\n例如，对于使用 1D thread block 启动的 kernal，以下的 VecAdd 内核将实现合并内存访问。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>__global__ <span class=\"token keyword\">void</span> <span class=\"token function\">vecAdd</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">float</span><span class=\"token operator\">*</span> A<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> B<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> C<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> vectorLength<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token keyword\">int</span> workIndex <span class=\"token operator\">=</span> threadIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> blockIdx<span class=\"token punctuation\">.</span>x<span class=\"token operator\">*</span>blockDim<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span>workIndex <span class=\"token operator\">&lt;</span> vectorLength<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>        C<span class=\"token punctuation\">[</span>workIndex<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> A<span class=\"token punctuation\">[</span>workIndex<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> B<span class=\"token punctuation\">[</span>workIndex<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>值得注意的是，并不存在这样的规定，即连续的线程必须访问内存中的连续元素才能实现协同式内存访问，这只是实现协同式访问的常见方式而已。只要线程组中的所有线程以某种线性或置换的方式访问来自相同 32 字节内存段的元素，就会发生协同式内存访问。换句话说，实现协同式内存访问的最佳方式是将使用的字节数与传输的字节数的比例最大化。<br>\n确保全局内存访问的正确合并是编写高效 CUDA 内核时最重要的性能考量之一。应用程序必须尽可能高效地利用内存系统。</p>\n<p>矩阵转置例子 合并内存访问</p>\n<p>将一个  <code>N*N</code>  的 float 型外部矩阵，从 A 转置为 C，这个例子使用 2D grid，并假设使用 32<em>32 线程的 2D 线程块，因此  <code>blockDim.x = 32</code> ， <code>blockDim.y = 32</code> ，每个线程块要操作 32</em>32 的矩阵切块。每个线程都只对矩阵中的一个特定元素进行处理，因此无需对线程进行显式的同步操作。图 12 展示了这一矩阵转置操作。内核源代码与该图相对应。</p>\n<p><img loading=\"lazy\" data-src=\"1769953336088.jpg\" alt=\"矩阵置换示意图\"></p>\n<p>每个矩阵顶部和左侧的标签分别是二维线程块的索引，也可以视为分块索引，其中每个小方块代表矩阵中将由二维线程块处理的一个分块。在这个例子中，分块大小为 32 x 32 个元素，因此每个小方块代表矩阵的一个 32 x 32 的分块。绿色阴影方块显示了一个示例分块在转置操作前后的位置。<br>\n注分块矩阵转置，块的位置转置（即行列互换）每个块自身也转置。：<br>\n<img loading=\"lazy\" data-src=\"1769953350197.jpg\" alt=\"分块矩阵转置\"></p>\n<p>此外，提前介绍一下 CUDA C++ 代码中关于线程用到的参数变量：<br>\n<img loading=\"lazy\" data-src=\"1769953364342.jpg\" alt=\"参数示意图\"></p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">/* macro to index a 1D memory array with 2D indices in row-major order */</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\">/* ld is the leading dimension, i.e. the number of columns in the matrix     */</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">define</span> <span class=\"token macro-name function\">INDX</span><span class=\"token expression\"><span class=\"token punctuation\">(</span> row<span class=\"token punctuation\">,</span> col<span class=\"token punctuation\">,</span> ld <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span> <span class=\"token punctuation\">(</span> <span class=\"token punctuation\">(</span>row<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>ld<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>col<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">)</span></span></span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\">/* CUDA kernel for naive matrix transpose */</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>__global__ <span class=\"token keyword\">void</span> <span class=\"token function\">naive_cuda_transpose</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> m<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>a<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>c <span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>    <span class=\"token keyword\">int</span> myCol <span class=\"token operator\">=</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>    <span class=\"token keyword\">int</span> myRow <span class=\"token operator\">=</span> blockDim<span class=\"token punctuation\">.</span>y <span class=\"token operator\">*</span> blockIdx<span class=\"token punctuation\">.</span>y <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span> myRow <span class=\"token operator\">&lt;</span> m <span class=\"token operator\">&amp;&amp;</span> myCol <span class=\"token operator\">&lt;</span> m <span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>        c<span class=\"token punctuation\">[</span><span class=\"token function\">INDX</span><span class=\"token punctuation\">(</span> myCol<span class=\"token punctuation\">,</span> myRow<span class=\"token punctuation\">,</span> m <span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> a<span class=\"token punctuation\">[</span><span class=\"token function\">INDX</span><span class=\"token punctuation\">(</span> myRow<span class=\"token punctuation\">,</span> myCol<span class=\"token punctuation\">,</span> m <span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span> <span class=\"token comment\">/* end if */</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>    <span class=\"token keyword\">return</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre><span class=\"token punctuation\">&#125;</span> <span class=\"token comment\">/* end naive_cuda_transpose */</span></pre></td></tr></table></figure><p>以上代码：</p>\n<ul>\n<li>每个 CUDA 线程通过自己的 (blockIdx, threadIdx) 计算出它在逻辑二维网格中的位置 (myRow, myCol)。</li>\n<li>这个位置直接对应输入矩阵 a 中的一个元素：a [myRow][myCol]。<br>\n有对应的位置，线程 (myRow, myCol) 负责读取 a [myRow][myCol]，并将其写入 c [myCol][myRow]<br>\n 假设位于 global memory 中的待转置矩阵是按照行主序连续存放的（一般都是这样），那么由以上代码可知，取数据阶段，同一 warp 中的 32 个线程所需的数据是连续的，因此可以合并访存，效率很高，如下图所示：</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"1769953414177.jpg\" alt=\"合并访存示意图\"></p>\n<p>而这 32 个线程在存储结果的时候，相互之间数据间隔超过 32 个字节，无法合并访存，效率很低。</p>\n<h1 id=\"共享内存访问\"><a class=\"anchor\" href=\"#共享内存访问\">#</a> 共享内存访问</h1>\n<p>共享内存有 32 个存储单元，其组织方式是：连续的 32 位数据会映射到连续的存储单元上。每个存储单元的带宽为每时钟周期 32 位。<br>\n当同一线程 warp 中的多个线程试图访问同一缓存区中的不同元素时，就会发生缓存冲突。在这种情况下，对该缓存区中的数据的访问将被串行化处理，直到所有请求该数据的线程都获取到该数据为止。这种访问的串行化会导致性能下降。<br>\n这种情况的两个例外情况发生在同一 warp 中的多个线程同时访问（无论是读取还是写入）同一个共享内存位置时。对于读取操作，数据会被广播给请求的线程。对于写入操作，每个共享内存地址仅由其中的一个线程进行写入（哪个线程执行写入操作是不确定的）。<br>\n下图展示了一些分段访问的示例。内存 bank 内的红色方框表示共享内存中的一个特定位置。图中，左边和右边示例都没有访问冲突，但是中间示例有两路访问冲突。</p>\n<p><img loading=\"lazy\" data-src=\"1769953435830.jpg\" alt=\"bank访问示意图\"></p>\n<p>另一个示例，如下图所示：<br>\n左图：通过随机排列实现无冲突访问。<br>\n中图：由于线程 3、4、6、7 和 9 都访问了同一存储单元中的第 5 个存储器组，所以实现了无冲突访问。<br>\n右图：无冲突广播访问（同 warp 内的线程访问同一存储单元）。<br>\n<img loading=\"lazy\" data-src=\"1769953452832.jpg\" alt=\"bank访问示意图\"></p>\n<h1 id=\"矩阵转置例子-使用共享内存\"><a class=\"anchor\" href=\"#矩阵转置例子-使用共享内存\">#</a> 矩阵转置例子 使用共享内存</h1>\n<p>在上一个 “使用全局内存的矩阵转置示例” 中，展并未针对全局内存的高效使用进行优化，因为 c 矩阵的写入操作没有得到恰当的合并。在本示例中，共享内存将被视为用户管理的缓存，用于在全局内存中进行加载和存储操作，从而实现矩阵转置例子的读和写操作的全局内存合并访问。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">/* definitions of thread block size in X and Y directions */</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">define</span> <span class=\"token macro-name\">THREADS_PER_BLOCK_X</span> <span class=\"token expression\"><span class=\"token number\">32</span></span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">define</span> <span class=\"token macro-name\">THREADS_PER_BLOCK_Y</span> <span class=\"token expression\"><span class=\"token number\">32</span></span></span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\">/* macro to index a 1D memory array with 2D indices in column-major order */</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\">/* ld is the leading dimension, i.e. the number of rows in the matrix     */</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">define</span> <span class=\"token macro-name function\">INDX</span><span class=\"token expression\"><span class=\"token punctuation\">(</span> row<span class=\"token punctuation\">,</span> col<span class=\"token punctuation\">,</span> ld <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span> <span class=\"token punctuation\">(</span> <span class=\"token punctuation\">(</span>col<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>ld<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>row<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">)</span></span></span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre></pre></td></tr><tr><td data-num=\"11\"></td><td><pre><span class=\"token comment\">/* CUDA kernel for shared memory matrix transpose */</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>__global__ <span class=\"token keyword\">void</span> <span class=\"token function\">smem_cuda_transpose</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> m<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>                                    <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>a<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>                                    <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>c <span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>    <span class=\"token comment\">/* declare a statically allocated shared memory array */</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>    __shared__ <span class=\"token keyword\">float</span> smemArray<span class=\"token punctuation\">[</span>THREADS_PER_BLOCK_X<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>THREADS_PER_BLOCK_Y<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>    <span class=\"token comment\">/* determine my row and column indices for the error checking code */</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> myRow <span class=\"token operator\">=</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> myCol <span class=\"token operator\">=</span> blockDim<span class=\"token punctuation\">.</span>y <span class=\"token operator\">*</span> blockIdx<span class=\"token punctuation\">.</span>y <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre></pre></td></tr><tr><td data-num=\"27\"></td><td><pre>    <span class=\"token comment\">/* determine my row tile and column tile index */</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre></pre></td></tr><tr><td data-num=\"29\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> tileX <span class=\"token operator\">=</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"30\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> tileY <span class=\"token operator\">=</span> blockDim<span class=\"token punctuation\">.</span>y <span class=\"token operator\">*</span> blockIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"31\"></td><td><pre></pre></td></tr><tr><td data-num=\"32\"></td><td><pre>    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span> myRow <span class=\"token operator\">&lt;</span> m <span class=\"token operator\">&amp;&amp;</span> myCol <span class=\"token operator\">&lt;</span> m <span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"33\"></td><td><pre>    <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"34\"></td><td><pre>        <span class=\"token comment\">/* read from global memory into shared memory array */</span></pre></td></tr><tr><td data-num=\"35\"></td><td><pre>        smemArray<span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> a<span class=\"token punctuation\">[</span><span class=\"token function\">INDX</span><span class=\"token punctuation\">(</span> tileX <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">,</span> tileY <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">,</span> m <span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"36\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span> <span class=\"token comment\">/* end if */</span></pre></td></tr><tr><td data-num=\"37\"></td><td><pre></pre></td></tr><tr><td data-num=\"38\"></td><td><pre>    <span class=\"token comment\">/* synchronize the threads in the thread block */</span></pre></td></tr><tr><td data-num=\"39\"></td><td><pre>    <span class=\"token function\">__syncthreads</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"40\"></td><td><pre></pre></td></tr><tr><td data-num=\"41\"></td><td><pre>    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span> myRow <span class=\"token operator\">&lt;</span> m <span class=\"token operator\">&amp;&amp;</span> myCol <span class=\"token operator\">&lt;</span> m <span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"42\"></td><td><pre>    <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"43\"></td><td><pre>        <span class=\"token comment\">/* write the result from shared memory to global memory */</span></pre></td></tr><tr><td data-num=\"44\"></td><td><pre>        c<span class=\"token punctuation\">[</span><span class=\"token function\">INDX</span><span class=\"token punctuation\">(</span> tileY <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">,</span> tileX <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">,</span> m <span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> smemArray<span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"45\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span> <span class=\"token comment\">/* end if */</span></pre></td></tr><tr><td data-num=\"46\"></td><td><pre>    <span class=\"token keyword\">return</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"47\"></td><td><pre></pre></td></tr><tr><td data-num=\"48\"></td><td><pre><span class=\"token punctuation\">&#125;</span> <span class=\"token comment\">/* end smem_cuda_transpose */</span></pre></td></tr></table></figure><p>上述代码，第一步  <code>smemArray[threadIdx.x][threadIdx.y] = a[INDX( tileX + threadIdx.x, tileY + threadIdx.y, m )];</code>  将 global memory 中的数据保存至共享内存中，这个过程已经完成了 Tile 块内的数据转置。读取全局内存的过程中， <code>threadIdx.x</code>  经 <code>INDX( row, col, ld ) ( ( (col) * (ld) ) + (row) )</code>  映射后，地址连续变化，所以在读取内存中，已经合并内存访问。<br>\n第二步  <code>c[INDX( tileY + threadIdx.x, tileX + threadIdx.y, m )] = smemArray[threadIdx.y][threadIdx.x];</code> ，将共享内存中的数据搬到 global memory 中的指定位置。同样的，在存储过程中， <code>threadIdx.x</code>  连续变化，C [] 地址也连续变化，也实现了合并访存。</p>\n<p>这段代码展示了共享内存的两种常见用途。<br>\n共享内存用于将数据从全局内存中转移出来，以确保对全局内存的读取和写入操作都能得到正确地合并处理。<br>\n共享内存用于使同一线程块中的各个线程能够相互共享数据。</p>\n<p>但上述代码存在共享内存的 bank 冲突。<br>\nGPU 的 share memory 有 32 个 bank，每个 bank 的 cache line 长度为 4 字节（32bit），并采用低位交叉的地址映射方式。当我们申请 <code>__shared__ float smemArray[32][32]</code>  的共享内存空间时，第一行 32 个 32float 将分散到 32 个 bank 中。<br>\n下图中，同样的颜色属于同一 bank，数字代表 warp。比如草绿色这一列全部是 bank0，数字 0 是 warp0.</p>\n<p><img loading=\"lazy\" data-src=\"1769953518248.jpg\" alt=\"bank示意图\"></p>\n<p>回到刚才的矩阵砖石例子中，我们可以通过检查共享内存的使用情况来判断是否存在 bank 冲突。共享内存的首次使用情况是将全局内存中的数据存储到共享内存中：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>smemArray<span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> a<span class=\"token punctuation\">[</span><span class=\"token function\">INDX</span><span class=\"token punctuation\">(</span> tileX <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">,</span> tileY <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">,</span> m <span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>因为 C++ 数组是按行优先顺序存储的，所以同一 warp 中的连续线程（由连续的 threadIdx.x 值表示），由于 threadIdx.x 是共享内存数组的第一个索引，因此会以 32 个元素的步长访问 smemArray。这会导致 32 路 bank 冲突，如上图 的左图所示。<br>\n共享内存的第二种使用方式是将来自共享内存的数据写回全局内存：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>c<span class=\"token punctuation\">[</span><span class=\"token function\">INDX</span><span class=\"token punctuation\">(</span> tileY <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">,</span> tileX <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">,</span> m <span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> smemArray<span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>在这种情况下，由于 threadIdx.x 是 smemArray 数组中的第二个索引，同一线程组中的连续线程将以 1 个元素的步长访问 smemArray。这避免了存取冲突，如上图右侧。<br>\n如何避免 bank 冲突呢？常见方法是通过在数组的列维度上增加 1 来填充共享内存，具体操作如下：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>__shared__ <span class=\"token keyword\">float</span> smemArray<span class=\"token punctuation\">[</span>THREADS_PER_BLOCK_X<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>THREADS_PER_BLOCK_Y<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p><img loading=\"lazy\" data-src=\"1769953621010.jpg\" alt=\"bank避免冲突示意图\"></p>\n<p>这种方式，可使得共享内存存储和读取的时候，都不会有 bank 冲突。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，并指明哪一篇博客，看到一定及时回复！</p>\n",
            "tags": [
                "AI",
                "CUDA"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2026/01/16/CUDA02/",
            "url": "https://forcheetah.github.io/2026/01/16/CUDA02/",
            "title": "【CUDA C++】GPU存储【2】",
            "date_published": "2026-01-16T14:19:44.696Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇是介绍 GPU 的存储硬件。资料来源于 <a href=\"https://docs.nvidia.com/cuda/cuda-programming-guide/contents.html\">官网 CUDA Programming Guide</a>。本文会比官网教程简洁一些，去掉一些我不太感兴趣的内容（任性）。</p>\n<p>参考  <a href=\"https://docs.nvidia.com/cuda/cuda-programming-guide/contents.html\">官网 CUDA Programming Guide</a>。</p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"gpu-memory\"><a class=\"anchor\" href=\"#gpu-memory\">#</a> GPU Memory</h1>\n<p>在现代计算系统中，有效利用内存与最大限度地利用执行计算的逻辑单元同样重要。异构系统拥有多个内存空间，而图形处理器（GPU）除了缓存之外，还包含多种类型的可编程片上内存。</p>\n<p><img loading=\"lazy\" data-src=\"1768573069898.jpg\" alt=\"CPU 存储\"></p>\n<h1 id=\"global-memory全局内存\"><a class=\"anchor\" href=\"#global-memory全局内存\">#</a> Global Memory（全局内存）</h1>\n<ul>\n<li>物理本质：GPU 芯片外的 DRAM 芯片（即显存，VRAM）</li>\n<li>别名：GPU DRAM、Device Memory</li>\n<li>访问范围：所有 SM（Streaming Multiprocessors）均可访问</li>\n<li>特点：</li>\n<li>容量大（如 8GB–24GB）</li>\n<li>延迟高，但带宽极高（通过宽总线）</li>\n<li>在 CUDA 编程中通过 cudaMalloc () 分配</li>\n<li>在架构图中的位置</li>\n<li>位于 GPU 芯片外部，通过 Memory Controller 连接到 GPU 核心，通常标注为 &quot;GPU DRAM&quot; 或 &quot;Global Memory&quot;。</li>\n</ul>\n<h1 id=\"system-memory-host-memory系统内存-主机内存\"><a class=\"anchor\" href=\"#system-memory-host-memory系统内存-主机内存\">#</a> System Memory / Host Memory（系统内存 / 主机内存）</h1>\n<ul>\n<li>物理本质：CPU 旁边的 DDR4/DDR5 内存条</li>\n<li>访问者：CPU 可直接访问；GPU 需通过 PCIe 或 NVLink 访问（速度慢）</li>\n<li>在统一虚拟地址空间下：与 GPU 全局内存共用一个地址空间，但物理分离</li>\n<li>在架构图中的位置：<br>\n位于 CPU 一侧，通过 PCIe/NVLink 总线连接到 GPU，通常标注为 &quot;SYSTEM DRAM&quot; 或 &quot;Host Memory&quot;。</li>\n</ul>\n<h1 id=\"on-chip-memory片上内存属于每个-sm\"><a class=\"anchor\" href=\"#on-chip-memory片上内存属于每个-sm\">#</a> On-Chip Memory（片上内存）—— 属于每个 SM</h1>\n<h2 id=\"a-register-file寄存器文件\"><a class=\"anchor\" href=\"#a-register-file寄存器文件\">#</a> (a) Register File（寄存器文件）</h2>\n<ul>\n<li>归属：每个 SM 独有</li>\n<li>分配单位：每个线程（thread）</li>\n<li>用途：存储线程的局部变量（由编译器自动分配）</li>\n<li>特点：</li>\n<li>速度最快（零延迟访问）</li>\n<li>容量有限（如每个 SM 有 65536 个 32-bit 寄存器）</li>\n<li>线程块能否被调度到 SM，取决于寄存器是否够用</li>\n<li>在架构图中的位置：<br>\n位于 每个 SM 内部，紧邻 CUDA Core / FP32 ALU，通常标为 &quot;Register File&quot;。</li>\n</ul>\n<h2 id=\"b-shared-memory共享内存\"><a class=\"anchor\" href=\"#b-shared-memory共享内存\">#</a> (b) Shared Memory（共享内存）</h2>\n<ul>\n<li>归属：每个 SM 独有</li>\n<li>分配单位：每个线程块（thread block）</li>\n<li>用途：线程块内线程间通信、数据重用（如矩阵分块）</li>\n<li>特点：</li>\n<li>速度极快（接近寄存器）</li>\n<li>容量小（通常 64KB–164KB per SM），可与 L1 缓存动态划分</li>\n<li>程序员显式管理（ <code>__shared__</code>  关键字）</li>\n<li>在架构图中的位置：<br>\n位于 SM 内部，与 Register File 并列，常标为 &quot;Shared Memory&quot; 或包含在 &quot;Unified Data Cache&quot; 模块中（因与 L1 共享物理存储）。</li>\n</ul>\n<h1 id=\"caches缓存\"><a class=\"anchor\" href=\"#caches缓存\">#</a> Caches（缓存）</h1>\n<h2 id=\"a-l1-cache一级缓存\"><a class=\"anchor\" href=\"#a-l1-cache一级缓存\">#</a> (a) L1 Cache（一级缓存）</h2>\n<ul>\n<li>归属：每个 SM 独有</li>\n<li>物理实现：与 Shared Memory 共享同一块 SRAM（称为 Unified Data Cache）</li>\n<li>用途：缓存 global memory 的数据（可配置为更多 L1 或更多 Shared Memory）</li>\n<li>在架构图中的位置：<br>\n通常与 Shared Memory 合并表示为 &quot;Unified Data Cache&quot; 或 &quot;L1/Shared Memory&quot; 模块，位于 SM 内部。</li>\n</ul>\n<h2 id=\"b-l2-cache二级缓存\"><a class=\"anchor\" href=\"#b-l2-cache二级缓存\">#</a> (b) L2 Cache（二级缓存）</h2>\n<ul>\n<li>归属：整个 GPU 共享</li>\n<li>用途：缓存所有 SM 对 global memory 的访问，减少 DRAM 访问次数</li>\n<li>容量：几 MB 到几十 MB（如 RTX 4090 有 72MB L2）</li>\n<li>在架构图中的位置：<br>\n位于 所有 SM 之外、GPU DRAM 之前，通常画成一个大的 &quot;L2 Cache&quot; 模块，连接所有 SM 和 Memory Controller。</li>\n</ul>\n<h2 id=\"c-constant-cache常量缓存\"><a class=\"anchor\" href=\"#c-constant-cache常量缓存\">#</a> (c) Constant Cache（常量缓存）</h2>\n<ul>\n<li>归属：每个 SM 独有</li>\n<li>用途：缓存标记为  <code>__constant__</code>  的只读数据（如 kernel 参数）</li>\n<li>特点：小容量、广播式访问、低延迟</li>\n<li>在架构图中的位置：<br>\n通常在 SM 内部单独标出，或作为 L1 之外的一个小模块，标为 &quot;Constant Cache&quot;。</li>\n</ul>\n<h1 id=\"unified-memory统一内存\"><a class=\"anchor\" href=\"#unified-memory统一内存\">#</a> Unified Memory（统一内存）</h1>\n<ul>\n<li>注意：这不是一种 “物理内存”，而是一种编程模型 + 硬件 / 运行时支持机制</li>\n<li>作用：让 CPU 和 GPU 使用同一个指针访问数据，系统自动迁移数据</li>\n<li>底层仍使用：System Memory + Global Memory（物理上仍是两块）</li>\n<li>在架构图中：不对应具体硬件模块，但依赖 统一虚拟地址空间（Unified Virtual Addressing, UVA） 和 页迁移引擎（Page Migration Engine），这些通常由 MMU（内存管理单元） 和 IOMMU 支持，在高级架构图中可能不显式画出。</li>\n</ul>\n<h1 id=\"总结\"><a class=\"anchor\" href=\"#总结\">#</a> 总结：</h1>\n<p><img loading=\"lazy\" data-src=\"1768573153283.jpg\" alt=\"总结图\"></p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，并指明哪一篇博客，看到一定及时回复！</p>\n",
            "tags": [
                "AI",
                "CUDA"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2026/01/16/CUDA01/",
            "url": "https://forcheetah.github.io/2026/01/16/CUDA01/",
            "title": "【CUDA C++】GPU基本介绍【1】",
            "date_published": "2026-01-16T14:14:58.167Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇是介绍 CUDA C++ 的第一篇。资料来源于 <a href=\"https://docs.nvidia.com/cuda/cuda-programming-guide/contents.html\">官网 CUDA Programming Guide</a>。本文会比官网教程简洁一些，去掉一些我不太感兴趣的内容（任性）。</p>\n<p>参考  <a href=\"https://docs.nvidia.com/cuda/cuda-programming-guide/contents.html\">官网 CUDA Programming Guide</a>。</p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"gpu-和-cpu-对比\"><a class=\"anchor\" href=\"#gpu-和-cpu-对比\">#</a> GPU 和 CPU 对比</h1>\n<p>在相同的价格和功耗范围内，GPU 的指令处理量和内存带宽都远高于 CPU。许多应用程序利用这些特性，在 GPU 上运行的速度要比在 CPU 上快得多（详见 “GPU 应用” 部分）。其他计算设备，如 FPGA，同样具有很高的能效，但相比 GPU，其编程灵活性要低得多。</p>\n<p>图形处理器（GPU）和中央处理器（CPU）的设计目标各不相同。CPU 的设计初衷是尽可能快速地执行一系列连续的操作（称为 “线程”），并且能够并行执行几十个这样的线程；而 GPU 则旨在并行执行数千个线程，以牺牲单线程性能为代价来实现更高的总吞吐量。</p>\n<p>图形处理器（GPU）专用于高度并行的计算，并在数据处理单元上投入更多晶体管，而中央处理器（CPU）则在数据缓存和流程控制方面投入更多晶体管。下图 展示了 CPU 与 GPU 在芯片资源分配方面的示例情况。</p>\n<p><img loading=\"lazy\" data-src=\"1768572721571.jpg\" alt=\"CPU 与 GPU\"></p>\n<p>利用 GPU 提供的计算能力有多种方法。本指南介绍了使用 C++ 等高级语言在 CUDA GPU 平台上进行编程的方法。然而，在不需要直接编写 GPU 代码的应用程序中，也有许多利用 GPU 的方式。</p>\n<p>通过专门的库，可以获取来自不同领域的不断增多的算法和程序集。当某个库已经实现（尤其是由 NVIDIA 提供的那些）时，使用该库通常比从头开始重新实现算法更具效率和性能。像 cuBLAS、cuFFT、cuDNN 和 CUTLASS 这样的库只是帮助开发人员避免重新实现成熟算法的几个例子。这些库还有一个额外的好处，即针对每种 GPU 架构进行了优化，从而提供了高效性、性能和可移植性的理想组合。</p>\n<p>此外，还有一些框架（尤其是那些用于人工智能的框架）提供了基于 GPU 加速的构建模块。这些框架的加速功能是通过利用上述提到的 GPU 加速库来实现的。</p>\n<p>此外，诸如 NVIDIA 的 Warp 或 OpenAI 的 Triton 这类特定领域的语言（DSL）能够编译并直接在 CUDA 平台上运行。这为对 GPU 进行编程提供了一种比本指南中所涵盖的高级语言更高层次的方法。</p>\n<h1 id=\"异构系统\"><a class=\"anchor\" href=\"#异构系统\">#</a> 异构系统</h1>\n<p>CUDA 编程模型假定存在一种异构计算系统，即包含图形处理器（GPU）和中央处理器（CPU）的系统。与 CPU 直接相连的内存被称为主机和主机内存。与 GPU 直接相连的内存则被称为设备和设备内存。在某些片上系统（SoC）中，这些可能共用一个封装。在更大的系统中，可能会有多个 CPU 或 GPU。</p>\n<p>CUDA 应用程序会在 GPU 上执行其部分代码，但应用程序总是从 CPU 开始执行。主机代码（即在 CPU 上运行的代码）可以使用 CUDA API 在主机内存和设备内存之间复制数据、启动在 GPU 上执行的代码，并等待数据复制或 GPU 代码完成。CPU 和 GPU 可以同时执行代码，并且通常通过最大限度地利用 CPU 和 GPU 的利用率来获得最佳性能。</p>\n<p>应用程序在 GPU 上执行的代码被称为设备代码，而用于在 GPU 上执行的函数则由于历史原因被称为内核。启动一个内核运行的过程被称为启动内核。内核启动可以理解为在 GPU 上同时启动许多执行内核代码的线程。内核启动类似于 CPU 上的线程，不过在正确性和性能方面存在一些重要差异。</p>\n<h1 id=\"gpu-硬件模型\"><a class=\"anchor\" href=\"#gpu-硬件模型\">#</a> GPU 硬件模型</h1>\n<p>与任何编程模型一样，CUDA 也依赖于对底层硬件的某种概念性模型。对于 CUDA 编程而言，GPU 可以被视为一组流式多处理器 Streaming Multiprocessors（SM），这些 SM 以称为图形处理集群 Graphics Processing Clusters（GPC）的组形式组织在一起。每个 SM 都包含一个本地寄存器文件 local register file、一个统一的数据缓存 unified data cache，以及执行计算的若干功能单元。统一的数据缓存为共享内存和 L1 缓存提供了物理资源。统一数据缓存的分配到 L1 和共享内存可以在运行时进行配置。不同类型的内存的大小以及每个 SM 内部的功能单元的数量在不同的 GPU 架构中可能会有所不同。</p>\n<p><img loading=\"lazy\" data-src=\"1768572794775.jpg\" alt=\"硬件模型\"></p>\n<p>上图是 CPU 和 GPU 的硬件架构对比图，可以看到总体结构类似。CPU 有主机内存 System DRAM，个人电脑常见的运行内存 8G、 16G、 32G。GPU 有他的显存 GPU DRAM，常见的 8G、12G 等，与主机内存处于同一水平，但是 GPU 显存带宽更高，速度更快。CPU 有三个缓存层级，L1 → L2 → L3，逐级变大变慢。而 GPU 为两层，L1 + 共享内存 → L2，没有 L3。CPU 有少量的核心数量，少（比如 4～16 个核心）。而 GPU 有大量的核心，成百上千个，这些核心被组织成 warp、block、cluster、grid、SM、GPC 等，后续会介绍到。</p>\n<h2 id=\"thread-blocks-和-grids\"><a class=\"anchor\" href=\"#thread-blocks-和-grids\">#</a> Thread Blocks 和 Grids</h2>\n<p>当一个应用程序启动内核时，它会使用大量线程，通常多达数百万个线程。这些线程被组织成不同的 block——Thread Block。Thread Block 被组织成一个 grid。grid 中的所有 Thread Block 都具有相同的大小和尺寸。</p>\n<p><img loading=\"lazy\" data-src=\"1768572820238.jpg\" alt=\"grid\"></p>\n<p>通过使用内置变量，每个执行内核的线程都能够确定其所在所在的包含 Block 的位置，以及其所在的包含 Grid 中的位置。线程还可以利用这些内置变量来确定 Thread Block 的尺寸以及内核被启动时所在的 grid 的尺寸。这使得每个线程在运行内核的所有线程中都具有独特的身份。这种身份通常用于确定某个线程负责处理哪些数据或执行哪些操作。</p>\n<p>一个 Thread Block 中的所有线程都在一个单一的 SM 中执行。这使得 Thread Block 内的线程能够高效地进行通信和同步。Thread Block 内的所有线程都能访问片上共享内存，该内存可用于 Thread Block 内各线程之间的信息交换。</p>\n<p>一个 grid 可能由数百万个 Thread Block 组成，而执行该 Grid 的 GPU 只可能拥有几十或几百个 SM（流处理器）。一个 Thread Block 中的所有线程都由一个单独的 SM 执行，并且在大多数情况下，这些线程会在该 SM 上完成运行。Thread Block 之间没有调度保证，因此一个 Thread Block 不能依赖其他 Thread Block 的结果，因为这些结果可能要等到该 Thread Block 完成之后才能被调度。下图展示了 grid 中的 Thread Block 如何被分配到一个 SM 上的示例。</p>\n<p><img loading=\"lazy\" data-src=\"1768572836438.jpg\" alt=\"SM\"></p>\n<p>每个 SM 中都有一个或多个活跃的 Thread Block。在本示例中，每个 SM 同时调度了三个 Thread Block。对于 Grid 中的 Thread Block 分配到 SM 的顺序，并没有任何保证。</p>\n<p>除了线程块之外，具有 compute capability 9.0  及更高版本的 GPU 还有一种可选的分组方式，称为 “Cluster”。Cluster 是由一组 Thread Block 组成的，与 Thread Block 和 Grid 一样，可以以 1 维、2 维或 3 维的形式排列。图 5 展示了一个网格化的 Thread Block，它被组织成了 Cluster。指定 Cluster 并不会改变 Grid 的尺寸或 Grid 内 Thread Block 的索引。</p>\n<p><img loading=\"lazy\" data-src=\"1768572854711.jpg\" alt=\"Cluster\"></p>\n<p>将相邻的 Thread Block 划分成 clusters ，并在 clusters 级别提供了更多的同步和通信机会。具体而言，一个 clusters 中的所有线程块都在单个 GPC 中执行。图 6 展示了在指定 clusters 时，Thread Block 如何在 GPC 中被分配到 SM 上。由于 Thread Block 是同时在单个 GPC 中进行调度的，因此同一 cluster 内的不同 Thread Block 中的线程可以使用由协作组提供的软件接口进行通信和同步。Cluster 中的线程可以访问 Cluster 中所有 Thread Block 的共享内存，这被称为分布式共享内存。Cluster 的最大大小取决于硬件，并且在不同的设备中有所不同。</p>\n<p><img loading=\"lazy\" data-src=\"1768572868342.jpg\" alt=\"SM cluster\"></p>\n<h2 id=\"warps-和-simt\"><a class=\"anchor\" href=\"#warps-和-simt\">#</a> Warps 和 SIMT</h2>\n<p>在一个线程块内，线程被组织成由 32 个线程组成的组，这些组被称为 “warps”。一个 warp 按照 “单指令多线程”（SIMT）模式执行内核代码。在 SIMT 模式中，warp 中的所有线程都在执行相同的内核代码，但每个线程可能会根据不同的分支路径来执行代码。也就是说，尽管程序的所有线程都执行相同的代码，但这些线程并不需要遵循相同的执行路径。</p>\n<p>当线程由一个 warp 执行时，它们会被分配一个线程组通道 warp lane。warp lane 的编号范围为 0 到 31，而来自一个线程块的线程会按照硬件多线程中所详述的可预测方式分配到 warp 中。</p>\n<p>在 warp 中的所有线程会同时执行相同的指令。如果一个 warp 中的某些线程在执行过程中遵循了控制流分支，而其他线程没有这样做，那么不遵循分支的线程将会被屏蔽掉，而遵循分支的线程则会被执行。例如，如果一个条件仅在 warp 中的半数线程中为真，那么另一半织线中的线程将会被屏蔽掉，而活跃的线程则会执行那些指令。这种情况如下图所示。当一个 warp 中的不同线程遵循不同的代码路径时，这有时被称为 warp 分歧。因此，当 warp 中的线程遵循相同的控制流路径时，GPU 的利用率会达到最大化。</p>\n<p><img loading=\"lazy\" data-src=\"1768572880609.jpg\" alt=\"warp lanes\"></p>\n<p>在 SIMT 模型中，同一线程 warp 中的所有线程会同步地在内核中执行。硬件执行方式可能有所不同。有关此区别在何处重要的更多信息，请参阅 “独立线程执行” 部分。利用了解线程 warp 实际映射到实际硬件的方式的知识是不被提倡的。CUDA 编程模型和 SIMT 表示，同一线程 warp 中的所有线程会一起执行代码。只要遵循编程模型，硬件就可以以对程序不可见的方式优化掩码通道。如果程序违反了这个模型，这可能会导致未定义的行为，且这种行为在不同的 GPU 硬件上可能有所不同。</p>\n<p>虽然在编写 CUDA 代码时无需考虑线程组（warp）的问题，但了解线程组的执行模型有助于理解诸如全局内存协同和共享内存 bank 访问模式等概念。一些高级编程技术会利用线程块内线程 warp 的特化来限制线程的分歧并最大限度地提高利用率。这些以及其他优化措施都利用了在执行时线程会被分组为线程 warp 这一事实。</p>\n<p>线程执行的一个影响是，线程块的最佳设定应使其总线程数为 32 的倍数。可以使用任意数量的线程，但当总数不是 32 的倍数时，线程块的最后一组线程在执行过程中会有部分线程处于未使用状态。这很可能会导致该线程 warp 的功能单元利用率和内存访问效率降低。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，并指明哪一篇博客，看到一定及时回复！</p>\n",
            "tags": [
                "AI",
                "CUDA"
            ]
        }
    ]
}
{
    "version": "https://jsonfeed.org/version/1",
    "title": "Пусть этот камень будет более крепким, чем человек",
    "description": "有自己的博客很帅，但是我很懒，要命！！！",
    "home_page_url": "https://forcheetah.github.io",
    "items": [
        {
            "id": "https://forcheetah.github.io/2024/06/27/conv3/",
            "url": "https://forcheetah.github.io/2024/06/27/conv3/",
            "title": "【im2col】昇腾卷积加速算法",
            "date_published": "2024-06-27T15:21:34.031Z",
            "content_html": "<h1 id=\"前置信息\"><a class=\"anchor\" href=\"#前置信息\">#</a> 前置信息</h1>\n<p><strong>（1）本文讲解使用的例子</strong></p>\n<p>以如下的卷积为例，进行昇腾 Im2Col 卷积过程：</p>\n<ul>\n<li>Input 输入维度为 NHWC ：【2，25，25，17】</li>\n<li>外圈蓝色代表 pad</li>\n<li>Kernal 维度为  CCHkWk  ：【34，17，3，3】</li>\n<li>操作为 3*3 卷积 pad=1, Group=1, Stride=1， 2D 卷积</li>\n<li>得到输出的维度 为 NHWC : 【22，25，25，18】</li>\n</ul>\n<p>从图上可以轻易看出相关信息。</p>\n<p><img loading=\"lazy\" data-src=\"1719500181052.jpg\" alt=\"例子\"></p>\n<blockquote>\n<p>现在想起来，光是遇到你这个家伙，就感觉自己赚到了。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<p><strong>（2）矩阵乘运算单元</strong></p>\n<p>昇腾达芬奇架构设计了 16*16 的矩阵乘运算单元，能够提供强大的并行乘加计算能力，可以以一条指令实现两个 16*16 的矩阵相乘的运算。所以昇腾 Im2Col 卷积的目的就是让卷积能够高效地利用 “矩阵乘运算单元” 进行计算。</p>\n<p><img loading=\"lazy\" data-src=\"1719500301302.jpg\" alt=\"davincii\"></p>\n<p>感兴趣的可以阅读昇腾架构介绍书籍。</p>\n<blockquote>\n<p>矩阵计算单元可以⽤⼀条指令完成两个 16×16 矩阵的相乘运算（标记为 16<sup>3，也是 Cube 这⼀名称的来历），等同于在极短时间内进⾏了 16</sup>3＝4096 个乘加运算，并且可以实现 FP16 的运算精度。如图 3-7 所⽰，矩阵计算单元在完成 C＝A×B 的矩阵运算时，会事先将矩阵 A 按⾏存放在输⼊缓冲区中，同时将矩阵 B 按列存放在输⼊缓冲区中，通过矩阵计算单元计算后得到的结果矩阵 C 按⾏存放在输出缓冲区中。在矩阵相乘运算中，矩阵 C 的第⼀元素由矩阵 A 的第⼀⾏的 16 个元素和矩阵 B 的第⼀列的 16 个元素由矩阵计算单元⼦电路进⾏ 16 次乘法和 15 次加法运算得出。矩阵计算单元中共有 256 个矩阵计算⼦电路，可以由⼀条指令并⾏完成矩阵 C 的 256 个元素计算。                                          摘自《昇腾 AI 处理器架构与编程》</p>\n</blockquote>\n<h1 id=\"权重排布\"><a class=\"anchor\" href=\"#权重排布\">#</a> 权重排布</h1>\n<p>昇腾 Im2Col 五维卷积加速算法   基本流程：</p>\n<p>输入为 nhwc 输出为 nhwc</p>\n<p>权重维度变化： 权重的维度变化离线进行，不消耗神经网络推理时间。（神经网络推理大致分为 模型转换 量化 推理三个步骤，权重的维度转换可以在模型转换时进行，不占用推理的时间）。下面是权重变换的分步流程，代码实现可以一步完成，也可以分多步完成（因为不影响推理时间。）</p>\n<p><img loading=\"lazy\" data-src=\"1719500423112.jpg\" alt=\"weight change\"></p>\n<p>上方的变换如果比较抽象的话，可以结合后面的流程来理解。</p>\n<h2 id=\"权重-从kernel-4d变换到kernel-2d\"><a class=\"anchor\" href=\"#权重-从kernel-4d变换到kernel-2d\">#</a> 权重 从 kernel 4D 变换到 kernel 2D</h2>\n<p><img loading=\"lazy\" data-src=\"1719500512598.jpg\" alt=\"weight change\"></p>\n<p><img loading=\"lazy\" data-src=\"1719500556415.jpg\" alt=\"weight change2\"></p>\n<p>上图是 Kernel 2D 的数据排布方式，维度为【2*3*3*16，34】，为了简便，跳过昇腾 5D 结构，直接从 4D 转到 2D。下面介绍 4D 数据和 2D 数据的一一对应关系。</p>\n<ul>\n<li>D 图 ① 覆盖区域表示 一个卷积核【17，3，3】展开成 2D 中的一列。对应于 A 图中一整个卷积核。34 个卷积核将展开为 34 列。因此每列代表一个卷积核。</li>\n<li>B 图，卷积核通道数为 17，需要补零为 16 的倍数 32，并拆分成 2 块（分别是紫色、黄色）。E 图：每一列（每一个卷积核）的紫色部分②是卷积核通道方向拆分的第一块（B 图中的紫色），黄色部分③是拆分的第二块（B 图中的黄色）。</li>\n<li>拆分的每一块（比如紫色部分）又分成 3*3（kernel 行 * 列），F 图: ④覆盖的是 kernel 第一行 (对应于 C 图中的④的部分)，⑤覆盖的是 kernel 第二行（对应于 C 图中⑤的部分），相似的⑥覆盖的是 kernel 第三行（对应于 C 图中⑥的部分）。3*3 卷积核一共就三行</li>\n<li>每一个紫色的小方格代表通道方向的 16 个数。</li>\n<li>至此，kernel 4D 和 kernel 2D 所有的数据都一一对应了。例如 F 图中：⑦代表第 6 个卷积核、通道拆分的第二块、第一行、第二列、通道方向的 16 个数。</li>\n</ul>\n<p>通过上述对应关系，我们不难得到维度为【2*3*3*16，34】的卷积核 2D 形式。由于昇腾卷积算法的 AI 计算核心是 16*16 的矩阵乘运算单元，同时为了取数方便，还需要将卷积核 2D 转换为大 Z 小 N 排布方式。</p>\n<h2 id=\"权重-从kernel2d变换到大z小n\"><a class=\"anchor\" href=\"#权重-从kernel2d变换到大z小n\">#</a> 权重 从 kernel2D 变换到大 Z 小 N</h2>\n<p><img loading=\"lazy\" data-src=\"1719500684744.jpg\" alt=\"2d\"></p>\n<p>第一步，将 2D【2*3*3*16，34】中 34 补零为 16 的倍数，即 48，得到【2*3*3*16，48】。</p>\n<p>第二步，将其按照 16*16 的方格进行划分，得到【2*3*3，3】个【16，16】的小块。（图中画成了 4 个小块，实际应该是 3 个，示意图，见谅）</p>\n<p>第三步，将这些小块按照大 Z 小 N 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube8 这种 “Z” 字形排布；小 N 指的是内部按照列优先，即每个 16*16 的 Cube，先排第一列，然后是第二列...  详见最右边的彩色表示。</p>\n<p>多说一句，之所以专门按照 “小 N” 排布，是因为在矩阵运算中，权重作为矩阵乘的第二个参数，数据是按列取的。这就意味着在实际内存中要跳着取数（内存中都是按照行优先排序），自然效率低。提前将其按照列优先的方式进行排布，那么在矩阵乘运算中可以连续取数。至此，我们得到了 【2*3*3，3，16*16】的权重大 Z 小 N 排布形式，这种形式使得能够一次性取出 256 个数参与计算，效率很高。</p>\n<p>下面的代码一次性完成了 权重 4D nhwc  到权重大 Z 小 N 排布，仅供参考。还是那句话，权重的变换离线进行，不占用宝贵的推理时间，所以无须关心转换的效率。完整代码可以下载 <a href=\"https://github.com/ForCheetah/ConvAccelerate\">加速算法模拟</a>，并运行其中的  <code>TestAscendConvLayer();</code>  函数。可以看到三个测试函数，它们的区别在于不同的输入排布方式。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayer();</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token comment\">//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNCHW();</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token comment\">//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNHWC();</span></pre></td></tr></table></figure><pre><code>void WeightTrans_A(const float* filters, const TensorDim weight_dim, Ascend5Dim we_5D_dim, float* we_tran5D, \n            AscendTransform5Dim we_tran5D_dim, int CUBE_row, int CUBE_col)&#123;\n    int lastdim4 = we_tran5D_dim.move * we_tran5D_dim.channel * we_tran5D_dim.LW * we_tran5D_dim.cube;\n    int lastdim3 = we_tran5D_dim.channel * we_tran5D_dim.LW * we_tran5D_dim.cube;\n    int lastdim2 = we_tran5D_dim.LW * we_tran5D_dim.cube;\n    int single_filter_num = weight_dim.c * weight_dim.h * weight_dim.w;\n    int single_filter_channel = weight_dim.h * weight_dim.w;\n\n    for(int ch_cube=0; ch_cube&lt;we_tran5D_dim.batch; ch_cube++)&#123;  //通道方向块   ch_cube\n        int index_1 = ch_cube * lastdim4;\n        for(int hk=0; hk&lt;we_tran5D_dim.move; hk++)&#123;  // filter 长  \n            int index_2 = index_1 + hk * lastdim3;\n            for(int wk=0; wk&lt;we_tran5D_dim.channel; wk++)&#123;  // filter 宽\n                int index_3 = index_2 + wk * lastdim2;\n                for(int cout_cube=0; cout_cube&lt;we_tran5D_dim.LW; cout_cube++)&#123; // cout方向块 \n                    int index_4 = index_3 + cout_cube*we_tran5D_dim.cube;\n                    for(int cube_row=0; cube_row&lt;CUBE_row; cube_row++)&#123;\n                        for(int cube_col=0; cube_col&lt;CUBE_col; cube_col++)&#123;\n                            int index = index_4 + cube_row*CUBE_col + cube_col;                       \n                            if((cout_cube*CUBE_col+cube_row)&gt;=weight_dim.n  || (ch_cube*CUBE_col+cube_col)&gt;=weight_dim.c)&#123;\n                                we_tran5D[index] = 0;\n                            &#125;else&#123;\n                                // 第几个filter  第几个通道  第几行  第几列  还要注意 大Z小N排布方式     大Z小N排布方式（行变列，列变行）\n                                int index_from = (cout_cube*CUBE_col+cube_row)*single_filter_num + (ch_cube*CUBE_col+cube_col)*single_filter_channel + hk*weight_dim.w+ wk;                                \n                                we_tran5D[index] = filters[index_from];\n                            &#125;  \n                        &#125;\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1 id=\"输入排布\"><a class=\"anchor\" href=\"#输入排布\">#</a> 输入排布</h1>\n<p>输入 tensor 的内存排布为 nhwc 输出为 nhwc</p>\n<p>昇腾算法的维度详细变换如图下图所示。这里展示了输入 input 从 4D 维度转换到 昇腾 5D 结构，然后再转换到 2D 结构，最后转换到大 Z 小 Z 维度。写这么详细只是为了方便读者理解，而在实际操作中，由于 Input 的变换是在线进行，消耗宝贵的推理时间，所以如华为昇腾书中所说：input 先是从 4D 维度 通过软件算法转换为 昇腾 5D 维度（在模型推理过程中这一步可能不需要，因为中间层的 tensor 已经处于昇腾 5D 维度了），之后从昇腾 5D 维度通过 硬件直接转换到大 Z 小 Z 排布（模型推理过程肯定是边转换变计算，所以不会将整个 tensor 转换为大 Z 小 Z 之后，才进行矩阵运算阶段的。本博客为方便，将整个 tensor 完全转换到大 Z 小 Z，再进行后面计算。）</p>\n<p>说完这些，就可以介绍一下昇腾算法极致高效的输入的排布转换过程了！</p>\n<p><img loading=\"lazy\" data-src=\"1719500920323.jpg\" alt=\"input\"></p>\n<h2 id=\"输入-从input-4d-到input-5d\"><a class=\"anchor\" href=\"#输入-从input-4d-到input-5d\">#</a> 输入 从 Input 4D 到 Input 5D</h2>\n<p>还是再强调一下，昇腾可以做到整个模型的中间层的 tensor 均保持昇腾 5D 的维度，所以思考一下，可能只有最初输入到模型的 tensor 需要 从 Input 4D 转 到 Input 5D，或者再数据预处理的时候就将数据处理为 5D 排布。</p>\n<p><img loading=\"lazy\" data-src=\"1719500976081.jpg\" alt=\"trans6\"></p>\n<ul>\n<li>G 图是最原始的 Input4D 结构，当然，batch 维度 N=2 没有画，只画了一个。它的维度是【25，25，17】</li>\n<li>H 图为昇腾 5D 结构图，首先要将通道方向的 17 补齐为 16 的倍数 32，同时每 16 个进行一次拆分，拆成两组。</li>\n<li>最后注意一下数据的排布顺序就好了：注意 5D 结构中，K_cube 位于最内层，这些数据是连续的，所以先把 高 h=1, 宽 w=1 位置的 16 个数据排在一起。</li>\n<li>紧接着将宽度方向 25 个 K_cube 排在一起，变成 25*16</li>\n<li>然后再遍历高的方向。变成 25*25*16</li>\n<li>最后是遍历两组，得到昇腾的 5D 结构【2，25，25，16】</li>\n</ul>\n<p>此处数据搬运较为简单，可以参考代码<a href=\"https://github.com/ForCheetah/ConvAccelerate\">加速算法模拟</a></p>\n<h2 id=\"输入-从input-5d-直接搬到-大z小z\"><a class=\"anchor\" href=\"#输入-从input-5d-直接搬到-大z小z\">#</a> 输入 从 Input 5D 直接搬到 大 Z 小 Z</h2>\n<p>昇腾通过专门设计的硬件，将 input 从 5D 格式直接搬到 大 Z 小 Z 排布。想要知道怎么搬以及为什么这么搬，还真不得不把其 2D 排布讲明白。  《昇腾 AI 处理器架构与编程》这本书中直接跳过了 2D 排布，导致晦涩难懂。</p>\n<h3 id=\"input-5d-到-input-2d\"><a class=\"anchor\" href=\"#input-5d-到-input-2d\">#</a> Input 5D 到 Input 2D</h3>\n<p>所以我们直接看 Input2D 与 Weight 2D 的对应情况，如下图所示。</p>\n<p><img loading=\"lazy\" data-src=\"1719501086578.jpg\" alt=\"trans5\"></p>\n<ul>\n<li>J 图为 input2D 【25*25，2*3*3*16】   K 图为 Weight2D 【2*3*3*16，34】。再回忆一下 Weight2D 数据每一行和每一列的数据的意义，它的一列数据 2*3*3*16 代表什么呢？  2*3*3*16 代表一整个卷积核，2 代表该卷积核通道方向拆成两块，那么 3*3*16 就是每一块的 高 * 宽 * K_cube。</li>\n<li>好巧！Input2D 的一行也是 2*3*3*16！（废话，不一样就没法算了）。既然 weight2D 一列数据的意义一清二楚，那么对应的 Input2D 数据一行的意义也就呼之欲出啦！ Input2D 的一行 就是卷积核在某个滑动窗口位置对应的 input 数据。例如，Input2D 的第一行，就对应于 I 图 3*3 的彩色窗口数据（没有 Pad 的情况下）。</li>\n<li>也就可以推知，Input2D 的每一行绿色部分，就是 I 图通道方向拆分的第一块（拆分的绿色部分）；每一行的的蓝色部分，就是 I 图通道防线拆分的第二块（中间深蓝宽度 1，和补齐的浅蓝 15）</li>\n<li>那么，为什么 Input2D 有足足 625 行呢？因为滑动窗口纵向滑动 25 次，每次纵向滑动，都包含横向的 25 次，总共 625 次。</li>\n</ul>\n<p>假如直接计算 Input2D 矩阵乘 Weight2D，卷积计算就得到最终结果啦！这就是普通的 Im2Col 算法，不清楚的小伙伴们还可以去读一下 <a href=\"https://forcheetah.github.io/2024/05/23/conv1/\">Im2Col 算法 NCHW</a> 和 <a href=\"https://forcheetah.github.io/2024/05/23/conv2/\">Im2Col 算法 NHWC</a>。</p>\n<p>从 2D 的角度来看，算法是不是很简单啊。</p>\n<p>不要高兴的太早，还没完呢。</p>\n<h3 id=\"input-2d-到-大z小z\"><a class=\"anchor\" href=\"#input-2d-到-大z小z\">#</a> Input 2D 到 大 Z 小 Z</h3>\n<p><img loading=\"lazy\" data-src=\"1719501234541.jpg\" alt=\"trans4\"></p>\n<p>接下来是将 Input2D 转换到大 Z 小 Z 排布</p>\n<p>第一步，将 Input2D【25*25，2*3*3*16】中 25*25 补零为 16 的倍数，即 640，得到【640，2*3*3*16】  ，如图 L。</p>\n<p>第二步，将其按照 16*16 的方格进行划分，即得到【40，18】个【16，16】的小块，如图 M。</p>\n<p>第三步，将这些小块按照大 Z 小 Z 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube720 这些块按照 “Z” 字形排布；像 N 图上方排成一行；小 Z 指的是内部也按照行优先，即每个 16*16 的 Cube，先排第一行，然后是第二行... 详见 N 图中的颜色表示。</p>\n<p><img loading=\"lazy\" data-src=\"1719501296597.jpg\" alt=\"trans3\"></p>\n<p>上图来自《昇腾 AI 处理器架构与编程》，矩阵 A 的排布为大 Z 小 Z，矩阵 B 的排布为大 Z 小 N，大家可以再理解一下。</p>\n<p>至此，Input 的大 Z 小 Z 排布已经实现，接下来就是 16*16 的矩阵乘了。</p>\n<p><img loading=\"lazy\" data-src=\"1719501324297.jpg\" alt=\"trans2\"></p>\n<ul>\n<li>Input 现在是【40，18】个【16，16】小块，如左图，当然，它现在处于大 Z 小 Z 的一维排布。</li>\n<li>Weight 现在是 【18，3】个【16，16】小块，如中间图，当然，它现在处于大 Z 小 N 的一维排布。</li>\n<li>不知道分块矩阵乘的小伙伴可以再搜索下 《线性代数》中的分块矩阵乘运算。</li>\n<li>内部，进行两个 16*16 块的矩阵乘运算，由于 weight 已经按照列优先进行排布，所以矩阵乘的顺序如上图最右边所示。</li>\n<li>外部，对【40，18】和【18，3】做矩阵乘运算。</li>\n<li>至此，我们得到了【640，18】的矩阵。</li>\n<li>然后将上图两图灰色部分对应的多余数据裁掉，就得到了卷积结果【25，25，34】 ，当然，还得遍历一下 batch，得到【2，25，25，34】</li>\n</ul>\n<h3 id=\"input5d搬到大z小z\"><a class=\"anchor\" href=\"#input5d搬到大z小z\">#</a> Input5D 搬到大 Z 小 Z</h3>\n<p>前两小节介绍了 Input5D 变换到 Input 2D，再变换到 大 Z 小 Z 的过程。而在昇腾芯片中，从 Input5D 到 Input2D 由硬件一步实现。</p>\n<p>如果前面两小节已经看明白了的话，那么搬运的秘密就呼之欲出了。</p>\n<p><img loading=\"lazy\" data-src=\"1719501381618.jpg\" alt=\"trans1\"></p>\n<ul>\n<li>看上图，左图是 Input 的 5D 维度排布【2，25，25，16】，右边是 Input 2D 排布【25*25，2*3*3*16】。中间是个滑动窗口示意图，3*3，因为本文中用的例子就是 3*3 卷积。</li>\n<li>回忆一下右边 2D 排布的数据的意义，每一个小格子是通道方向的 16 个数，每一行是滑动窗口每一个位置对应的 2*3*3*16 个数。滑动窗口纵向滑动 25 次，每次要横向滑动 25 次，所以有 625 行数据，再加上补齐的 15 行，才达到了 640 行数据。</li>\n<li>那么右图红色 1 的位置是滑动窗口 a 在第一个位置所对应的 16 个数字；红色 2 的位置是滑动窗口 a 横向滑动一次对应的 16 个数字；红色 3 的位置是滑动窗口 a 横向滑动第三次对应的 16 个数字；依次类推，红色 16 的位置是滑动窗口横向滑动第 16 次对应的 16 个数字。这 16 次滑动，滑动窗口的 a 在左图从 1 滑倒 16！</li>\n<li>也就是说，右图红色框的 1-16 与左图 1-16 一一对应！</li>\n<li>再来回忆一下，左图中 1-16 这 16*16 的数据是连续的吗？是！（不清楚的再回去看 Input 的维度变换）</li>\n<li>那么右图中的 1-16 这 16*16 个数据是连续的吗？它是！ 根据大 Z 小 Z 排布，这红色框中 16*16 的数据刚好被分到一个小 Cube 中！</li>\n<li>昇腾能够从 Input5D 中一次性拷贝 256 个数据到大 Z 小 Z 排布！</li>\n</ul>\n<h1 id=\"代码模拟\"><a class=\"anchor\" href=\"#代码模拟\">#</a> 代码模拟</h1>\n<p>当然，我猜测昇腾应该是设计了 16 个 DMA 组成的 DAM 队列，来实现一次 256 个数据的搬运。真的是相当高效了！</p>\n<p>我提供了 C 语言代码模拟整个昇腾的卷积运算流程。完整代码可以在 <a href=\"https://github.com/ForCheetah/ConvAccelerate\">加速算法模拟</a>下载，该工程提供了以下三个测试函数，它们的区别在于不同的输入排布方式。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayer();</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token comment\">//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNCHW();</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token comment\">//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token comment\">// TestAscendConvLayerNHWC();</span></pre></td></tr></table></figure><p>还要再提一句，该工程中采用 C 语言函数 memcpy () 来模拟昇腾的批量数据拷贝功能。数据搬运中并不是所有的情况都是 256 个数据内存连续的，所以可以看到代码运行中分两次、三次才能拷贝完 256 个数据的情况。昇腾硬件中设计的 DMA 队列不会出现这种问题。此外，硬件肯定设计为边搬运边计算的工作模式，不会像我工程中完全得到 Input 大 Z 小 Z 排布再进行矩阵运算。</p>\n<p>文章好长啊！画了好多图！</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "Linux",
                "openBlas"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/06/18/deployTVMPython/",
            "url": "https://forcheetah.github.io/2024/06/18/deployTVMPython/",
            "title": "【TVM】Python脚本实现模型编译和保存",
            "date_published": "2024-06-18T13:01:39.606Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇博客提供简单的 Python 脚本代码，实现 onnx 模型转换编译，保存为 TVM 的  <code>.so .params .json</code>  文件 。</p>\n<blockquote>\n<p>望长城内外，惟余莽莽；大河上下，顿失滔滔。<br>\n--------------- 教员<br>\n ------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"python脚本实现模型编译和保存\"><a class=\"anchor\" href=\"#python脚本实现模型编译和保存\">#</a> Python 脚本实现模型编译和保存</h1>\n<p>脚本中需要修改的就一些路径，很容易看明白，就不再过多介绍了。</p>\n<pre><code class=\"language-Python\">\nimport onnx\nfrom tvm.contrib.download import download_testdata\nfrom PIL import Image\nimport numpy as np\nimport tvm.relay as relay\nimport tvm\nfrom tvm.contrib import graph_executor\n\n\n# 图片\nimg_path = &quot;../image/imagenet_cat.png&quot;\n# img_url = &quot;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&quot;\n# img_path = download_testdata(img_url, &quot;../image/imagenet_cat.png&quot;, module=&quot;data&quot;)\n\n# 重设大小为 224x224\nresized_image = Image.open(img_path).resize((224, 224))\nimg_data = np.array(resized_image).astype(&quot;float32&quot;)\n\n# 输入图像是 HWC 布局，而 ONNX 需要 CHW 输入，所以转换数组\nimg_data = np.transpose(img_data, (2, 0, 1))\n\n# 根据 ImageNet 输入规范进行归一化\nimagenet_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\nimagenet_stddev = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\nnorm_img_data = (img_data / 255 - imagenet_mean) / imagenet_stddev\n\n\n# 添加 batch 维度，期望 4 维输入：NCHW。\nimg_data = np.expand_dims(norm_img_data, axis=0)\n# 保存为 bin 文件  \nnorm_img_data.astype(&quot;float32&quot;).tofile(&quot;../image/imagenet_cat.bin&quot;)\n\n\n# 目标设备配置\ntarget = 'llvm'  # 以CPU为例\n\ninput_name = &quot;data&quot;\nshape_dict = &#123;input_name: img_data.shape&#125;\n\nonnx_model = onnx.load(&quot;../model/simple.onnx&quot;)\n\nmod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n\nwith tvm.transform.PassContext(opt_level=3):\n    lib = relay.build(mod, target=target, params=params)\n\n\n# 运行相关\ndev = tvm.device(str(target), 0)\nmodule = graph_executor.GraphModule(lib[&quot;default&quot;](dev))\n\n# 保存库文件\nlib_fname = &quot;../lib/mod.so&quot;\nlib.export_library(lib_fname)\n\n# 保存模型参数\nparams_fname = &quot;../lib/mod.params&quot;\nwith open(params_fname, &quot;wb&quot;) as param_file:\n    param_file.write(relay.save_param_dict(lib.get_params()))\n\n# 保存JSON格式的计算图\njson_fname = &quot;../lib/mod.json&quot;\nwith open(json_fname, &quot;w&quot;) as json_file:\n    json_file.write(lib.get_executor_config())\n\ndtype = &quot;float32&quot;\nmodule.set_input(input_name, img_data)\nmodule.run()\noutput_shape = (1, 10)\ntvm_output = module.get_output(0, tvm.nd.empty(output_shape)).numpy()\n\nfrom scipy.special import softmax\n\n# 下载标签列表\nlabels_url = &quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;\nlabels_path = download_testdata(labels_url, &quot;synset.txt&quot;, module=&quot;data&quot;)\n\nwith open(labels_path, &quot;r&quot;) as f:\n    labels = [l.rstrip() for l in f]\n\n# 打开输出文件并读取输出张量\nscores = softmax(tvm_output)    #   直接输出模型结果\nscores = np.squeeze(tvm_output)\nranks = np.argsort(scores)[::-1]\nfor rank in ranks[0:5]:\n    print(&quot;class='%s' with probability=%f&quot; % (labels[rank], scores[rank]))\n\n</code></pre>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "tvm",
                "cmake",
                "runtime"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/06/16/engine2/",
            "url": "https://forcheetah.github.io/2024/06/16/engine2/",
            "title": "【推理引擎】常见AI推理框架",
            "date_published": "2024-06-16T12:28:49.173Z",
            "content_html": "<h1 id=\"昇腾\"><a class=\"anchor\" href=\"#昇腾\">#</a> 昇腾</h1>\n<p><img loading=\"lazy\" data-src=\"1718540428003.jpg\" alt=\"shengteng\"></p>\n<p><a href=\"https://www.hiascend.com/zh/document\">昇腾官方文档</a></p>\n<p>华为昇腾（HUAWEI Ascend）是华为公司推出的一系列人工智能处理器，标志着华为在自主设计人工智能（AI）芯片领域的重要进展。昇腾芯片基于华为自主研发的达芬奇架构，这一架构设计旨在优化 AI 计算的效率和性能，特别是针对深度学习任务进行了专门优化。</p>\n<p>还可以去找一下 《昇腾 AI 处理器架构与编程：深⼊理解 CANN 技术原理及应⽤》等相关书籍。</p>\n<h1 id=\"ncnn\"><a class=\"anchor\" href=\"#ncnn\">#</a> NCNN</h1>\n<p><img loading=\"lazy\" data-src=\"1718540505840.jpg\" alt=\"ncnn\"></p>\n<p><a href=\"https://github.com/Tencent/ncnn\">腾讯 NCNN</a></p>\n<p>ncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。 ncnn 从设计之初深刻考虑手机端的部署和使用。 无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。 基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行， 开发出人工智能 APP，将 AI 带到你的指尖。 ncnn 目前已在腾讯多款应用中使用，如：QQ，Qzone，微信，天天 P 图等。</p>\n<h1 id=\"tvm\"><a class=\"anchor\" href=\"#tvm\">#</a> TVM</h1>\n<p><img loading=\"lazy\" data-src=\"1718540553346.jpg\" alt=\"tvm\"></p>\n<p><a href=\"https://github.com/apache/tvm\">TVM</a></p>\n<p>Apache TVM 是用于深度学习系统的编译器堆栈。它旨在缩小以生产力为重点的深度学习框架与以性能和效率为重点的硬件后端之间的差距。TVM 与深度学习框架合作，为不同的后端提供端到端编译。</p>\n<h1 id=\"tengine\"><a class=\"anchor\" href=\"#tengine\">#</a> Tengine</h1>\n<p><img loading=\"lazy\" data-src=\"1718540586818.jpg\" alt=\"tengine\"></p>\n<p><a href=\"https://github.com/OAID/Tengine\">Open AI Lib Tengine</a></p>\n<p>Tengine 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目使用 C 语言进行核心模块开发，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署，降低评估、迁移成本。</p>\n<p>尽管 Tengine 已经挺久不再维护了，但是作为一个完全使用 C 语言承担推理部分工作的框架，tengine 还是有轻量、框架结构清晰、易于拓展、容易学习、部署简单等优势，还是非常值得学习的。</p>\n<h1 id=\"地平线\"><a class=\"anchor\" href=\"#地平线\">#</a> 地平线</h1>\n<p><img loading=\"lazy\" data-src=\"1718540635472.jpg\" alt=\"horizon\"></p>\n<p><a href=\"https://developer.horizon.cc/api/v1/fileData/horizon_j5_open_explorer_cn_doc/index.html\">地平线征程五官方文档</a></p>\n<p>地平线 BPU（Brain Processing Unit）是地平线公司自主研发的专为智能驾驶及边缘计算场景设计的 AI 处理器架构。BPU 旨在通过软硬件深度结合，提供针对神经网络处理的高效解决方案，特别强调在自动驾驶、物联网、智能摄像头等领域的应用。地平线 J5 算法工具链（以下简称工具链）是一套完整的边缘计算平台算法落地解决方案，可以帮助您把浮点模型量化为定点模型， 并在地平线计算平台上快速部署自研算法模型。</p>\n<h1 id=\"tensorrt\"><a class=\"anchor\" href=\"#tensorrt\">#</a> TensorRT</h1>\n<p><img loading=\"lazy\" data-src=\"1718540670403.jpg\" alt=\"tensorrt\"></p>\n<p><a href=\"https://github.com/NVIDIA/TensorRT\">TensorRT</a></p>\n<p>NVIDIA TensorRT  是一个用于高性能深度学习推理的 API 生态系统。TensorRT 包括推理运行时和模型优化，可为生产应用程序提供低延迟和高吞吐量。TensorRT 生态系统包括 TensorRT、TensorRTLLM、TensorRT 模型优化器和 TensorRTCloud。</p>\n<h1 id=\"mnn\"><a class=\"anchor\" href=\"#mnn\">#</a> MNN</h1>\n<p><img loading=\"lazy\" data-src=\"1718540735181.jpg\" alt=\"mnn\"></p>\n<p><a href=\"https://github.com/alibaba/MNN\">alibaba MNN</a></p>\n<p>MNN 是一个高效、轻量级的深度学习框架。它支持深度学习模型的推理和训练，并在设备上具有业界领先的推理和培训性能。目前，MNN 已整合到阿里巴巴旗下淘宝、天猫、优酷、钉钉、闲鱼等 30 多个应用中，涵盖直播、短视频拍摄、搜索推荐、图片搜货、互动营销、权益分销、安全风控等 70 多个使用场景。此外，MNN 还用于嵌入式设备，如物联网。</p>\n<h1 id=\"nvdla\"><a class=\"anchor\" href=\"#nvdla\">#</a> NVDLA</h1>\n<p><img loading=\"lazy\" data-src=\"1718540758631.jpg\" alt=\"nvdla\"></p>\n<p><a href=\"http://nvdla.org/\">NVDLA 官方文档</a></p>\n<p>NVIDIA 深度学习加速器（NVDLA）是一种免费开放的架构，旨在推广设计深度学习推理加速器的标准方法。NVDLA 具有模块化架构，可扩展、高度可配置，旨在简化集成和可移植性。</p>\n<h1 id=\"arm\"><a class=\"anchor\" href=\"#arm\">#</a> ARM</h1>\n<p><img loading=\"lazy\" data-src=\"1718540818151.jpg\" alt=\"arm\"></p>\n<p><a href=\"https://github.com/ARM-software/armnn\">ARM NN</a></p>\n<p>ARM NN 是一个开源的软件库，用于在基于 ARM 的平台上运行预先训练好的神经网络。它充当诸如 TensorFlow Lite、Caffe、ONNX 等框架与底层硬件加速器（如 GPU、NPU）之间的桥梁。ARM NN 支持异构执行，可以在 CPU、GPU 和 NPU 之间分配工作负载，以实现最优性能。</p>\n<p>CMSIS-NN 是一个高度优化的神经网络内核集合，特别为基于 ARM Cortex-M 系列的微控制器设计。它提供了一系列高度优化的函数，用于在资源受限的设备上执行卷积神经网络 (CNNs) 和其他常见的神经网络操作。这些函数可以在 Keil MDK-ARM 或 GCC 等工具链中使用，帮助开发者在 MCU 上实现高效的机器学习推理。</p>\n<h1 id=\"mace\"><a class=\"anchor\" href=\"#mace\">#</a> MACE</h1>\n<p><img loading=\"lazy\" data-src=\"1718540862260.jpg\" alt=\"xiaomi mace\"></p>\n<p><a href=\"https://github.com/XiaoMi/mace\">XiaoMi MACE</a></p>\n<p>XiaoMi MACE 是一个深度学习推理框架，针对 Android、iOS、Linux 和 Windows 设备上的移动异构计算进行了优化。</p>\n<h1 id=\"pulsar2\"><a class=\"anchor\" href=\"#pulsar2\">#</a> Pulsar2</h1>\n<p><img loading=\"lazy\" data-src=\"1718540894460.jpg\" alt=\"pulsar\"></p>\n<p><a href=\"https://pulsar2-docs.readthedocs.io/zh-cn/latest/\">Pulsar2</a></p>\n<p>爱芯元智 AX620A 视觉芯片  使用的编译工具链是 Pulsar2</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "tengine",
                "ncnn"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/06/15/zatanE3/",
            "url": "https://forcheetah.github.io/2024/06/15/zatanE3/",
            "title": "【3D建模】T110E3卡迪夫蓝调皮肤模型",
            "date_published": "2024-06-15T12:42:27.991Z",
            "content_html": "<p>新的坦克模型，T110E3 卡迪夫蓝调皮肤模型，快要问世了。这里展示制作过程。</p>\n<h1 id=\"卡迪夫蓝调皮肤\"><a class=\"anchor\" href=\"#卡迪夫蓝调皮肤\">#</a> 卡迪夫蓝调皮肤</h1>\n<p><img loading=\"lazy\" data-src=\"20240615204753.jpg\" alt=\"111\"></p>\n<p><img loading=\"lazy\" data-src=\"20240615204855.jpg\" alt=\"222\"></p>\n<h1 id=\"建模过程\"><a class=\"anchor\" href=\"#建模过程\">#</a> 建模过程</h1>\n<p><img loading=\"lazy\" data-src=\"20240615204959.png\" alt=\"solidworks 3D 建模\"></p>\n<p><img loading=\"lazy\" data-src=\"20240615205022.png\" alt=\"solidworks 3D 建模\"></p>\n<p><img loading=\"lazy\" data-src=\"20240615205036.png\" alt=\"solidworks 3D 建模\"></p>\n<h1 id=\"打印组装\"><a class=\"anchor\" href=\"#打印组装\">#</a> 打印组装</h1>\n<p><img loading=\"lazy\" data-src=\"20240615205118.jpg\" alt=\"打印件出炉\"></p>\n<p><img loading=\"lazy\" data-src=\"20240615205148.jpg\" alt=\"组装\"></p>\n<p><img loading=\"lazy\" data-src=\"20240615205210.jpg\" alt=\"上色\"></p>\n<h1 id=\"最后\"><a class=\"anchor\" href=\"#最后\">#</a> 最后</h1>\n<p>此模型是摆件，没有行动能力。还是很期待啊。</p>\n",
            "tags": [
                "tank",
                "zatan"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/06/10/deployTVM/",
            "url": "https://forcheetah.github.io/2024/06/10/deployTVM/",
            "title": "【TVM】C++部署运行TVM",
            "date_published": "2024-06-10T11:47:15.090Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇博客主要介绍如何通过 G++ 编译器编译 C++ 代码，部署 TVM。</p>\n<blockquote>\n<p>总感觉，属于我们的时代还没开始，就要结束了呢。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"现状\"><a class=\"anchor\" href=\"#现状\">#</a> 现状</h1>\n<p>TVM 官方文档:<a href=\"https://tvm.apache.org/docs\"> 英文文档</a> <a href=\"https://tvm.hyper.ai/\">中文文档</a> 主要介绍了通过 Python 脚本和 Python 命令行 tvmc 来编译和部署 TVM。但是以这两种方式部署，部署设备还需要安装 Python 运行环境，带来额外空间占用和开销。显然不能以这种方式部署。</p>\n<p>TVM 项目的 <a href=\"https://github.com/apache/tvm/tree/main/apps/howto_deploy\">howto_deploy</a> 目录下提供了 G++ 编译 C++ 代码部署 TVM 的方式。遗憾的是给的例子没有包含模型的权重.params 和图结构.json 的加载，也没有输入图片的加载。</p>\n<p>因此本博客提供了一个简单的 C++ 部署 TVM 工程，可以在 <a href=\"https://github.com/ForCheetah/TvmCppDeploy\">TvmCppDeploy 项目</a> 找到并下载，用于你的 TVM 项目部署。</p>\n<p>该项目没有使用 <a href=\"https://github.com/apache/tvm/tree/main/apps/howto_deploy\">TVM 项目 howto_deploy</a> 中的 Makefile，而是重写了 CMakeLists.txt 文件，更方便读懂和修改。</p>\n<h1 id=\"使用方式\"><a class=\"anchor\" href=\"#使用方式\">#</a> 使用方式</h1>\n<p>下载 <a href=\"https://github.com/ForCheetah/TvmCppDeploy\">TvmCppDeploy 项目</a> 到你的本地，可以通过下载 zip 文件后解压缩，也可以直接 git：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">git</span> clone https://github.com/ForCheetah/TvmCppDeploy.git</pre></td></tr></table></figure><p>进入项目根目录，进行必要的路径修改。和 <a href=\"https://github.com/apache/tvm/tree/main/apps/howto_deploy\">TVM 项目 howto_deploy</a> 一样，本项目也提供了两种部署方式，所需要修改的内容也有些不同。</p>\n<h1 id=\"重新编译-tvm_runtime\"><a class=\"anchor\" href=\"#重新编译-tvm_runtime\">#</a> 重新编译 tvm_runtime</h1>\n<p>重新编译 tvm_runtime，和个人的 C++ 文件编译在一起，编译好的可执行文件可独立执行。</p>\n<ul>\n<li>第 1 步：打开  <code>src/Resnet50_deploy.cc </code> 文件，找到 81 行  <code>const std::string artifacts_folder(&quot;/home/xiamu/whs/temp/resnet50-tvm/&quot;);</code>  ，将其中的 <code>/home/xiamu/whs/temp/resnet50-tvm/</code>  修改为自己的已经编译好的模型路径，该路径下应该存在有  <code>mod.so, mod.params, mod.json</code> 。</li>\n<li>第 2 步：还是 <code>src/Resnet50_deploy.cc </code> 文件， 找到 132 行，将其中的图片路径 <code>/home/xiamu/whs/python/remote_tvm/imagenet_cat.bin</code>  改为自己的图片路径，该 bin 文件应当是已经转换好的 float 格式文件。</li>\n<li>第 3 步：打开   <code>src/tvm\\_runtime\\_pack.cc</code> ， 将文件中所有的路径中的  <code>/home/xiamu/tvm</code>   修改为你本地 TVM 工程的根目录路径。 修改完一定要检查一下对应的目录中是否有相应的文件。</li>\n<li>第 4 步：打开  <code>CMakeLists.txt</code> , 找到第 10 行  <code>set(TVM_ROOT /home/xianmu/tvm)</code> ，将其中的 <code>/home/xianmu/tvm</code>  改成你本地 TVM 工程的根目录路径。</li>\n<li>为防止编译报错，可以将 <code>部署方式二： tvm_runtime.so 作为动态链接库编译</code> 对应的代码（43 至 63 行） 注释掉（当前可能还没有对其进行修改）。</li>\n<li>第 5 步：编译和执行，在根目录下：</li>\n</ul>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span> <span class=\"token parameter variable\">-j4</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>./MyRunnable</pre></td></tr></table></figure><h1 id=\"tvm_runtimeso-作为动态链接库编译\"><a class=\"anchor\" href=\"#tvm_runtimeso-作为动态链接库编译\">#</a> tvm_runtime.so 作为动态链接库编译</h1>\n<p>tvm_runtime.so 作为动态链接库，仅编译个人的 C++ 文件，运行时需要链接 libtvm_runtime.so</p>\n<p>这种方式的修改与第一种方式略有不同，修改如下：</p>\n<ul>\n<li>第 1 步：打开  <code>src/Resnet50_deploy.cc </code> 文件，找到 81 行  <code>const std::string artifacts_folder(&quot;/home/xiamu/whs/temp/resnet50-tvm/&quot;);</code>  ，将其中的 <code>/home/xiamu/whs/temp/resnet50-tvm/</code>  修改为自己的已经编译好的模型路径，该路径下应该存在有  <code>mod.so, mod.params, mod.json</code> 。</li>\n<li>第 2 步：还是 <code>src/Resnet50_deploy.cc </code> 文件， 找到 132 行，将其中的图片路径 <code>/home/xiamu/whs/python/remote_tvm/imagenet_cat.bin</code>  改为自己的图片路径，该 bin 文件应当是已经转换好的 float 格式文件。</li>\n<li>第 3 步：打开  <code>CMakeLists.txt</code> , 找到第 10 行  <code>set(TVM_ROOT /home/xianmu/tvm)</code> ，将其中的 <code>/home/xianmu/tvm</code>  改成你本地 TVM 工程的根目录路径。将 62 行的 <code>$&#123;TVM_ROOT&#125;/build</code>  libtvm_runtime.so 路径修改为你存放 libtvm_runtime.so 库的路径。</li>\n<li>为防止编译报错，可以将 <code>部署方式一： 重新编译 tvm_runtime</code>  对应的代码（17 至 38 行）注释掉（当前可能还没有对其进行修改）。</li>\n<li>第 5 步：编译和执行，在根目录下：</li>\n</ul>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span> <span class=\"token parameter variable\">-j4</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>./MyExcute</pre></td></tr></table></figure><h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "tvm",
                "cmake",
                "runtime"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/30/engine1/",
            "url": "https://forcheetah.github.io/2024/05/30/engine1/",
            "title": "【推理引擎】NCNN和Tengine量化推理逻辑对比",
            "date_published": "2024-05-30T13:44:20.534Z",
            "content_html": "<p>本文讨论了 ncnn 和 tengine 两个框架在量化推理上的逻辑，并比较了它们之间的区别与联系，以及一点自己的思考。</p>\n<blockquote>\n<p>风雨送春归，飞雪迎春到。<br>\n已是悬崖百丈冰，犹有花枝俏。  ---- 教员<br>\n ------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"量化\"><a class=\"anchor\" href=\"#量化\">#</a> 量化</h1>\n<p>这里仅简单介绍一下量化，不会展开，有机会再详细写一下量化算法。</p>\n<h2 id=\"什么是量化\"><a class=\"anchor\" href=\"#什么是量化\">#</a> 什么是量化</h2>\n<p>量化的核心思想是将连续的浮点数域映射到一个更小的、离散的整数集合上。这通过将浮点数值舍入或映射到最近的整数值来实现，同时保持整个模型表示的动态范围。例如，一个原本使用 32 位浮点数表示的权重值，在量化后可能会被转换为 8 位的整数，这样每个权重只需占用更少的存储空间，并且计算时可以利用专门针对整数优化的硬件加速器，从而提高推理速度并降低能耗。</p>\n<h2 id=\"量化的分类\"><a class=\"anchor\" href=\"#量化的分类\">#</a> 量化的分类</h2>\n<p><img loading=\"lazy\" data-src=\"1717075166684.jpg\" alt=\"分类\"></p>\n<p>tensorflow 官网上的这张图详细的解释了量化的种类和区别：</p>\n<ul>\n<li>基本的 float32 模型：      tensorflow 导出的 TFlite models</li>\n<li>训练后 float16 模型 ：        量化为 float16，仍然是浮点数模型，精度损失较小</li>\n<li>量化感知训练：           在训练过程中就引入量化操作，使模型有机会适应量化带来的误差，通常能获得比单纯训练后量化更好的性能。</li>\n<li>训练后动态范围量化：        对固定参数进行量化，但是权重，但是层的输入输出没有进行量化。  不需要校准数据集</li>\n<li>训练后整形量化：            全整形量化，固定参数和层的输入输出都进行了量化，需要校准数据集</li>\n<li>训练后整形量化 int16 的激活：        激活层采用 int16 量化，其余 int8</li>\n</ul>\n<p>下面介绍和对比的 NCNN 和 Tengine 仅仅指的是运行再 CPU 上是采用的 Int8 量化策略，都属于训练后整形量化。</p>\n<h1 id=\"ncnn-量化推理逻辑\"><a class=\"anchor\" href=\"#ncnn-量化推理逻辑\">#</a> NCNN 量化推理逻辑</h1>\n<h2 id=\"ncnn\"><a class=\"anchor\" href=\"#ncnn\">#</a> ncnn</h2>\n<p><img loading=\"lazy\" data-src=\"1717075407294.jpg\" alt=\"ncnn\"></p>\n<p><a href=\"https://github.com/Tencent/ncnn\">腾讯 NCNN</a><br>\nncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。 ncnn 从设计之初深刻考虑手机端的部署和使用。 无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。 基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行， 开发出人工智能 APP，将 AI 带到你的指尖。 ncnn 目前已在腾讯多款应用中使用，如：QQ，Qzone，微信，天天 P 图等。</p>\n<h2 id=\"量化逻辑\"><a class=\"anchor\" href=\"#量化逻辑\">#</a> 量化逻辑</h2>\n<p>一、ncnn 仅对两个部分进行了量化</p>\n<ul>\n<li>i. 权重部分 ： 卷积、深度可分离卷积、全连接 三种算子的 权重，量化因子数量等于权重通道数。</li>\n<li>ii. 输入输出 tensor 部分： 卷积、深度可分离卷积、全连接 三种算子 的输入 tensor，量化因子数量为 1.</li>\n<li>iii. 卷积、深度可分离卷积、全连接算子的 bias 部分维持 float 不变</li>\n</ul>\n<p>这也就是说，只有卷积、深度可分离卷积、全连接三种算子采用量化的 int8 推理，其余算子仍然采用 float 推理。这样的方式是合理的，因为很多算子因为运算效率和精度的原因，不适合量化推理，所以分成量化的和非量化的算子是同行的做法。地平线公司生产的征程系列芯片对算子是否采用量化推理作出了更精细的分类（主动量化、被动量化、手动量化），感兴趣的可以学习一下地平线的量化逻辑。</p>\n<p><img loading=\"lazy\" data-src=\"1717075507282.jpg\" alt=\"结果\"></p>\n<p>上图是一个简单的神经网络模型经 ncnn 量化之后的结果。<br>\n这个模型有两个卷积层 + 一个全连接层，外加一些激活、BN、维度变换层。这里可以看到：</p>\n<ul>\n<li>1. 前三行是权重的量化，后三行是输入输出 tensor 的量化</li>\n<li>2. 第一行是卷积 conv1 权重量化，该卷积有 4 个卷积核，因此量化因子有 4 个 （ncnn 的量化因子之所以这么大，是因为 ncnn 存储的是量化因子的倒数）</li>\n<li>3. 第二行是卷积 conv2 权重量化，该卷积有 8 个卷积核，因此量化因子有 8 个</li>\n<li>4. 第三行是全连接层的权重量化</li>\n<li>5. 后三行是对应算子的输入 tensor 的量化因子，都只有 1 个</li>\n</ul>\n<p>Tengine 框架就不专门介绍算子量化因子的数量了，因为都是一样的。</p>\n<p><img loading=\"lazy\" data-src=\"1717075577838.jpg\" alt=\"ncnn流程\"></p>\n<p>上图是 ncnn 的量化模型在卷积推理时的逻辑，先忽略掉图中的红线流程，只看蓝线部分的一般流程：</p>\n<ul>\n<li>1.ncnn 的基本思路是 float 类型输入，float 类型输出</li>\n<li>2. 跟随蓝线，输入 Input 为 float 类型，经判断不是 int8，将 input 量化为 int8</li>\n<li>3.int8 类型的 input 于 int8 类型的权重进行卷积运算，运算结果是 int32</li>\n<li>4. 判断是否需要转 int8，True 还是 Flase 后面再讲，一般流程中为 False，然后将 int32 反量化为 float</li>\n<li>5.float 经过加偏置 Bias（float 类型）和激活操作之后，得到输出 Output</li>\n</ul>\n<p>一般流程中，卷积、深度可分离卷积和全连接这些算子都要进行 量化 - 计算 - 反量化过程，其他算子仍然采用 float 推理。为了减少量化和反量化的计算，ncnn 对特殊情况做了优化，也就是红线流程：</p>\n<ul>\n<li>1. 当神经网络中存在以下两种情况时走红线流程：1）两个卷积层相邻  conv -&gt; conv   2) 两个卷积层中间夹个 split  conv-&gt; split -&gt; conv</li>\n<li>2. 判断 input 输入是否为 int8，是的话无须量化操作</li>\n<li>3. 权重和 int8 的输入进行卷积计算之后得到 int32 的输出</li>\n<li>4. 再以上两种情况下，to Int8 判断为 True，直接将 int32 强转为 int8</li>\n<li>5. 输出 int8</li>\n</ul>\n<p>这种情况下减少了量化 和反量化的操作，一定程度上提高了运行效率。但是在实际测试中似乎效果不好，原因在于，现在的神经网络在卷积层和卷积层之间，基本上会有偏置、BN、激活等操作，很难会有两个卷积层相连的情况。</p>\n<p>下面是 ncnn 卷积算子的量化推理的简单抽象：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>// f: float  I:int8   Scale:量化因子  in: input  w:weight  out:output   bias:偏置</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>f_in <span class=\"token operator\">=</span> I_in * Scale_in    </pre></td></tr><tr><td data-num=\"3\"></td><td><pre>f_w  <span class=\"token operator\">=</span> I_w  * Scale_w      </pre></td></tr><tr><td data-num=\"4\"></td><td><pre>f_out <span class=\"token operator\">=</span> f_in * f_w + f_bias</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>        <span class=\"token operator\">=</span> I_in * I_w  * <span class=\"token punctuation\">(</span>Scale_in * Scale_w<span class=\"token punctuation\">)</span> + f_bias</pre></td></tr></table></figure><p>第一行对符号进行了注释。</p>\n<p>第二行和第三行是量化的基本等式</p>\n<p>最后 输出的是  <code>f_out</code> ，反量化因子是  <code>(Scale_in * Scale_w)</code>   ，还要加上 float 类型的 偏置  <code>f_bias</code></p>\n<h1 id=\"tengine量化推理逻辑\"><a class=\"anchor\" href=\"#tengine量化推理逻辑\">#</a> Tengine 量化推理逻辑</h1>\n<h2 id=\"tengine\"><a class=\"anchor\" href=\"#tengine\">#</a> Tengine</h2>\n<p><img loading=\"lazy\" data-src=\"1717075777490.jpg\" alt=\"tengine\"></p>\n<p><a href=\"https://github.com/OAID/Tengine\">Open AI Lib Tengine</a></p>\n<p>Tengine 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目使用 C 语言进行核心模块开发，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署，降低评估、迁移成本。</p>\n<p>尽管 Tengine 已经挺久不再维护了，但是作为一个完全使用 C 语言承担推理部分工作的框架，tengine 还是有轻量、框架结构清晰、易于拓展、容易学习、部署简单等优势，还是非常值得学习的。</p>\n<h2 id=\"量化逻辑-2\"><a class=\"anchor\" href=\"#量化逻辑-2\">#</a> 量化逻辑</h2>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>// f: float  int8:int8  int32:int32  Scale:量化因子  in: input  w:weight  out:output   bias:偏置</pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># 一般性等价关系</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>f_w <span class=\"token operator\">=</span> int8_w * scale_w    f_out <span class=\"token operator\">=</span> int8_out * scale_out    f_in <span class=\"token operator\">=</span> int8_in * scale_in</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>f_out <span class=\"token operator\">=</span> f_w * f_in + int32_bias</pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\"># tengine 等一些推理引擎  直接将 bias 的量化因子定为：</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>f_bias <span class=\"token operator\">=</span> int32_bias * <span class=\"token punctuation\">(</span>scale_w * scale_in<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token comment\"># float 类型输出等式</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>f_out     <span class=\"token operator\">=</span> f_w * f_in + f_bias</pre></td></tr><tr><td data-num=\"12\"></td><td><pre>          <span class=\"token operator\">=</span> int8_w * scale_w  *  int8_in * scale_in   + f_bias</pre></td></tr><tr><td data-num=\"13\"></td><td><pre>          <span class=\"token operator\">=</span> int8_w * int8_in * <span class=\"token punctuation\">(</span>scale_w  * scale_in<span class=\"token punctuation\">)</span>  + int32_bias * <span class=\"token punctuation\">(</span>scale_w * scale_in<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>          <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>int8_w * int8_in + int32_bias<span class=\"token punctuation\">)</span> * <span class=\"token punctuation\">(</span>scale_w * scale_in<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>int8_out * scale_out <span class=\"token operator\">=</span>    <span class=\"token punctuation\">(</span>int8_w * int8_in + int32_bias<span class=\"token punctuation\">)</span> * <span class=\"token punctuation\">(</span>scale_w  * scale_in<span class=\"token punctuation\">)</span>                                     </pre></td></tr><tr><td data-num=\"17\"></td><td><pre>int8_out <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>int8_w * int8_in + int32_bias<span class=\"token punctuation\">)</span> * <span class=\"token punctuation\">(</span>scale_w  * scale_in / scale_out<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>为了能讲清楚 Tengine 的量化推理逻辑，需要先看懂上面的等式推理。</p>\n<p>尽管行数较多，但实际上很简单，只有最基本的多项式推导。首先记一下第一行符号代表的意思，挺直观就不过多介绍了。</p>\n<ul>\n<li>1.tengine 每个算子都是严格的 int8 输入 int8 输出</li>\n<li>2. 一般性等价关系中展示了 float 类型和 其 int8 类型的关系</li>\n<li>3. 第 8 行中，tengine (还有其他一些引擎，如地平线) 对卷积的偏置进行了量化。量化因子直接指定为当前层的 <code>(scale_w * scale_in)</code> 。量化后的模型，偏置为 int32 类型。注意肯定不能是 int8 类型，因为其量化因子是 <code>(scale_w * scale_in)</code> ，采用 int8 绝对会溢出。</li>\n<li>4. 我们先来看第 11 行，float 类型的输出，将等式  <code>f_out   = f_w * f_in + f_bias</code>  中右边的 float 类型全部转为 int 类型，这个等式很直观。此时你也应该看到了将 bias 的量化因子直接定为 <code>(scale_w * scale_in)</code>  的好处了。</li>\n<li>5. 继续推理等式，第 16 行，将 11 行的等式左边也换成 int8 类型。（ tengine 输入输出都是 int8）</li>\n<li>6.17 行，int8_out ，也就是 int8 类型的输出，等价于 数据运算  <code>int8_w * int8_in + int32_bias</code>  乘上量化因子  <code>scale_w  * scale_in / scale_out</code></li>\n</ul>\n<p>从上面的推理来看，把  <code>scale_w  * scale_in / scale_out</code>  作为一个整体，每个卷积算子只需要进行一次量化运算就行了，和 ncnn (每个卷积算子都需要 量化 反量化 两次计算) 相比，足足减少了一半的量化运算。</p>\n<p>但是上面的推理忽略了一个问题，激活。基本上每个卷积层后面都跟着激活层，<strong>激活层能不能在量化形式下（整形状态）进行</strong>呢，这又是一个值得讨论的问题。</p>\n<p>tengine 没有去讨论这个问题，将上面的推理分成两步去做，先通过 <code> * scale_w  * scale_in</code>  反量化为 float 类型，做激活，然后再通过   <code>/ scale_out </code>  量化为 int8 类型，具体过程看下图：</p>\n<p><img loading=\"lazy\" data-src=\"1717075907713.jpg\" alt=\"运行逻辑\"></p>\n<p>再简单介绍一下过程，这个过程在前面的公式推理中已经基本提到了。</p>\n<ul>\n<li>1. 从图中可以看到，输入输出都是 int8</li>\n<li>2.int8 类型的 输入和权重 经过卷积层 得到的输出是 int32 类型，再于 int32 类型的 bias 相加</li>\n<li>3. 为了在 float 形式下进行激活操作， <code> * scale_w  * scale_in</code>  反量化为 float 类型</li>\n<li>4. 激活</li>\n<li>5. 通过 <code>/ scale_out </code>  量化为 int8 类型</li>\n</ul>\n<h1 id=\"量化讨论和疑问留存\"><a class=\"anchor\" href=\"#量化讨论和疑问留存\">#</a> 量化讨论和疑问留存</h1>\n<h2 id=\"简单比较\"><a class=\"anchor\" href=\"#简单比较\">#</a> 简单比较</h2>\n<p><img loading=\"lazy\" data-src=\"1717075973433.jpg\" alt=\"其他算子\"></p>\n<ul>\n<li>a.tengine 和 ncnn 都是将权重量化为 int8，ncnn 将 bias 保留 float 形式，tengine 将 bias 量化为 int32。float 和 int32 所占字节相同，模型量化前后的空间占用上，两者是相当的。</li>\n<li>b. 前面提到 ncnn 仅有 3 个算子采用量化推理，这是合理的，ncnn 主要在手机等设备上运行，效率很高；专门设计的 NPU 芯片支持十几个甚至更多 的算子量化推理；而 tengine 在架构上没有区分量化运行的算子和非量化运行的算子，所有算子一股脑全部采用量化推理，至少直到不再维护后的<a href=\"https://github.com/OAID/Tengine\">最后一个版本</a>都是这样。笔者认为这个设计很失败，因为大量不支持量化推理的算子，不得不先反量化，然后进行 float 运算，再量化，徒徒增加了时间、空间，还降低了精度。</li>\n<li>c. 讨论一下 《Tengine 量化推理逻辑》一节中提到的 <strong>激活层能不能在量化形式下（整形状态）进行</strong> 这个问题。由于大多数卷积算子用到的激活函数 是 Relu，而我们用到的又是 int8 这种对称量化，在这些条件下，理论是可以直接在 整形条件下进行激活计算的。那么整个过程就从下图的左边变成了右边，减少了量化一半的计算量。笔者修改 Tengine 工程并验证了一下，从结果来区别不大。当然笔者没有进行大量测试，感兴趣的欢迎测试和交流。</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"1717076022465.jpg\" alt=\"compare\"></p>\n<h2 id=\"疑问留存\"><a class=\"anchor\" href=\"#疑问留存\">#</a> 疑问留存</h2>\n<p>既然量化和反量化过程费时又费力 ，那么有没有一种可能，多个连续的卷积层条件下，<strong>只在首尾进行量化和反量化</strong>呢？</p>\n<p>本博客将在不久后，通过公式、仿真的形式探讨一下其可能性及条件。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv",
                "tengine",
                "ncnn"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/26/zatan3D/",
            "url": "https://forcheetah.github.io/2024/05/26/zatan3D/",
            "title": "【3D建模】IS-7攻城锤流纹岩皮肤展示",
            "date_published": "2024-05-26T08:19:13.185Z",
            "content_html": "<h1 id=\"坦克世界皮肤图片\"><a class=\"anchor\" href=\"#坦克世界皮肤图片\">#</a> 坦克世界皮肤图片</h1>\n<p><img loading=\"lazy\" data-src=\"1024x.jpg\" alt=\"洪水猛兽\"></p>\n<p><img loading=\"lazy\" data-src=\"24x.jpg\" alt=\"突击重坦\"></p>\n<p><img loading=\"lazy\" data-src=\"2x.jpg\" alt=\"钢铁之盾\"></p>\n<p><img loading=\"lazy\" data-src=\"2024x.jpg\" alt=\"胜利之师\"></p>\n<p>IS-7 皮肤台词：&quot; 原来是这样的： 如果有人回到基地，至少一辆坦克，那就算是一次营救任务。如果无人返回，那就是一次侦察任务。那就是他们在总部报告中对我们的分类方式，而我就是参加了一次营救任务。事情是这样的：我们整个小队一起移动，一些奇怪的大雾导致能见度为 0，坦克只能摸索着前行，就好像在牛奶中前进一样。然后坦克出现了。D 系的坦克。我记得它们的外形，也知道如何应对它们。你也知道，我以前经历过一些战事。接下来又遇到了没有沙子的沙尘暴… 我也不知道该怎么解释。我经历了持续的耳鸣，一些嗡嗡的声音。眼睛仿佛沾满了焦油。奇怪的低语。黑暗… 如同活物一般。我们摔得东倒西歪，被分散了。我害怕吗？当然不怕。起初我们都很困惑，但后来就适应了。没有什么区别，我们来都来了，还有什么好怕的？什么都改变不了。服从命令，祈祷可以回家。所以我就回来了… 一个人回来了。上尉，你的问题很奇怪。当小队重新集结时，我会回去吗？你已经知道答案，我们发过同样的誓言。而这只会发生一次。发了誓，就要遵守它。现在不是犹豫不决的时候了。我不希望加入一次... 侦察任务。当我们抵达那里的时候... 我们会挺过去的。</p>\n<h1 id=\"一些制作过程\"><a class=\"anchor\" href=\"#一些制作过程\">#</a> 一些制作过程</h1>\n<p><img loading=\"lazy\" data-src=\"0XGEG466R.png\" alt=\"solidworks 3D 建模\"></p>\n<p><img loading=\"lazy\" data-src=\"aaaaa.png\" alt=\"solidworks 3D 建模\"></p>\n<p><img loading=\"lazy\" data-src=\"6I6P.png\" alt=\"solidworks 3D 建模\"></p>\n<p><img loading=\"lazy\" data-src=\"194309.jpg\" alt=\"打印件出炉\"></p>\n<p><img loading=\"lazy\" data-src=\"120504.jpg\" alt=\"组装\"></p>\n<p><img loading=\"lazy\" data-src=\"120514.jpg\" alt=\"上色\"></p>\n<h1 id=\"最后\"><a class=\"anchor\" href=\"#最后\">#</a> 最后</h1>\n<p>该 IS-7 流纹岩皮肤模型还履带转动、炮塔旋转、炮管上下、夜视灯开关和发动机声音模拟功能。</p>\n<p>感兴趣的小伙伴可以到 bilibili 观看制作过程的视频呀！</p>\n<p>视频链接：</p>\n<p><a href=\"https://www.bilibili.com/video/BV1Tp4y1c719/\">【坦克模型】IS-7 攻城锤 坦克世界 is7 流纹岩 3D 皮肤模型</a></p>\n",
            "tags": [
                "tank",
                "zatan"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/24/tvm1/",
            "url": "https://forcheetah.github.io/2024/05/24/tvm1/",
            "title": "【TVM】根据例子走通代码库",
            "date_published": "2024-05-24T14:49:36.319Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>最近开始学习 TVM。感觉 TVM 英文文档中 <a href=\"https://tvm.apache.org/docs/dev/tutorial/codebase_walkthrough.html\">TVM Codebase Walkthrough by Example</a>    一节对于理解 TVM 工程非常有用。本篇文章只是翻译，可以直接跳转查看英文全文。</p>\n<blockquote>\n<p>这个时代有这么多愿意开源并将技术介绍给我们的行业大牛，真是我们的荣幸，膜拜！<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"codebase-structure-overview\"><a class=\"anchor\" href=\"#codebase-structure-overview\">#</a> Codebase Structure Overview</h1>\n<p>在 TVM 存储库的根目录中，我们有以下子目录，它们共同构成了大部分代码库。</p>\n<ul>\n<li><strong>src</strong><br>\nC++ code for operator compilation and deployment runtimes.<br>\n 算子编译 、 runtime 部署 的 C++ 代码</li>\n<li><strong>src/relay</strong><br>\nImplementation of Relay, a new functional IR for deep learning framework.<br>\nRelay IR 的实现      算子的映射关系在 src/relay/op</li>\n<li><strong>python</strong><br>\nPython frontend that wraps C++ functions and objects implemented in src.<br>\npython 前端</li>\n<li><strong>src/topi</strong><br>\nCompute definitions and backend schedules for standard neural network operators.<br>\n 计算标准神经网络算子的定义和后端调度</li>\n</ul>\n<p>TVM 中 Python 和 C++ 的互操作性不是单向的。尽管在 TVM 中 C++ 完成繁重的内部执行工作，Python 完成用户接口， TVM 中也存在 C++ 调用 Python 的情况：For example, the convolution operator is implemented in Python, and its implementation is invoked from C++ code in Relay.（Relay 中的 C++ 调用 Python 实现的卷积算子）</p>\n<h1 id=\"vector-add-example\"><a class=\"anchor\" href=\"#vector-add-example\">#</a> Vector Add Example</h1>\n<p>使用 vector add 的例子来查看底层 TVM API.</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>n <span class=\"token operator\">=</span> <span class=\"token number\">1024</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>A <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">'A'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>B <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">'B'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>C <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>compute<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> <span class=\"token keyword\">lambda</span> i<span class=\"token punctuation\">:</span> A<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> B<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"C\"</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这里面 A、B、C 都是  <code>tvm.tensor.Tensor</code>   其 Python 定义位于 <code>python/tvm/te/tensor.py</code> . 支撑的 C++ 定义位于 <code>include/tvm/te/tensor.h</code>  和 <code>src/te/tensor.cc</code>  所有的 Python 类型定义都能找到对应的相同名字的 C++ 定义。</p>\n<p>Python 对 C++ 的包装位于  <code>python/tvm/_ffi/</code> 。</p>\n<p>一个 Tensor 包含一个 Operation 类，定义于 python/tvm/te/tensor.py，对应的 C++ 实现位于 <code>include/tvm/te/operation.h</code>  和 <code>src/tvm/te/operation</code>  。 <code>Tensor</code>  是  <code>Operation</code>  类的输出。</p>\n<p>我们将输出张量 C 对应的操作传递给 <code>tvm.te.create_schedule()</code>  函数 （来自于 <code>python/tvm/te/schedule.py</code> 。）</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>s <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>create_schedule<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">.</span>op<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这个函数映射到 C++ 函数 <code>include/tvm/schedule.h</code> 。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>inline Schedule create_schedule<span class=\"token punctuation\">(</span>Array<span class=\"token operator\">&lt;</span>Operation<span class=\"token operator\">></span> ops<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>  <span class=\"token keyword\">return</span> Schedule<span class=\"token punctuation\">(</span>ops<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><code>Schedule</code>  包含 <code>Stage</code>  输出  <code>Operation</code>  的集合。</p>\n<p><code>Stage</code>  对应于一个操作 <code>Operation</code> 。上面的 vector add 操作中有两个 placeholder ops 和一个 compute op. 所以 <code>Schedule s</code>  有三个状态  <code>Stage</code> ，每个 <code>Stage</code>  持有以下信息： 循环嵌套结构、每个循环的类型（ <code>Parallel，Vectorized，Unrolled</code> ）、以及在下一个循环嵌套 <code>Stage</code>  中在哪里执行它自己的计算。</p>\n<p><code>Schedule</code>  和 <code>Stage</code>  本身定义在 <code>tvm/python/te/schedule.py</code> ，  <code>include/tvm/te/schedule.h</code> ， 和 <code>src/te/schedule/schedule_ops.cc</code> 。</p>\n<p>为简单起见，我们使用 <code>tvm.build(...)</code>  处理上方 <code>create_schedule()</code>  函数创建的默认 <code>Schedule s</code>  和 &lt;em&gt;。我们必须添加必要的线程绑定，来使得其能在 GPU 上运行：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>target <span class=\"token operator\">=</span> <span class=\"token string\">\"cuda\"</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>bx<span class=\"token punctuation\">,</span> tx <span class=\"token operator\">=</span> s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">.</span>op<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> factor<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span>bx<span class=\"token punctuation\">,</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>thread_axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"blockIdx.x\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span>tx<span class=\"token punctuation\">,</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>thread_axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"threadIdx.x\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>fadd <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>build<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>A<span class=\"token punctuation\">,</span> B<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> target<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p><code>tvm.build(...)</code> ，定义在 <code>python/tvm/driver/build_module.py</code> ， 需要输入一个 <code>Schedule</code> ;  <code>input</code> , <code>output Tensor</code> ; 以及一个 <code>target</code> 。返回一个 <code>tvm.runtime.Module</code> 。</p>\n<p>整个 <code>tvm.build(...)</code>  过程可以分成两步：</p>\n<ul>\n<li>\n<p>i. 降级 高级的、初始的循环嵌套结构被转换为 最终的、低级的 IR</p>\n</li>\n<li>\n<p>ii. 代码生成 low level IR 生成目标机器码</p>\n</li>\n</ul>\n<p>降级是通过 <code>tvm.lower()</code>  函数完成的，它定义在 <code>python/tvm/build\\_module.py</code> 。第一，指定绑定推理，一个最初的循环嵌套结构就创建好了。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">def</span> <span class=\"token function\">lower</span><span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>          args<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>          name<span class=\"token operator\">=</span><span class=\"token string\">\"default_function\"</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>          binds<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>          simple_mode<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>   <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>   bounds <span class=\"token operator\">=</span> schedule<span class=\"token punctuation\">.</span>InferBound<span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>   stmt <span class=\"token operator\">=</span> schedule<span class=\"token punctuation\">.</span>ScheduleOps<span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">,</span> bounds<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>   <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr></table></figure><p>边界推断是推断所有循环边界和中间缓冲区大小的过程。如果你的目标是 CUDA，且你用了 share memory，它需要的最小 size 在此处确定。绑定推理时在 <code>src/te/schedule/bound.cc，src/te/schedule/graph.cc </code>  和  <code>src/te/schedule/message\\_passing.cc</code>  中实现的。</p>\n<p><code>stmt</code> ， <code>ScheduleOps()</code>  的输出，表示一个初识的循环嵌套结构。如果在 schedule 中已经应用了 <code>reorder</code>  和 <code>split</code>  原语，那么初始的循环嵌套结构已经反映了这些变化。 <code>ScheduleOps()</code>  定义在 <code>rc/te/schedule/schedule_ops.cc</code> 。</p>\n<p>接下来应用一些 lowering passes to  <code>stmt</code>  . 这些 passes 在 <code>src/tir/pass</code>  子文件夹下实现。举个例子，如果在你的 <code>schedule</code>  中应用了 <code>vectorize</code>  或者 <code>unroll</code>  原语，他们会被应用到循环 vectorization 和 unrolling passes。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>stmt <span class=\"token operator\">=</span> ir_pass<span class=\"token punctuation\">.</span>VectorizeLoop<span class=\"token punctuation\">(</span>stmt<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>stmt <span class=\"token operator\">=</span> ir_pass<span class=\"token punctuation\">.</span>UnrollLoop<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    stmt<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_step<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_depth<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_extent<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>unroll_explicit<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr></table></figure><p>在降级 lowering 结束后， <code>build()</code>  函数生成目标机器代码。如果你的设备是 X86, 这个代码可能包含 SSE 或者 AVX 指令；如果是 CUDA 设备，将包含 PTX 指令。 此外，除了目标特定的机器代码之外，TVM 还生成负责内存管理、内核启动等的主机端代码。</p>\n<p><code>build\\_module()</code>  函数完成代码生成，定义在 <code>python/tvm/target/codegen.py</code> 。在 C++ 端代码生成定义在 <code>src/target/codegen</code> 。 <code>build\\_module()</code>  Python 函数会搜索在 <code>src/target/codegen/codegen.cc</code>  中的 <code>build()</code>  函数。</p>\n<p><code>build()</code>  函数 <code>PackedFunc</code>  注册表中为目标设备查找代码生成器，并调用找到的函数。例如， <code>codegen.build\\_cuda</code>  函数注册在 <code>src/codegen/build_cuda_on.cc</code> ，就像这样：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>TVM_REGISTER_GLOBAL<span class=\"token punctuation\">(</span><span class=\"token string\">\"codegen.build_cuda\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token punctuation\">.</span>set_body<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>TVMArgs args<span class=\"token punctuation\">,</span> TVMRetValue<span class=\"token operator\">*</span> rv<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token operator\">*</span>rv <span class=\"token operator\">=</span> BuildCUDA<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  <span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>上方的 <code>BuildCUDA()</code>  函数使用定义在 <code>src/codegen/codegen_cuda.cc</code>  的 <code>CodeGenCUDA</code>  类，从 lowered IR 生成 CUDA kernel source，并使用 NVRTC 编译 kernel。如果你的目标设备使用 LLVM，包括 X86、ARM、NVPTX 和 AMDGPU，代码可由定义在 <code>src/codegen/llvm/codegen_llvm.cc</code>  的 <code>CodeGenLLVM</code>  来生成。 <code>CodeGenLLVM</code>  将 TVM IR 转换成 LLVM IR，运行一些 LLVM 优化 passes，以及生成目标机器码。</p>\n<p>在 <code>src/codegen/codegen.cc</code>  中的 <code>Build()</code>  函数会返回一个 <code>runtime::Module</code>  类，它定义在 <code>include/tvm/runtime/module.h</code>  和 <code>src/runtime/module.cc</code> 。一个 <code>Module</code>  类是一个潜在目标 设备的特定 <code>ModuleNode</code>  的容器。</p>\n<p>每个后端都实现一个 <code>ModuleNode</code>  的子类，来添加目标特定的 runtime API 调用。 例如，CUDA 后端在 <code>src/runtime/cuda/cuda_module.cc</code>  实现 <code>CUDAModuleNode</code>  类，来管理 CUDA 驱动 API。上方的 <code>BuildCUDA()</code>  函数用 <code>runtime::Module</code>  包装了 <code>CUDAModuleNode</code> ，并包装到 Python 端。LLVM 后端在 <code>src/codegen/llvm/llvm_module.cc</code>  实现了 <code>LLVMModuleNode</code> ，处理了 JIT 执行和编译代码。其他对应各个后端的 <code>ModuleNode</code>  子类可以在 <code>src/runtime</code>  子文件夹找到。<br>\n返回的 <code>module</code> ，可以被认作编译函数和设备 API 的组合，可以被 TVM 的 NDArray objects 调用。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>dev <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span>target<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>a <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>uniform<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>b <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>uniform<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>B<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>c <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>C<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>fadd<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>output <span class=\"token operator\">=</span> c<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>在幕后，TVM 会自动分配设备内存并管理内存传输。为了实现这个目标，每个后端都需要继承在 <code>include/tvm/runtime/device_api.h</code>  定义的 <code>DeviceAPI</code>  类，使用设备特定的 API 重写里面的内存管理方法。例如，CUDA 后端在 <code>src/runtime/cuda/cuda_device_api.cc</code>  使用 <code>cudaMalloc</code> 、 <code>cudaMemcpy</code>  实现了 <code>CUDADeviceAPI</code> .</p>\n<p>第一次使用 <code>fadd(a, b, c)</code>  调用编译后的模块时，会调用  <code>ModuleNode</code>  的  <code>GetFunction()</code>  方法来获取可用于内核调用的  <code>PackedFunc</code> 。例如，在 <code>src/runtime/cuda/cuda_module.cc</code>  CUDA 后端实现了 <code>CUDAModuleNode::GetFunction()</code>  函数如下：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>PackedFunc CUDAModuleNode<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>GetFunction<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>      const std<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>string<span class=\"token operator\">&amp;</span> name<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>      const std<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>shared_ptr<span class=\"token operator\">&lt;</span>ModuleNode<span class=\"token operator\">></span><span class=\"token operator\">&amp;</span> sptr_to_self<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  auto it <span class=\"token operator\">=</span> fmap_<span class=\"token punctuation\">.</span>find<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>  const FunctionInfo<span class=\"token operator\">&amp;</span> info <span class=\"token operator\">=</span> it<span class=\"token operator\">-</span><span class=\"token operator\">></span>second<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>  CUDAWrappedFunc f<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>  f<span class=\"token punctuation\">.</span>Init<span class=\"token punctuation\">(</span>this<span class=\"token punctuation\">,</span> sptr_to_self<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>arg_types<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>launch_param_tags<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>  <span class=\"token keyword\">return</span> PackFuncVoidAddr<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>arg_types<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><code>PackedFunc</code>  的重载函数 <code>operator()</code>  会被调用。从而会调用定义在 <code>src/runtime/cuda/cuda_module.cc</code>  的 <code>CUDAWrappedFunc</code>  的 <code>operator()</code>  函数，最终我们会看到 <code>cuLaunchKernel</code>  驱动会调用：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"><span>p</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">CUDAWrappedFunc</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre> <span class=\"token keyword\">public</span><span class=\"token operator\">:</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>  <span class=\"token keyword\">void</span> <span class=\"token function\">Init</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>  <span class=\"token keyword\">void</span> <span class=\"token keyword\">operator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>TVMArgs args<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>                  TVMRetValue<span class=\"token operator\">*</span> rv<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>                  <span class=\"token keyword\">void</span><span class=\"token operator\">*</span><span class=\"token operator\">*</span> void_args<span class=\"token punctuation\">)</span> <span class=\"token keyword\">const</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token keyword\">int</span> device_id<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    <span class=\"token function\">CUDA_CALL</span><span class=\"token punctuation\">(</span><span class=\"token function\">cudaGetDevice</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>device_id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>fcache_<span class=\"token punctuation\">[</span>device_id<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>      fcache_<span class=\"token punctuation\">[</span>device_id<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> m_<span class=\"token operator\">-></span><span class=\"token function\">GetFunc</span><span class=\"token punctuation\">(</span>device_id<span class=\"token punctuation\">,</span> func_name_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    CUstream strm <span class=\"token operator\">=</span> <span class=\"token generic-function\"><span class=\"token function\">static_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span>CUstream<span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span><span class=\"token class-name\">CUDAThreadEntry</span><span class=\"token double-colon punctuation\">::</span><span class=\"token function\">ThreadLocal</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">-></span>stream<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>    ThreadWorkLoad wl <span class=\"token operator\">=</span> launch_param_config_<span class=\"token punctuation\">.</span><span class=\"token function\">Extract</span><span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    CUresult result <span class=\"token operator\">=</span> <span class=\"token function\">cuLaunchKernel</span><span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>        fcache_<span class=\"token punctuation\">[</span>device_id<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">grid_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">grid_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">grid_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">block_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">block_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">block_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>        <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> strm<span class=\"token punctuation\">,</span> void_args<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>  <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>本文概括了 TVM 如何编译和执行函数。 虽然本文没有详细说明 TOPI 或 Relay，但最终所有神经网络算子都会经历与上述相同的编译过程。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv",
                "tvm"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/24/category/",
            "url": "https://forcheetah.github.io/2024/05/24/category/",
            "title": "博客汇总目录",
            "date_published": "2024-05-24T13:25:53.973Z",
            "content_html": "<h1 id=\"暮冬z羡慕-的博客-文章汇总\"><a class=\"anchor\" href=\"#暮冬z羡慕-的博客-文章汇总\">#</a> 暮冬 Z 羡慕 的博客  文章汇总</h1>\n<h1 id=\"卷积加速算法\"><a class=\"anchor\" href=\"#卷积加速算法\">#</a> 卷积加速算法</h1>\n<ul>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/23/conv1/\">【Im2Col】卷积加速算法【1】 NCHW</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/23/conv2/\">【Im2Col】卷积加速算法【2】NHWC</a></p>\n</li>\n</ul>\n<h1 id=\"ai推理引擎\"><a class=\"anchor\" href=\"#ai推理引擎\">#</a> AI 推理引擎</h1>\n<ul>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/06/16/engine2/\">【推理引擎】常见推理引擎</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/30/engine1/\">【推理引擎】NCNN 和 Tengine 量化推理逻辑对比</a></p>\n</li>\n</ul>\n<h1 id=\"ai编译优化\"><a class=\"anchor\" href=\"#ai编译优化\">#</a> AI 编译优化</h1>\n<ul>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/06/10/deployTVM/\">【TVM】C++ 部署运行 TVM</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/06/18/deployTVMPython/\">【TVM】Python 脚本实现模型编译和保存</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/24/tvm1/\">【TVM】根据例子走通代码库</a></p>\n</li>\n</ul>\n<h1 id=\"问题解决\"><a class=\"anchor\" href=\"#问题解决\">#</a> 问题解决</h1>\n<ul>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/15/openBlas/\">openBlas 库的安装与简单使用</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/14/cpplib/\">C 语言工程调用 Cpp 库解决方案</a></p>\n</li>\n</ul>\n<h1 id=\"杂谈\"><a class=\"anchor\" href=\"#杂谈\">#</a> 杂谈</h1>\n<ul>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/26/zatan3D/\">【3D 建模】IS-7 攻城锤流纹岩皮肤展示</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/06/15/zatanE3/\">【3D 建模】T110E3 卡迪夫蓝调皮肤模型</a></p>\n</li>\n</ul>\n<h1 id=\"其他\"><a class=\"anchor\" href=\"#其他\">#</a> 其他</h1>\n<blockquote>\n<p>持续更新中 ...</p>\n</blockquote>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/23/conv2/",
            "url": "https://forcheetah.github.io/2024/05/23/conv2/",
            "title": "【Im2Col】卷积加速算法【2】NHWC",
            "date_published": "2024-05-23T12:12:27.275Z",
            "content_html": "<p>本文为最基本的 Im2Col 算法的原理及实现。</p>\n<p><a href=\"https://forcheetah.github.io/2024/05/23/conv1/\">【Im2Col】卷积加速算法 NHWC</a> 【1】中已经讲了在输入和输出都是 nchw 排布下 Im2Col 算法的实现方式。常见的 tensor 输入有 NCHW 和 NHWC 两种内存排布方式，不同的排布方式各有优劣。排布方式不同，Im2Col 也有区别，本篇主要是在 NHWC 内存排布情况下的 Im2Col 算法原理和基本实现。</p>\n<blockquote>\n<p>慌乱的时候全是破绽，冷静下来，能够找到对方的破绽。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"两种内存排布\"><a class=\"anchor\" href=\"#两种内存排布\">#</a> 两种内存排布</h1>\n<p>卷积神经网络（CNN）的输入数据布局主要有两种标准：NCHW（通道、高度、宽度）和 NHWC（高度、宽度、通道）。主要深度学习框架对这两种布局的支持情况如下：</p>\n<p>PyTorch：主要采用 NCHW 格式。这是 PyTorch 在大多数情况下的默认布局，尤其是在涉及 GPU 计算时。</p>\n<p>Caffe：采用 NCHW 格式。Caffe 框架倾向于使用这种通道优先的布局。</p>\n<p>TensorFlow：默认使用 NHWC 格式，特别是在早期版本中，这是由于 TensorFlow 最初设计时主要针对 CPU 进行优化，NHWC 布局在这种场景下有更好的内存访问局部性。<br>\nKeras：Keras 本身是一个高级 API，集成到 Tensorflow 之后跟随 tensorflow 的内存排布方式。</p>\n<h1 id=\"内存排布的优缺点\"><a class=\"anchor\" href=\"#内存排布的优缺点\">#</a> 内存排布的优缺点</h1>\n<p><strong>NCHW 格式的优点：</strong></p>\n<ul>\n<li>\n<p>1. 在 GPU 中计算卷积时，比 NHWC 要快 2.5 倍左右。这是因为在 GPU 中，NCHW 格式的数据布局更符合 GPU 的内存访问模式和计算方式。</p>\n</li>\n<li>\n<p>2.NCHW 格式更适合那些需要对每个通道单独做运算的操作，比如 “MaxPooling”。这是因为 NCHW 格式的同一通道的像素值连续排布，使得对每个通道的数据可以更高效地进行运算。</p>\n</li>\n</ul>\n<p><strong>NCHW 格式的缺点：</strong></p>\n<ul>\n<li>\n<p>1.NCHW 格式需要把所有通道的数据都读取到，才能进行运算，因此在计算时需要的存储更多。这可能会限制其在一些具有限制的硬件环境下的应用。</p>\n</li>\n<li>\n<p>2.NCHW 格式的访存与计算的控制逻辑相对简单，这使得在一些需要精细控制访存和计算的场景下，可能不是最佳的选择。</p>\n</li>\n</ul>\n<p><strong>NHWC 格式的优点：</strong></p>\n<ul>\n<li>\n<p>1.NHWC 格式的访存局部性更好。这意味着每三个输入像素就可以得到一个输出像素，因此在一些特定的计算操作中，可以更高效地利用硬件资源。</p>\n</li>\n<li>\n<p>2.NHWC 格式更适合那些需要对不同通道的同一像素做某种运算的操作，比如 “Conv1x1”。这是因为 NHWC 格式的不同通道中的同一位置元素顺序存储，使得对不同通道的数据可以进行更高效的运算。</p>\n</li>\n<li>\n<p>3.NHWC 格式在早期的 CPU 开发中应用较多，因此对于主要基于 CPU 开发的深度学习框架和算法，NHWC 格式可能更受欢迎。</p>\n</li>\n</ul>\n<p><strong>NHWC 格式的缺点：</strong></p>\n<ul>\n<li>\n<p>1. 在使用 GPU 进行计算加速时，NHWC 格式不如 NCHW 格式高效。这是因为 NCHW 格式更符合 GPU 的内存访问模式和计算方式。</p>\n</li>\n<li>\n<p>2. 对于一些需要精细控制访存和计算的场景，NHWC 格式的控制逻辑可能相对复杂一些。</p>\n</li>\n</ul>\n<h1 id=\"im2col变换\"><a class=\"anchor\" href=\"#im2col变换\">#</a> Im2Col 变换</h1>\n<p>Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 input_2D 放在前面，也就是 (input_2D * kernel_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。</p>\n<p>依然采用下面这个简单的卷积样例，输入 tensor 按照 nhwc 排布，所以是一个 3 通道 5*5 的 input tensor。卷积核有 9 个，pad 为 1，因此输出是【1，5，5，9】。</p>\n<p><img loading=\"lazy\" data-src=\"1716464754440.jpg\" alt=\"卷积样例\"></p>\n<p>直接展开 2D 形式，两个矩阵形式如下。</p>\n<p><img loading=\"lazy\" data-src=\"1716464821580.jpg\" alt=\"Im2Col 2D示意图\"></p>\n<p><strong>上图右边是权重 kernel_2D</strong></p>\n<ul>\n<li>\n<p>1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序展成一列，也就是第一列绿色部分，共有 27 个数。</p>\n</li>\n<li>\n<p>2. 依次将余下 8 个 kernel 按照相同的方式展成一列，就得到了 kernel_2D。</p>\n</li>\n<li>\n<p>3.kernel_2D 维度为【27，9】</p>\n</li>\n</ul>\n<p>从内存排布上来看，需要将原本的 kernel_4D 进行数据重排。</p>\n<p><strong>上图左边是输入 input_2D</strong></p>\n<ul>\n<li>\n<p>1. 矩阵乘是 行 * 列；kernel_2D 一列代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一行是 “一个卷积核滑动窗口” 对应的数据，也就是 27 个。</p>\n</li>\n<li>\n<p>2. 第一行橘黄色部分是第一个滑动窗口对应的数据，未填的数代表 Pad。</p>\n</li>\n<li>\n<p>3. 滑动窗口需要纵移 5 次，每次纵移需要横移 5 次，因此有 5*5 行数据。</p>\n</li>\n<li>\n<p>4. 例如第 5 行蓝色是滑动窗口在图一中向右移动到第 5 格，蓝色格子时对应的数据。由于 Input_2D 的维度是 NHWC，也就是说图一中 “0，25，50” 三个数在内存中是相邻的（分别位于同一个 HW 位置的第一、第二、第三通道）。为了减少数据的搬运，这些连续的数被搬运到一起。即：图一中 3*3 的蓝色滑动窗口中前 3 个都对应 Pad，所以图二第 5 行蓝色行先有 3*3 (channel) 个 Pad 数据。图一蓝色滑动窗口在第 4 和第 5 个格子出现数据，所以图二 9 个 Pad 后面跟着 3，28，53；4，29，54。紧接着又出现了 Pad，以此类推。</p>\n</li>\n<li>\n<p>5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一行 27 个数据。input_2D 的维度为【25，27】</p>\n</li>\n<li>\n<p>6. 总结完 input_2D 的数据排布，那么 kernel_2D 每一列的数据具体怎么排也就清楚了，需要和 input_2D 的每一行一一对应。</p>\n</li>\n</ul>\n<p><strong>输出 output</strong></p>\n<p>input_2D【25，27】* kernel_2D 【27，9】得到结果 output_2D 【25，9】，刚好是输出 output【1，5，5，9】的内存排布方式。因此输出也不需要额外的内存转换。</p>\n<p><strong>具体实现</strong></p>\n<p>这里没有专门写 Im2Col 在 NHWC 排布情况下的代码，可以在卷积加速算法模拟下载完整的测试代码。并参考 “TestIm2FlavorConvLayer ();” 函数及文章<a href=\"https://forcheetah.github.io/2024/05/15/accelerate1/\"> im2col 卷积加速算法 NHWC</a>，完成 NHWC 内存排布情况下的算法。</p>\n<h1 id=\"矩阵乘先后顺序的影响\"><a class=\"anchor\" href=\"#矩阵乘先后顺序的影响\">#</a> 矩阵乘先后顺序的影响</h1>\n<p>以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。</p>\n<p><img loading=\"lazy\" data-src=\"1716466223424.jpg\" alt=\"两种排布情况下矩阵乘先后顺序不同对数据变换的影响\"></p>\n<p>已经讲解了简单的 im2col 算法在 NCHW 排布<a href=\"https://forcheetah.github.io/2024/05/15/accelerate1/\">上一篇文章</a>和 NHWC 排布情况下的 2D 内存排布情况。上图总结了两种排布情况下 矩阵乘先后顺序不同对数据变换的影响。</p>\n<ul>\n<li>NCHW 排布  kernel 在前  只需要对 input 做 im2col 变换</li>\n<li>NCHW 排布  input 在前  多出来对权重的转置变换 和输出的转置变换</li>\n<li>NHWC 排布  kernel 在前  需要对权重数据进行重排 以及输出进行转置</li>\n<li>NHWC 排布  input 在前  需要对权重进行转置</li>\n</ul>\n<p>当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/23/conv1/",
            "url": "https://forcheetah.github.io/2024/05/23/conv1/",
            "title": "【Im2Col】卷积加速算法【1】 NCHW",
            "date_published": "2024-05-23T11:31:49.363Z",
            "content_html": "<p>本文为最基本的 Im2Col 算法的原理及实现。<br>\n加速算法道阻且长，想要选择最优的算法，需要通盘考虑现实需求、软件算法、硬件支持，这就是 “坚持理论联系实际”。 所以这里只是对 Im2Col 算法最基本的原理探讨。<br>\n探索本就是由正确和错误交织而成，还望各位不吝赐教！</p>\n<blockquote>\n<p>现在想起来，光是遇到你这个家伙，就感觉自己赚到了。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"卷积算法\"><a class=\"anchor\" href=\"#卷积算法\">#</a> 卷积算法</h1>\n<p>将 Im2col 算法之前，不得不再提一下卷积。卷积是一种运算，在神经网络中是提取特征的过程，具体的操作过程是在输入特征中不断滑动卷积核大小的窗口，与卷积核做乘加运算，得到输出结果。</p>\n<p>先定义一下维度的符号： 卷积核 ：【C_out, C_in, Hk, Wk】   输入 ： 【B, C_in, H, W】  输出：【B, C_out, Ho, Wo】</p>\n<p><img loading=\"lazy\" data-src=\"1715770544973.jpg\" alt=\"卷积运算流程\"></p>\n<p>上图是维度非常小的一个卷积运算的图示，左边卷积核维度为【9，3，3，3】，即个数为 9，通道数为 3（个数和通道数均未在图中展示出来）的 3*3 卷积核。中间为 Input 的维度，周围方格蓝色代表 tensor 的 pad，中间黄色代表维度为【1，3，5，5】的 Input，即通道为 3，长宽 5*5。</p>\n<p>进行卷积过程中，卷积核（3*3，通道为 3）先横向滑动（5 次），再纵向滑动（5 次）；每到一个位置计算 kernel 与 Input 对应位置的乘积和，因此得到 output 5*5 的结果。9 个卷积核依次进行，得到输出大小【1，9，5，5】</p>\n<p>想要理解卷积乃至实现卷积加速算法，不仅要知道计算过程，还要格外关注数据在内存中的排布顺序。在内存中所有的数据都是一维存储的，例如 Input【1，3，5，5】，在内存中只有一个连续的、大小为 75 的数组，【1，3，5，5】只是它的逻辑维度；最后一个维度的 5 个数字是连续的（0~4），紧接着是下一行（5~9）......</p>\n<p>图中仅画出了平面的大小，通道方向就要靠大家的想象了，上图中每个格子后面还有 2 个格子（通道数为 3）。<br>\n简单的卷积更容易看清 Im2col 算法的流程，本文以及后续系列文章都将以这个例子进行。</p>\n<h1 id=\"im2col算法\"><a class=\"anchor\" href=\"#im2col算法\">#</a> Im2Col 算法</h1>\n<p>为什么要对卷积算法进行加速呢？</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> batch <span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> batch<span class=\"token operator\">&lt;</span>in_n<span class=\"token punctuation\">;</span> batch<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>        <span class=\"token keyword\">while</span> <span class=\"token punctuation\">(</span>s <span class=\"token operator\">&lt;</span> out_c<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> out_row <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> out_row <span class=\"token operator\">&lt;</span> out_h<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>out_row<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>                <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> out_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> out_col <span class=\"token operator\">&lt;</span> out_w<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>out_col<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>                    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> imap <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> imap <span class=\"token operator\">&lt;</span> in_c<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>imap<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kr <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kr <span class=\"token operator\">&lt;</span> ker_size<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>kr<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>                            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kc <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kc <span class=\"token operator\">&lt;</span> ker_size<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>kc<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>                                dosomething<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>                <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>上方是个典型的卷积算法，要实现一个卷积，不仅要遍历 batch、C_out、Ho、Wo，还要遍历整个卷积核的维度，整个循环达到了 7 层之多。当输入数据维度变大时，整个卷积占用的资源让人难以接受。提高神经网络的推理速度是推理引擎和 AI 芯片设计者不断的追求，而卷积又占了神经网络推理的大部分时间，因此卷积的加速是重中之重。<br>\nIm2Col 算法的原理就是将卷积运算<br>\n转化为矩阵运算，这一转换带来以下好处：</p>\n<ul>\n<li>\n<p>易于优化：由于 GEMM 操作在计算库中被广泛研究和优化，开发者可以利用这些库的最新进展提高效率。</p>\n</li>\n<li>\n<p>并行化和硬件支持：矩阵乘法天然适合并行处理；大多硬件平台都会充分利用其硬件资源，对矩阵乘法进行深度优化。通过使用如 CUDA、cuDNN、OpenBLAS 等库，可以实现并行计算，极大地加速卷积运算。</p>\n</li>\n<li>\n<p>内存管理灵活性：虽然 im2col 需要额外的内存来存储展开后的矩阵，但通过调整实现策略（如分块处理），可以在内存使用和计算效率之间找到平衡。</p>\n</li>\n</ul>\n<p>总之，尽管 Im2col 算法没有减少任何计算量，甚至还给内存管理带来挑战，但是矩阵的高效运算、硬件、并行等仍然能够在很多场景下提高卷积计算效率。</p>\n<h1 id=\"im2col变换\"><a class=\"anchor\" href=\"#im2col变换\">#</a> Im2Col 变换</h1>\n<p>Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 kernel_2D 放在前面，也就是 (kernel_2D * input_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。</p>\n<p>再看一遍下面这个简单的卷积样例，输入 tensor 按照 nchw 排布。</p>\n<p><img loading=\"lazy\" data-src=\"1715770588007.jpg\" alt=\"卷积样例\"></p>\n<p>直接展开 2D 形式，两个矩阵形式如下。</p>\n<p><img loading=\"lazy\" data-src=\"1715770625347.jpg\" alt=\"Im2Col 2D示意图\"></p>\n<p><strong>图 “Im2Col 2D 示意图” 左边是 kernel_2D</strong></p>\n<ul>\n<li>\n<p>1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序完全展平，也就是第一行绿色部分，共有 28 个数。</p>\n</li>\n<li>\n<p>2. 依次将余下 8 个 kernel 按照相同的方式展平，就得到了 kernel_2D。</p>\n</li>\n<li>\n<p>3.kernel_2D 维度为【9，27】</p>\n</li>\n</ul>\n<p>惊喜地发现，kernel_2D 的内存排布和 kernel_4D 完全一致，不需要任何内存搬运！</p>\n<p><strong>图 “Im2Col 2D 示意图” 右边是 Input_2D</strong></p>\n<ul>\n<li>\n<p>1. 矩阵乘是 行 * 列；kernel_2D 中，一行代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一列是 “一个卷积核滑动窗口” 对应的数据。</p>\n</li>\n<li>\n<p>2. 右图中，前半部分空数据代表 pad,  后面的大面积空白只是懒得填上数字。</p>\n</li>\n<li>\n<p>3. 例如，右图中蓝色的一列数据，代表图 2 中，卷积核滑动到蓝色窗口时对应的 input 数据。即第一个 channel：(pad,pad,pad;3,4,pad;8,9,pad)； 第二个 channel：(pad,pad,pad;28,29,pad;33,34,pad)；第三个 channel：(pad,pad,pad;53,54,pad;58,59,pad)</p>\n</li>\n<li>\n<p>4. 那么 input_2D 一行数据代表什么呢？代表 kernel 在 input 中窗口的横向移动和纵向移动。在本例子中，窗口需要纵向滑动 5 次，每次纵向滑动都要横向滑动 5 次，一共产生 25 次窗口滑动。</p>\n</li>\n<li>\n<p>5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一列 27 个数据。input_2D 的维度为【27，25】</p>\n</li>\n</ul>\n<p><strong>输出 output</strong></p>\n<p>kernel_2D 【9，27】* input_2D【27，25】得到结果 output_2D 【9，25】，刚好是输出 output【1，9，5，5】的内存排布方式。因此输出也不需要额外的内存转换。</p>\n<p><strong>具体实现</strong></p>\n<p>这里是将 input 由 nchw 转为 2D 排布的代码。如果感兴趣，可以在<a href=\"https://github.com/ForCheetah/ConvAccelerate\">卷积加速算法模拟</a> 下载完整的测试代码。并通过 “TestIm2FlavorConvLayer ();” 函数进行测试。同时函数 “TestIm2ColConvIMW” 是 NCHW 排布下 Input_2D 在前，kernel_2D 在后的 Im2Col 算法实现。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">Im2Col</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>data_im<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> channels<span class=\"token punctuation\">,</span><span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> height<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> width<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> kernel_h<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>        <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> kernel_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> pad_h<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> pad_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> stride_h<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> stride_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>data_col<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> output_h <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>height <span class=\"token operator\">+</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> pad_h <span class=\"token operator\">-</span> kernel_h <span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> stride_h <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> output_w <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>width <span class=\"token operator\">+</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> pad_w <span class=\"token operator\">-</span> kernel_w<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> stride_w <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> channel_size <span class=\"token operator\">=</span> height <span class=\"token operator\">*</span> width<span class=\"token punctuation\">;</span> </pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> channel <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> channel <span class=\"token operator\">&lt;</span> channels<span class=\"token punctuation\">;</span> channel<span class=\"token operator\">++</span><span class=\"token punctuation\">,</span> data_im <span class=\"token operator\">+=</span> channel_size<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kernel_row <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kernel_row <span class=\"token operator\">&lt;</span> kernel_h<span class=\"token punctuation\">;</span> kernel_row<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kernel_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kernel_col <span class=\"token operator\">&lt;</span> kernel_w<span class=\"token punctuation\">;</span> kernel_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>                <span class=\"token keyword\">int</span> input_row <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>pad_h <span class=\"token operator\">+</span> kernel_row<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>                <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_rows <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_rows<span class=\"token operator\">&lt;</span>output_h<span class=\"token punctuation\">;</span> output_rows<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>                    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">!</span><span class=\"token function\">is_a_ge_zero_and_a_lt_b</span><span class=\"token punctuation\">(</span>input_row<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_cols <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_cols<span class=\"token operator\">&lt;</span>output_w<span class=\"token punctuation\">;</span> output_cols<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>                            <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>                        <span class=\"token keyword\">int</span> input_col <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>pad_w <span class=\"token operator\">+</span> kernel_col<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_col<span class=\"token operator\">&lt;</span>output_w<span class=\"token punctuation\">;</span> output_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>                            <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token function\">is_a_ge_zero_and_a_lt_b</span><span class=\"token punctuation\">(</span>input_col<span class=\"token punctuation\">,</span> width<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>                                <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> data_im<span class=\"token punctuation\">[</span>input_row <span class=\"token operator\">*</span> width <span class=\"token operator\">+</span> input_col<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>                                <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>                            input_col <span class=\"token operator\">+=</span> stride_w<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>                    input_row <span class=\"token operator\">+=</span> stride_h<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"27\"></td><td><pre>                <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre>            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"30\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"31\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><h1 id=\"矩阵乘先后顺序的影响\"><a class=\"anchor\" href=\"#矩阵乘先后顺序的影响\">#</a> 矩阵乘先后顺序的影响</h1>\n<p>以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。</p>\n<p><img loading=\"lazy\" data-src=\"1715770640439.jpg\" alt=\"Kernel_2D * Input_2D\"><br>\n 在输入为 nchw 排布，输出也是 nchw 排布情况下，kernel_2D 在前，Input_2D 在后，只需要对 Input 进行 Im2Col 变换。</p>\n<p><img loading=\"lazy\" data-src=\"1715770657999.jpg\" alt=\"Input_2D * Kernel_2D\"><br>\n 在输入为 nchw 排布，输出也是 nchw 排布情况下，Input_2D 在前，kernel_2D 在后，多出了对 kernel 和 output 的转置操作。</p>\n<p>当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/15/openBlas/",
            "url": "https://forcheetah.github.io/2024/05/15/openBlas/",
            "title": "openBlas库的安装与简单使用",
            "date_published": "2024-05-15T12:27:08.224Z",
            "content_html": "<h1 id=\"编译\"><a class=\"anchor\" href=\"#编译\">#</a> 编译</h1>\n<p>在官网上下载 zip 或通过 git 下载：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>weget https://github.com/OpenMathLib/OpenBLAS.git</pre></td></tr></table></figure><p>将文件解压后进入工程主目录</p>\n<p>编译 openblas 库</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">make</span> <span class=\"token parameter variable\">-j32</span></pre></td></tr></table></figure><p>导出 依赖库</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">make</span> <span class=\"token function\">install</span> <span class=\"token assign-left variable\">PREFIX</span><span class=\"token operator\">=</span>/path/to/your/admire</pre></td></tr></table></figure><h1 id=\"cblas_sgemm函数\"><a class=\"anchor\" href=\"#cblas_sgemm函数\">#</a> cblas_sgemm 函数</h1>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"><span>c</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">cblas_sgemm</span><span class=\"token punctuation\">(</span>order<span class=\"token punctuation\">,</span> transA<span class=\"token punctuation\">,</span> transB<span class=\"token punctuation\">,</span> M<span class=\"token punctuation\">,</span> N<span class=\"token punctuation\">,</span> K<span class=\"token punctuation\">,</span> ALPHA<span class=\"token punctuation\">,</span> A<span class=\"token punctuation\">,</span> LDA<span class=\"token punctuation\">,</span> B<span class=\"token punctuation\">,</span> LDB<span class=\"token punctuation\">,</span> BETA<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">,</span> LDC<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>order <span class=\"token operator\">:</span> ClasRowMajow 行优先  ClasColMajow 列优先</pre></td></tr><tr><td data-num=\"4\"></td><td><pre>transA <span class=\"token operator\">:</span> CblasTrans  转置     CblasNoTrans 不转置</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>transA <span class=\"token operator\">:</span> CblasTrans  转置     CblasNoTrans 不转置</pre></td></tr><tr><td data-num=\"6\"></td><td><pre>M <span class=\"token operator\">:</span> 表示 A或C的行数。如果A转置，则表示转置后的行数    A的行数（transA处理之后）</pre></td></tr><tr><td data-num=\"7\"></td><td><pre>N <span class=\"token operator\">:</span> 表示 B或C的列数。如果B转置，则表示转置后的列数    B的列数（transB处理之后）</pre></td></tr><tr><td data-num=\"8\"></td><td><pre>K <span class=\"token operator\">:</span> K <span class=\"token operator\">=</span> A的列数 <span class=\"token operator\">=</span> B的行数</pre></td></tr><tr><td data-num=\"9\"></td><td><pre>ALPHA <span class=\"token operator\">:</span> <span class=\"token function\">ALPHA</span><span class=\"token punctuation\">(</span>A<span class=\"token operator\">*</span>B<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span>BETA<span class=\"token operator\">*</span>C  矩阵权重</pre></td></tr><tr><td data-num=\"10\"></td><td><pre>A <span class=\"token operator\">:</span> 输入矩阵<span class=\"token function\">A</span>  <span class=\"token punctuation\">(</span>M<span class=\"token operator\">*</span>K<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>LDA <span class=\"token operator\">:</span> A所在矩阵的列数   有可能 LAD<span class=\"token operator\">></span>K</pre></td></tr><tr><td data-num=\"12\"></td><td><pre>B <span class=\"token operator\">:</span> 输入矩阵<span class=\"token function\">B</span>  <span class=\"token punctuation\">(</span>K<span class=\"token operator\">*</span>N<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>LDB <span class=\"token operator\">:</span> B所在矩阵的列数   有可能 LDB<span class=\"token operator\">></span>N</pre></td></tr><tr><td data-num=\"14\"></td><td><pre>BETA <span class=\"token operator\">:</span> <span class=\"token function\">ALPHA</span><span class=\"token punctuation\">(</span>A<span class=\"token operator\">*</span>B<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span>BETA<span class=\"token operator\">*</span>C  偏置权重</pre></td></tr><tr><td data-num=\"15\"></td><td><pre>C <span class=\"token operator\">:</span> 偏置C  同时也是输出<span class=\"token function\">C</span>  <span class=\"token punctuation\">(</span>M<span class=\"token operator\">*</span>N<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>LDC <span class=\"token operator\">:</span> C所在矩阵的列数   有可能 LDC<span class=\"token operator\">></span>N</pre></td></tr></table></figure><h1 id=\"使用例子\"><a class=\"anchor\" href=\"#使用例子\">#</a> 使用例子</h1>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;vector></span></span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;iostream></span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;cblas.h></span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>using namespace std<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> M<span class=\"token operator\">=</span><span class=\"token number\">4</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> N<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> K<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span> alpha<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span> beta<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> lda<span class=\"token operator\">=</span>K<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> ldb<span class=\"token operator\">=</span>N<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> ldc<span class=\"token operator\">=</span>N<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span> A<span class=\"token punctuation\">[</span>M<span class=\"token operator\">*</span>K<span class=\"token punctuation\">]</span><span class=\"token operator\">=</span><span class=\"token punctuation\">&#123;</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span><span class=\"token number\">9</span><span class=\"token punctuation\">,</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span> B<span class=\"token punctuation\">[</span>K<span class=\"token operator\">*</span>N<span class=\"token punctuation\">]</span><span class=\"token operator\">=</span><span class=\"token punctuation\">&#123;</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>    <span class=\"token keyword\">float</span> C<span class=\"token punctuation\">[</span>M<span class=\"token operator\">*</span>N<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>    </pre></td></tr><tr><td data-num=\"19\"></td><td><pre>    <span class=\"token function\">cblas_sgemm</span><span class=\"token punctuation\">(</span>CblasRowMajor<span class=\"token punctuation\">,</span> CblasNoTrans<span class=\"token punctuation\">,</span> CblasNoTrans<span class=\"token punctuation\">,</span> M<span class=\"token punctuation\">,</span> N<span class=\"token punctuation\">,</span> K<span class=\"token punctuation\">,</span> alpha<span class=\"token punctuation\">,</span> A<span class=\"token punctuation\">,</span> lda<span class=\"token punctuation\">,</span> B<span class=\"token punctuation\">,</span> ldb<span class=\"token punctuation\">,</span> beta<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">,</span> ldc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>    </pre></td></tr><tr><td data-num=\"21\"></td><td><pre>    <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> i<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span>i<span class=\"token operator\">&lt;</span>M<span class=\"token punctuation\">;</span>i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>        <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> j<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span>j<span class=\"token operator\">&lt;</span>N<span class=\"token punctuation\">;</span>j<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>            cout<span class=\"token operator\">&lt;&lt;</span>C<span class=\"token punctuation\">[</span>i<span class=\"token operator\">*</span>N<span class=\"token operator\">+</span>j<span class=\"token punctuation\">]</span><span class=\"token operator\">&lt;&lt;</span><span class=\"token string\">\" \"</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span>   </pre></td></tr><tr><td data-num=\"25\"></td><td><pre>        cout<span class=\"token operator\">&lt;&lt;</span>endl<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span>  </pre></td></tr><tr><td data-num=\"27\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>对应的 CMakeLists.txt 文件</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>cmake_minimum_required <span class=\"token punctuation\">(</span>VERSION <span class=\"token number\">3.5</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>project <span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>include_directories <span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/include</pre></td></tr><tr><td data-num=\"6\"></td><td><pre>                    /path/to/your/admire/include<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>find_library<span class=\"token punctuation\">(</span>OpenBlas_Lib openblas HINTS /path/to/your/admire/lib<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre></pre></td></tr><tr><td data-num=\"10\"></td><td><pre></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>AUX_SOURCE_DIRECTORY<span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src   SRC_FILE<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre><span class=\"token builtin class-name\">set</span> <span class=\"token punctuation\">(</span>SRC_FILE <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/test.cpp<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>add_executable <span class=\"token punctuation\">(</span>demo <span class=\"token variable\">$&#123;SRC_FILE&#125;</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>target_link_libraries <span class=\"token punctuation\">(</span>demo <span class=\"token variable\">$&#123;OpenBlas_Lib&#125;</span> m<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>在工程主目录</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>./demo</pre></td></tr></table></figure><h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "Linux",
                "openBlas"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/14/cpplib/",
            "url": "https://forcheetah.github.io/2024/05/14/cpplib/",
            "title": "C语言工程调用Cpp库解决方案",
            "date_published": "2024-05-14T12:37:34.628Z",
            "content_html": "<p>本文为 C 语言工程调用 C++ 库的解决方案。</p>\n<p>应用场景： 需要 C++ 程序编译成的库提供函数接口，来解决 C 语言工程的需求。</p>\n<p>想要快速解决问题，直接看 三、通用解决方法</p>\n<blockquote>\n<p>人的出场顺序真的很重要，很多人如果换一个时间认识，换一个时间共处，一切都将是不一样的场景，不一样的结局。所以，人生有无限种可能，我的人生，是现在这一种。感谢大家恰到好处的出现，组成我最好的一种可能。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"c库可以编辑的情况\"><a class=\"anchor\" href=\"#c库可以编辑的情况\">#</a> C++ 库可以编辑的情况</h1>\n<p>C++ 库由自己编写，可以决定头文件书写的位置</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// hello.cpp</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"hello.h\"</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"iostream\"</span> <span class=\"token comment\">// 将用到的 C++ 标准库，如 iostream，放在本 cpp 文件中</span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">using</span> <span class=\"token keyword\">namespace</span> std<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    cout<span class=\"token operator\">&lt;&lt;</span> <span class=\"token string\">\" # iostream: i am saying hello !\"</span> <span class=\"token operator\">&lt;&lt;</span> endl<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\" # c: i am saying hello !\\n\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>创建 hello.cpp 文件，实现 sayHello () 功能，分别用 C++ 标准库和 C 标准库的输入输出功能打印 hello！</p>\n<p>①需要注意的是 “将 C++ 标准库放在该 cpp 文件中”，原因后续指出。</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// hello.h</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"stdio.h\"</span>  <span class=\"token comment\">// 这里不能出现 C++ 标准库</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>创建头文件 hello.h   这里可以添加 C 语言标准库，但是不要把 C++ 标准库放在这里。</p>\n<p>以上 hello.h 和 hello.cpp 模拟了 C++ 库。为了使 C 语言工程能够调用该库，需要增加一个中间层</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// helloWapper.cpp</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"helloWapper.h\"</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>        <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>创建中间层 helloWapper.cpp ， 对想要使用的 C++ 库函数进行封装，即： 通过 Wapper_sayHello () 调用 sayHello ()</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// helloWapper.h</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"hello.h\"</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">ifdef</span> <span class=\"token expression\">__cplusplus</span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">extern</span> <span class=\"token string\">\"C\"</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">endif</span></span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token keyword\">void</span> <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">ifdef</span> <span class=\"token expression\">__cplusplus</span></span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">endif</span></span></pre></td></tr></table></figure><p>创建中间层头文件 helloWapper.h，暴露 Wapper_sayHello () 接口。中间出现的 extern &quot;C&quot; {} 是告诉 G++ 编译器，对中间的函数按照 C 语言的方式进行编译。</p>\n<p>然后将上述两个 CPP 文件编译成 静态库 ，使用的 CMakeLists.txt 文件如下：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>cmake_minimum_required <span class=\"token punctuation\">(</span>VERSION <span class=\"token number\">3.5</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>project <span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>include_directories <span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/include <span class=\"token punctuation\">)</span> <span class=\"token comment\"># 指定头文件位置</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token builtin class-name\">set</span> <span class=\"token punctuation\">(</span>test_call_LIST <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/hello.cpp</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>                    <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/helloWapper.cpp<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 指定需要编译的 CPP 文件</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>set<span class=\"token punctuation\">(</span>LIBRARY_OUTPUT_PATH <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 指定库输出路径</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>add_library<span class=\"token punctuation\">(</span>hello STATIC <span class=\"token variable\">$&#123;test_call_LIST&#125;</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 编译</span></pre></td></tr></table></figure><p>所有文件结构如下：<br>\n<img loading=\"lazy\" data-src=\"1715602113090.jpg\" alt=\"文件结构图\"></p>\n<p>在工程根目录输入</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span> </pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span></pre></td></tr></table></figure><p>bin 文件夹下就会出现 封装好的库 libhello.a</p>\n<p>创建 mian.c 来模拟 C 语言工程：</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"stdio.h\"</span></span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"helloWapper.h\"</span>  <span class=\"token comment\">// 调用 C++ 库接口</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><strong>A：通过命令行进行编译运行</strong></p>\n<p>回到工程根目录，编译 main.c ， 运行：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token builtin class-name\">cd</span> <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>gcc <span class=\"token parameter variable\">-o</span> Hello ./src/main.c -I./include -L./bin <span class=\"token parameter variable\">-lhello</span> -lstdc++</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>./Hello</pre></td></tr></table></figure><p><strong>B：通过 Cmake 编译运行</strong></p>\n<p>在工程根目录，将 CMakeList.txt 内容替换为：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># CMakeList.txt</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake_minimum_required <span class=\"token punctuation\">(</span>VERSION <span class=\"token number\">3.5</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>project <span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>include_directories <span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/include<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 指定头文件目录</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>find_library<span class=\"token punctuation\">(</span>Hello_LIB hello HINTS <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 引入 libhello.a 静态库</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token builtin class-name\">set</span> <span class=\"token punctuation\">(</span>hello_List <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/main.c<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>add_executable <span class=\"token punctuation\">(</span>Hello <span class=\"token variable\">$&#123;hello_List&#125;</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 默认 gcc 编译 main.c</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>target_link_libraries <span class=\"token punctuation\">(</span>Hello <span class=\"token variable\">$&#123;Hello_LIB&#125;</span> stdc++<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token builtin class-name\">cd</span> build <span class=\"token operator\">&amp;&amp;</span> cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token function\">make</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>./Hello</pre></td></tr></table></figure><p>A B 两种方式均输出结果： 至此 C 语言工程能够成功调用 C++ 库</p>\n<p><img loading=\"lazy\" data-src=\"1715602427908.jpg\" alt=\"输出结果\"></p>\n<p>以上 A B 两种方式中均出现 stdc++ ，一般以 libstdc++.so 的方式存在，是 C++ 标准库。</p>\n<p>可以看下图 “G++ and GCC”</p>\n<p><img loading=\"lazy\" data-src=\"1715602427908.jpg\" alt=\" “G++ and GCC”\"></p>\n<p>GCC 在编译时不会自动链接 C++ 标准库， 因此 hello.cpp 用到的 类似 &quot;iostream&quot; 等 C++ 标准库需要手动链接，否则会出现以下错误：</p>\n<p><img loading=\"lazy\" data-src=\"1715686800105.jpg\" alt=\"找不到C++标准库错误\"></p>\n<h1 id=\"c库为第三方库无法编辑的情况\"><a class=\"anchor\" href=\"#c库为第三方库无法编辑的情况\">#</a> C++ 库为第三方库，无法编辑的情况</h1>\n<p>在《一、C<ins> 库可以编辑的情况》 中提到  【①需要注意的是 “ 将 C</ins> 标准库放在该 cpp 文件中”】 是因为 GCC 编译不仅找不到 C<ins> 标准库 stdc</ins> ，也找不到 C++ 标准库的头文件。如果在 hello.h 中引用 “iostream&quot;， 那么用 GCC 编译 C 语言工程时，会报找不到头文件错误。</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// hello.h</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"stdio.h\"</span>  </span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"iostream\"</span>  <span class=\"token comment\">// 这里出现了 C++ 标准库</span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>但是当我们想用的 C<ins> 库为第三方库，而它在头文件里引用了大量 C</ins> 标准库的情况下，该如何处理呢？</p>\n<p>再看图 “G++ and GCC”，里面指明 G<ins> 编译器能够编译 C 和 C</ins> 文件，且能够自动链接 C<ins> 标准库。所以在这种情况下，只需要在编译 C 语言工程的时候，指定 G</ins> 为编译器（编译 C 文件默认使用的是 GCC 编译器）就可以了。</p>\n<p>下面是与《一、C++ 库可以编辑的情况》相似的总体流程（有修改的地方会有注释）：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// hello.cpp</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"hello.h\"</span>  <span class=\"token comment\">// C++ 标准库头文件转移到 hello.h 中</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">using</span> <span class=\"token keyword\">namespace</span> std<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    cout<span class=\"token operator\">&lt;&lt;</span> <span class=\"token string\">\" # iostream: i am saying hello !\"</span> <span class=\"token operator\">&lt;&lt;</span> endl<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\" # c: i am saying hello !\\n\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>创建 hello.cpp 文件，实现 sayHello () 功能，分别用 C++ 标准库和 C 标准库的输入输出功能打印 hello！</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// hello.h</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"stdio.h\"</span> </span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"iostream\"</span>  <span class=\"token comment\">// C++ 标准库出现在这里</span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>创建头文件 hello.h<br>\n 以上 hello.h 和 hello.cpp 模拟了 C++ 库。为了使 C 语言工程能够调用该库，需要增加一个中间层：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// helloWapper.cpp</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"helloWapper.h\"</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>        <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>创建中间层 helloWapper.cpp ， 对想要使用的 C++ 库函数进行封装，即： 通过 Wapper_sayHello () 调用 sayHello ()</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// helloWapper.h</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"hello.h\"</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">ifdef</span> <span class=\"token expression\">__cplusplus</span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">extern</span> <span class=\"token string\">\"C\"</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">endif</span></span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token keyword\">void</span> <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">ifdef</span> <span class=\"token expression\">__cplusplus</span></span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">endif</span></span></pre></td></tr></table></figure><p>创建中间层头文件 helloWapper.h，暴露 Wapper_sayHello () 接口。中间出现的 extern &quot;C&quot; {} 是告诉 G++ 编译器，对中间的函数按照 C 语言的方式进行编译。<br>\n然后将上述两个 CPP 文件编译成 静态库 ，使用的 CMakeLists.txt 文件如下：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>cmake_minimum_required <span class=\"token punctuation\">(</span>VERSION <span class=\"token number\">3.5</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>project <span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>include_directories <span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/include <span class=\"token punctuation\">)</span> <span class=\"token comment\"># 指定头文件位置</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token builtin class-name\">set</span> <span class=\"token punctuation\">(</span>test_call_LIST <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/hello.cpp</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>                    <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/helloWapper.cpp<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 指定需要编译的 CPP 文件</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>set<span class=\"token punctuation\">(</span>LIBRARY_OUTPUT_PATH <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 指定库输出路径</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>add_library<span class=\"token punctuation\">(</span>hello STATIC <span class=\"token variable\">$&#123;test_call_LIST&#125;</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 编译</span></pre></td></tr></table></figure><p>文件结构还是图：文件结构图</p>\n<p>在工程根目录输入</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span> </pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span></pre></td></tr></table></figure><p>bin 文件夹下就会出现 封装好的库 libhello.a<br>\n 创建 mian.c 来模拟 C 语言工程：</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"stdio.h\"</span></span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"helloWapper.h\"</span>  <span class=\"token comment\">// 调用 C++ 库接口</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><strong>A：通过命令行进行编译运行</strong><br>\n回到工程根目录，编译 main.c ， 运行：<br>\n编译器使用了 g++ ，可以自动搜索 C<ins> 标准库路径及链接 C</ins> 标准库，因此不需要再加 “-lstdc++”。</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token builtin class-name\">cd</span> <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>g++ <span class=\"token parameter variable\">-o</span> Hello ./src/main.c -I./include -L./bin <span class=\"token parameter variable\">-lhello</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>./Hello</pre></td></tr></table></figure><p><strong>B：通过 Cmake 编译运行</strong><br>\n在工程根目录，将 CMakeList.txt 内容替换为：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># CMakeList.txt</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake_minimum_required <span class=\"token punctuation\">(</span>VERSION <span class=\"token number\">3.5</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>project <span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>SET<span class=\"token punctuation\">(</span>CMAKE_C_COMPILER <span class=\"token string\">\"g++\"</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 指定使用  g++ 编译器进行编译</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>include_directories <span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/include<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 指定头文件目录</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>find_library<span class=\"token punctuation\">(</span>Hello_LIB hello HINTS <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 引入 libhello.a 静态库</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token builtin class-name\">set</span> <span class=\"token punctuation\">(</span>hello_List <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/main.c<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>add_executable <span class=\"token punctuation\">(</span>Hello <span class=\"token variable\">$&#123;hello_List&#125;</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>target_link_libraries <span class=\"token punctuation\">(</span>Hello <span class=\"token variable\">$&#123;Hello_LIB&#125;</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>编译、执行：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token builtin class-name\">cd</span> build <span class=\"token operator\">&amp;&amp;</span> cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token function\">make</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>./Hello</pre></td></tr></table></figure><h1 id=\"通用方法\"><a class=\"anchor\" href=\"#通用方法\">#</a> 通用方法</h1>\n<p>可以忽略方法一和二。方法三可以采用 GCC 编译调用了 C<ins> 库的 C 语言工程。<br>\n解决的方法是： 在 中间层 helloWapper.cpp 中引用 C</ins> 库头文件 #include &quot;hello.h&quot;， 而不是在 helloWapper.h 中引用</p>\n<p>例子：<br>\n创建 C++ 库源文件</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// hello.cpp</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"hello.h\"</span>  </span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>using namespace std<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    cout<span class=\"token operator\">&lt;&lt;</span> <span class=\"token string\">\" # iostream: i am saying hello !\"</span> <span class=\"token operator\">&lt;&lt;</span> endl<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\" # c: i am saying hello !\\n\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>创建头文件 hello.h</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// hello.h</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"stdio.h\"</span> </span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"iostream\"</span> </span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>以上 hello.h 和 hello.cpp 模拟了 C++ 库。为了使 C 语言工程能够调用该库，需要增加一个中间层</p>\n<p>其中注意：一定要将 C<ins> 库的头文件 &quot;hello.h&quot; 加在 中间层的.cpp 文件中， 而不是放在中间层.h 文件中。这种情况下能够将 &quot;hello.h&quot; 等所有 C</ins> 库头文件编译到库当中，防止最后采用 GCC 编译时找不到 g++ 标准库的头文件。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">// helloWapper.cpp</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"hello.h\"</span>  <span class=\"token comment\">// 一定要将 C++ 库的头文件 \"hello.h\" 加在 中间层的 cpp 文件中</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"helloWapper.h\"</span></span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>        <span class=\"token function\">sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token comment\">// helloWapper.h</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">ifdef</span> <span class=\"token expression\">__cplusplus  </span><span class=\"token comment\">// 不要把 #include \"hello.h\" 放在这里</span></span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token keyword\">extern</span> <span class=\"token string\">\"C\"</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">endif</span></span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>    <span class=\"token keyword\">void</span> <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">ifdef</span> <span class=\"token expression\">__cplusplus</span></span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">endif</span></span></pre></td></tr></table></figure><p>创建中间层头文件 helloWapper.h，暴露 Wapper_sayHello () 接口。中间出现的 extern &quot;C&quot; {} 是告诉 G++ 编译器，对中间的函数按照 C 语言的方式进行编译。</p>\n<p>然后将上述两个 CPP 文件编译成 静态库 ，使用的 CMakeLists.txt 文件如下：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>cmake_minimum_required <span class=\"token punctuation\">(</span>VERSION <span class=\"token number\">3.5</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>project <span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>include_directories <span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/include <span class=\"token punctuation\">)</span> <span class=\"token comment\"># 指定头文件位置</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token builtin class-name\">set</span> <span class=\"token punctuation\">(</span>test_call_LIST <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/hello.cpp</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>                    <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/helloWapper.cpp<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 指定需要编译的 CPP 文件</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>set<span class=\"token punctuation\">(</span>LIBRARY_OUTPUT_PATH <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 指定库输出路径</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>add_library<span class=\"token punctuation\">(</span>hello STATIC <span class=\"token variable\">$&#123;test_call_LIST&#125;</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 编译</span></pre></td></tr></table></figure><p>文件结构还是图：文件结构图</p>\n<p>在工程根目录输入</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span> </pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span></pre></td></tr></table></figure><p>bin 文件夹下就会出现 封装好的库 libhello.a<br>\n 创建 mian.c 来模拟 C 语言工程：</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"stdio.h\"</span></span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">\"helloWapper.h\"</span>  <span class=\"token comment\">// 调用 C++ 库接口</span></span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token function\">Wapper_sayHello</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><strong>A：通过命令行进行编译运行</strong><br>\n回到工程根目录，编译 main.c ， 运行：<br>\n编译器使用了 gcc ，需要添加 C<ins> 标准库 “ -lstdc</ins>”（标准库的头文件已经在 C++ 库中了）。</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>cd <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>gcc <span class=\"token operator\">-</span>o Hello <span class=\"token punctuation\">.</span><span class=\"token operator\">/</span>src<span class=\"token operator\">/</span>main<span class=\"token punctuation\">.</span>c <span class=\"token operator\">-</span>I<span class=\"token punctuation\">.</span><span class=\"token operator\">/</span>include <span class=\"token operator\">-</span>L<span class=\"token punctuation\">.</span><span class=\"token operator\">/</span>bin <span class=\"token operator\">-</span>lhello <span class=\"token operator\">-</span>lstdc<span class=\"token operator\">++</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token operator\">/</span>Hello</pre></td></tr></table></figure><p><strong>B：通过 Cmake 编译运行</strong><br>\n在工程根目录，将 CMakeList.txt 内容替换为：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># CMakeList.txt</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake_minimum_required <span class=\"token punctuation\">(</span>VERSION <span class=\"token number\">3.5</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>project <span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>include_directories <span class=\"token punctuation\">(</span><span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/include<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 指定头文件目录</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>find_library<span class=\"token punctuation\">(</span>Hello_LIB hello HINTS <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 引入 libhello.a 静态库</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token builtin class-name\">set</span> <span class=\"token punctuation\">(</span>hello_List <span class=\"token variable\">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/main.c<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>add_executable <span class=\"token punctuation\">(</span>Hello <span class=\"token variable\">$&#123;hello_List&#125;</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>target_link_libraries <span class=\"token punctuation\">(</span>Hello <span class=\"token variable\">$&#123;Hello_LIB&#125;</span> stdc++<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>编译、执行：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token builtin class-name\">cd</span> build <span class=\"token operator\">&amp;&amp;</span> cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token function\">make</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>./Hello</pre></td></tr></table></figure><p>方法三是最通用的解决方法，不仅仍然可以使用 GCC 编译，而且想要在其他地方调用 C++ 库时，需要的头文件只有 “helloWapper.h” 一个。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "Linux",
                "lib"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/12/test/",
            "url": "https://forcheetah.github.io/2024/05/12/test/",
            "title": "foo",
            "date_published": "2024-05-12T13:14:18.428Z",
            "content_html": "<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"quick-start\"><a class=\"anchor\" href=\"#quick-start\">#</a> Quick Start</h2>\n<h3 id=\"create-a-new-post\"><a class=\"anchor\" href=\"#create-a-new-post\">#</a> Create a new post</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo new <span class=\"token string\">\"My New Post\"</span></pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"run-server\"><a class=\"anchor\" href=\"#run-server\">#</a> Run server</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo server</pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"generate-static-files\"><a class=\"anchor\" href=\"#generate-static-files\">#</a> Generate static files</h3>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"><span>c</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdio.h></span></span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Hello, world!\\n\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"deploy-to-remote-sites\"><a class=\"anchor\" href=\"#deploy-to-remote-sites\">#</a> Deploy to remote sites</h3>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"><span>p</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;iostream></span></span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    std<span class=\"token double-colon punctuation\">::</span>cout <span class=\"token operator\">&lt;&lt;</span> <span class=\"token string\">\"Hello, world!\"</span> <span class=\"token operator\">&lt;&lt;</span> std<span class=\"token double-colon punctuation\">::</span>endl<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n",
            "tags": [
                "bar",
                "baz"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/12/hello-world/",
            "url": "https://forcheetah.github.io/2024/05/12/hello-world/",
            "title": "Hello World",
            "date_published": "2024-05-12T05:14:13.974Z",
            "content_html": "<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h1 id=\"模板\"><a class=\"anchor\" href=\"#模板\">#</a> 模板</h1>\n<p>未删除的模板文件。</p>\n<h2 id=\"quick-start\"><a class=\"anchor\" href=\"#quick-start\">#</a> Quick Start</h2>\n<h3 id=\"create-a-new-post\"><a class=\"anchor\" href=\"#create-a-new-post\">#</a> Create a new post</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo new <span class=\"token string\">\"My New Post\"</span></pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"run-server\"><a class=\"anchor\" href=\"#run-server\">#</a> Run server</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo server</pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"generate-static-files\"><a class=\"anchor\" href=\"#generate-static-files\">#</a> Generate static files</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo generate</pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"deploy-to-remote-sites\"><a class=\"anchor\" href=\"#deploy-to-remote-sites\">#</a> Deploy to remote sites</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo deploy</pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n",
            "tags": []
        }
    ]
}
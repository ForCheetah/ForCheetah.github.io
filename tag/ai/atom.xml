<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://forcheetah.github.io</id>
    <title>Пусть этот камень будет более крепким, чем человек • Posts by &#34;ai&#34; tag</title>
    <link href="https://forcheetah.github.io" />
    <updated>2025-02-05T11:34:55.219Z</updated>
    <category term="bar" />
    <category term="baz" />
    <category term="Linux" />
    <category term="openBlas" />
    <category term="lib" />
    <category term="accelerate" />
    <category term="conv" />
    <category term="tvm" />
    <category term="tengine" />
    <category term="ncnn" />
    <category term="cmake" />
    <category term="runtime" />
    <category term="tank" />
    <category term="zatan" />
    <category term="systemc" />
    <category term="novel" />
    <category term="compile" />
    <category term="AI" />
    <category term="gemm" />
    <category term="quantize" />
    <category term="compiler" />
    <entry>
        <id>https://forcheetah.github.io/2025/02/05/aicompile04/</id>
        <title>【AI编译】如何进行内存分配</title>
        <link rel="alternate" href="https://forcheetah.github.io/2025/02/05/aicompile04/"/>
        <content type="html">&lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;本文讲解神经网络推理过程中的内存分配相关内容。&lt;/p&gt;
&lt;p&gt;作为初学者，错误在所难免，还望不吝赐教。&lt;/p&gt;
&lt;h1 id=&#34;tensor类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#tensor类型&#34;&gt;#&lt;/a&gt; Tensor 类型&lt;/h1&gt;
&lt;p&gt;不同的 tensor 有着不同的生命周期，神经网络推理过程中主要有三种 Tensor 类型：&lt;br&gt;
1.&lt;strong&gt; 输入输出 tensor&lt;/strong&gt;&lt;br&gt;
 输出 tensor 是下一节点的输入 tensor，它们一体两面，这种类型的 tensor 生命周期起于 “生产节点”，终于最后一个 “消费节点”。&lt;br&gt;
2.&lt;strong&gt; 权重 tensor&lt;/strong&gt;&lt;br&gt;
 权重 tensor 和算子绑定在一起，生命周期随着算子开始，也随算子结束。但也可能存在共享权重的情况，例如 tiling 操作产生的并行算子共享 Weight，&lt;a href=&#34;https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem&#34;&gt;算能 TPU 选择将 weight 常驻内存&lt;/a&gt;。&lt;br&gt;
3.&lt;strong&gt; 中间 tensor&lt;/strong&gt;&lt;br&gt;
 中间 tensor 指的是算子计算时不得不开辟的临时空间，比如用来存储中间结果等，其大小和算子的具体实现有关。中间 tensor 的生命周期和权重 tensor 类似，随着算子开始，也随算子结束，但没有权重 tensor 的共享问题。&lt;/p&gt;
&lt;h1 id=&#34;原地操作inplace-operation&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#原地操作inplace-operation&#34;&gt;#&lt;/a&gt; 原地操作（Inplace Operation）&lt;/h1&gt;
&lt;p&gt;原地覆盖不再需要的数据，不再开辟新的输出 tensor 空间。例如 Element-wise 算子、Relu 算子等可以在原地进行操作。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1738754981561.jpg&#34; alt=&#34;图 原地操作举例&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，B、C、D 三个 tensor 可以共用同一个内存空间。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1738755079647.jpg&#34; alt=&#34;图 原地操作举例2&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图，当 B 的生命周期未结束时，C 不能共用 B 的内存空间。&lt;/p&gt;
&lt;h1 id=&#34;内存共享memory-sharing&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#内存共享memory-sharing&#34;&gt;#&lt;/a&gt; 内存共享（Memory Sharing）&lt;/h1&gt;
&lt;p&gt;分析张量的生命周期，生命周期结束的张量及时释放，其所占据的空间可以被重复利用。拿算能 TPU 网站的图来举一个例子。如下图所示，这是一段简单的网络，有三个算子，分别是 Conv、Conv、和 Add。两个黑色的圆圈是权重，其余绿色的圆圈是输入输出 tensor 和中间 tensor。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1738755160616.jpg&#34; alt=&#34;图 算能TPU网络举例&#34;&gt;&lt;/p&gt;
&lt;p&gt;图源：&lt;a href=&#34;https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem&#34;&gt;算能 TPU 网络举例&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1738755172523.jpg&#34; alt=&#34;图 算能TPU网络举例2&#34;&gt;&lt;/p&gt;
&lt;p&gt;图源：&lt;a href=&#34;https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem&#34;&gt;算能 TPU 内存生命周期&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上图展示了所有 tensor 的生命周期，三个算子的周期分别标记为 T2，T5 和 T7。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;0 号 tensor 是首个 Conv 节点的输入 tensor，W1 是权重，所以 T2 开始 T2 结束，及时释放；&lt;/li&gt;
&lt;li&gt;3 号 tensor 有两个消费节点，所以需要到 T7 才释放；&lt;br&gt;
释放后的空间可以重复利用。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;简单的分析规则&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#简单的分析规则&#34;&gt;#&lt;/a&gt; 简单的分析规则&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1738755220468.jpg&#34; alt=&#34;图 分析规则&#34;&gt;&lt;/p&gt;
&lt;p&gt;这里给出一个基本的 Tensor 生命周期分析流程，如图所示。可以通过这个流程来确定输入输出的生命周期。&lt;/p&gt;
&lt;p&gt;先初始化 tensor 的生命周期，它等于 tensor 的出度，也就是后面连着几个 “消费者节点”。&lt;br&gt;
再按照执行顺序遍历网络结构，①如果当前节点的输入 Input 的生命周期大于 1，意味着它不能被释放，也不能被覆盖，需要为 Ouput 开辟新空间，同时 Input 生命周期减一；②如果当前节点的输入 Input 的生命周期等于 1，则进一步判断是否是 Inplace 节点；③如果不是 Inplace 节点，则仍需要为 Output 开辟空间，计算完成后 Input 可以释放；④是 Inplace 节点，则可以原地计算。&lt;/p&gt;
&lt;h1 id=&#34;内存拼图游戏&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#内存拼图游戏&#34;&gt;#&lt;/a&gt; 内存拼图游戏&lt;/h1&gt;
&lt;p&gt;在完成网络的 Tensor 生命周期分析之后，就要尝试进行实际的内存分配，以确定设备内存是否足够放得下。建立一个坐标系，以横轴为时间 (Time)，纵轴为空间 (Addr)，把所有 tensor 作为一个图块拼进去，图块与图块之间不能有交叠，就像一个拼图游戏。&lt;br&gt;
下图是算能 TPU 给出的内存分配例子，可供参考。不过这个图是以横轴为空间 (Addr)，纵轴为时间 (Time)。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1738755258225.jpg&#34; alt=&#34;图 算能TPU内存分配&#34;&gt;&lt;/p&gt;
&lt;p&gt;图源：&lt;a href=&#34;https://tpumlir.org/docs/developer_manual/10_layergroup.html#mem&#34;&gt;算能 TPU 内存分配&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个拼图游戏并没有看起来那么容易，当网络结构复杂的时候，tensor 大小多样，拼图的方式也多样。不同的拼图方式可能直接决定内存分配的成功与失败。这里举个简单的例子，如图所示：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1738755278680.jpg&#34; alt=&#34;图 内存分配例子&#34;&gt;&lt;/p&gt;
&lt;p&gt;假设有三个内存块需要分配，他们所占空间和生命周期都已经确定。设备内存只有 20 大小。假如按照左图的拼图方式，会发现图块③没有空间存放，超出内存限制。而采用右图的拼图方式，内存分配就顺利实现。&lt;/p&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
</content>
        <category term="compile" />
        <category term="AI" />
        <updated>2025-02-05T11:34:55.219Z</updated>
    </entry>
    <entry>
        <id>https://forcheetah.github.io/2025/01/15/aicompile03/</id>
        <title>【AI编译】layer-group之后如何tiling</title>
        <link rel="alternate" href="https://forcheetah.github.io/2025/01/15/aicompile03/"/>
        <content type="html">&lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;本篇讲解笔者实现 tiling 算法的一些经验。&lt;br&gt;
前述文章 &lt;a href=&#34;https://forcheetah.github.io/2025/01/14/aicompile02/&#34;&gt;《如何进行 layer-group》&lt;/a&gt;讲解了 layer group 的内容。&lt;a href=&#34;https://forcheetah.github.io/2024/10/18/aicompile01/&#34;&gt;《Tiling 操作能优化哪些时间》&lt;/a&gt;提到 Tiling 的概念和作用。感兴趣的话可以阅读。&lt;/p&gt;
&lt;p&gt;本篇文章参考过 &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1787593090401250411&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;《超强干货！地平线编译器大牛的编译优化实践总结》&lt;/a&gt;，&lt;a href=&#34;https://github.com/Arm-China/Compass_Optimizer&#34;&gt;《Arm 周易编译器工程》&lt;/a&gt;，&lt;a href=&#34;https://tpumlir.org/docs/developer_manual/10_layergroup.html#group&#34;&gt;《算能 TPU layer group 讲解》&lt;/a&gt;，&lt;a href=&#34;https://www.bilibili.com/video/BV1wo4y1z7AG/&#34;&gt;《算能 TPU 视频讲解》&lt;/a&gt; 等文章和工程，欢迎大家参考。&lt;/p&gt;
&lt;p&gt;作为初学者，错误在所难免，还望不吝赐教。&lt;/p&gt;
&lt;h1 id=&#34;回顾&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#回顾&#34;&gt;#&lt;/a&gt; 回顾&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736946396882.jpg&#34; alt=&#34;回顾流程&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，AI 编译优化的基本流程是 1. 图优化 (算子融合，常量折叠等) 2. 拆分 (layer group 和 tiling) 3. 并行和调度。最后得到当前编译的时间消耗。&lt;/p&gt;
&lt;h1 id=&#34;分支结构tiling&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#分支结构tiling&#34;&gt;#&lt;/a&gt; 分支结构 Tiling&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736946520498.jpg&#34; alt=&#34;分支结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;假设现在对某个 layer group 做 tiling，需要对当前这个 layer group 按照 tiling 块数构建平行分支。&lt;/p&gt;
&lt;p&gt;上图来自&lt;a href=&#34;https://tpumlir.org/docs/developer_manual/10_layergroup.html#group&#34;&gt;《算能 TPU layer group 讲解》&lt;/a&gt;。如图所示，含有三个卷积算子的 layer group ，将其 tiling 成四块，则 group 的图结构变成右边的形状，图上也表明了 tiling 之后 tensor 的大小。可以使用 crop 算子完成 tensor 的切分，使用 concat 算子实现 tensor 的合并。图中没有画出 crop 算子和 concat 算子。&lt;/p&gt;
&lt;p&gt;神经网络结构复杂多样，分支结构、多前序节点、多后续节点等给 tiling 的实际操作带来很大困难。根据前述的 layer group 动态搜索划分，我们可能需要在各种各样的结构上进行 tiling 操作。下图是一种较为复杂的分支结构在 tiling 前后的对比图，可以给 tiling 结构的创建带来更直观的感受。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736946566779.jpg&#34; alt=&#34;复杂分支结构tiling&#34;&gt;&lt;/p&gt;
&lt;p&gt;这是一个虚构的、较为复杂的分支结构。中间彩色的五个节点是当前要做 tiling 的 layer group，而上方和下方是当前 layer group 的前序节点和后续节点。&lt;br&gt;
为方便，只 tiling 成两块。&lt;/p&gt;
&lt;p&gt;tiling 后的结构如图右侧所示，原有彩色算子部分复制成两份；layer group 有几个输出端（当前 group 有两个输出端：Conv 和 Fc）就有几个 concat 节点。layer group 的输入端（Add 节点和 Relu 节点）前面要添加 crop 节点，当输入端有多个前序节点时（Add 有两个前序节点），每个都要添加 crop。&lt;/p&gt;
&lt;p&gt;大致的结构重建就是这个样子。在实际操作中可能还有些不同，例如为保持所有节点都只能有一个输出 tensor 的原则，Crop 节点不能输出两个 tensor。&lt;/p&gt;
&lt;h1 id=&#34;tiling维度&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#tiling维度&#34;&gt;#&lt;/a&gt; Tiling 维度&lt;/h1&gt;
&lt;p&gt;现有方案基本都选择在 N H W，也就是批次、高度、宽度这三个维度上做 tiling。&lt;/p&gt;
&lt;p&gt;N（批次）方向分块是最简单的方案，可以随意划分，对整个网络的运行没有影响。但是也许模型在推理的时候批次只有 1，无法 tiling。所以批次方向不是在所有情况下都适用。&lt;/p&gt;
&lt;p&gt;高度和宽度方向 tiling，比批次方向稍复杂，需要重新 infer shape 和调整参数（下一节讲解），但好在原有的算子推理仍然适用。&lt;/p&gt;
&lt;p&gt;基本上不考虑 C（通道）方向 tiling。通道方向分块，不仅要调整权重偏置 tensor，还需要在算子运算后加减 tensor，破坏了原有算子推理。&lt;br&gt;
现有方案基本支持批次、高度、宽度这三个维度上做 tiling。&lt;/p&gt;
&lt;h1 id=&#34;tiling逆向推理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#tiling逆向推理&#34;&gt;#&lt;/a&gt; Tiling 逆向推理&lt;/h1&gt;
&lt;p&gt;上面只完成了 layer group 在 tiling 之后的结构重建。想要实现模型的正确推理，还需要更新好多信息。&lt;/p&gt;
&lt;h2 id=&#34;逆向shape-推理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#逆向shape-推理&#34;&gt;#&lt;/a&gt; 逆向 shape 推理&lt;/h2&gt;
&lt;p&gt;推理 shape，在 tensor 分块后，维度大小自然需要更新。下图是一个简单的分支结构的 tensor shape 推理例子。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736946730182.jpg&#34; alt=&#34;逆向推理&#34;&gt;&lt;/p&gt;
&lt;p&gt;图①是推理之前的网络结构，仅包含两个卷积算子（均为 3*3，pad=1）。图上已经标明了每个算子的输入输出 tensor 的大小。现在将此 group 从 H（高）维度 tiling 成两份，也就是输出端（Add 节点）的输出 tensor 设置为 5*10。&lt;/p&gt;
&lt;p&gt;从输出端（Add 节点）开始逆序推导各个算子的输入输出 shape。如图②，普通算子如 Add，输入 tensor 的 shape 于输出 tensor 相同。而 Conv 这样的算子，需要根据参数、所在位置、输出 shape 来计算输入 tensor 的 shape。例如图②中 Conv_2 的输入变成 [6,10]，参数中 pad 需要修改为 [1,0,1,1]。&lt;/p&gt;
&lt;p&gt;现在出现一个问题：从 Add 节点直接往前推，Conv_1 的输出应该是 [5,10]，而走 Add-&amp;gt;Conv_2 路径，Conv_1 的输出应该是 [6,10]。Conv_1 不可能同时输出 [6,10] 和 [5,10] 两个 tensor。如何处理当前的歧义呢？&lt;/p&gt;
&lt;p&gt;也许增加一个 crop 节点是个可选的方法。如图③所示，增加一个新的 Crop 节点，将 Conv_1 的输出 tensor 从 [6,10] 变成 [5,10]。&lt;/p&gt;
&lt;h2 id=&#34;逆向深度优先搜索&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#逆向深度优先搜索&#34;&gt;#&lt;/a&gt; 逆向深度优先搜索&lt;/h2&gt;
&lt;p&gt;从逆向 shape 推理过程的分析来看，更新 layer group 的 shape，需要从输出端开始，逆向深度优先搜索每个分支，更新 shape 信息，还得判断采用哪些信息（如上一节中 Conv_1 的输出是 [6,10] 还是 [5,10]）。&lt;/p&gt;
&lt;p&gt;逆向深度优先搜索每个分支的意思如下图所示：从输出端搜索到输入端的每一条通路。下图结构中有四条通路。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736946743244.jpg&#34; alt=&#34;深度优先搜索&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
</content>
        <category term="compile" />
        <category term="AI" />
        <updated>2025-01-15T13:12:53.246Z</updated>
    </entry>
    <entry>
        <id>https://forcheetah.github.io/2025/01/14/aicompile02/</id>
        <title>【AI编译】如何进行layer-group</title>
        <link rel="alternate" href="https://forcheetah.github.io/2025/01/14/aicompile02/"/>
        <content type="html">&lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;本篇介绍 AI 编译领域 layer-group 算法。&lt;/p&gt;
&lt;p&gt;本篇文章参考过 &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1787593090401250411&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;《超强干货！地平线编译器大牛的编译优化实践总结》&lt;/a&gt;，&lt;a href=&#34;https://github.com/Arm-China/Compass_Optimizer&#34;&gt;《Arm 周易编译器工程》&lt;/a&gt;，&lt;a href=&#34;https://tpumlir.org/docs/developer_manual/10_layergroup.html#group&#34;&gt;《算能 TPU layer group 讲解》&lt;/a&gt;，&lt;a href=&#34;https://www.bilibili.com/video/BV1wo4y1z7AG/&#34;&gt;《算能 TPU 视频讲解》&lt;/a&gt; 等文章和工程，欢迎大家参考。&lt;/p&gt;
&lt;p&gt;作为初学者，错误在所难免，还望不吝赐教。&lt;/p&gt;
&lt;h1 id=&#34;layer-group&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#layer-group&#34;&gt;#&lt;/a&gt; Layer group&lt;/h1&gt;
&lt;p&gt;如图所示，AI 编译优化的基本流程是 1. 图优化 (算子融合，常量折叠等) 2. 拆分 (layer group 和 tiling) 3. 并行和调度。最后得到当前编译的时间消耗。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736856461365.jpg&#34; alt=&#34;如图1所示&#34;&gt;&lt;/p&gt;
&lt;p&gt;在 AI 编译领域，LayerGroup 指的是将神经网络中的多个层（layers (Operator) ）组合成一个逻辑单元或模块的过程。&lt;/p&gt;
&lt;p&gt;一般而言设备的运行内存很大，比如电脑的运行内存 16GB，但是它的速度比较慢，我们把它叫做 Global Memory。而做神经网络推理的专用 NPU 芯片，它的高速缓存速度很快，但是空间可能只有几 MB，我们把它叫做 Cache。我们无法将网络模型所有 layer 全部加载到 cache 中，那么意味着每个算子都需要 Cache 和外部 Global Memory 的交互，load 输入数据、store 结果。&lt;/p&gt;
&lt;p&gt;所以需要将网络拆分成小的 layer group。一般默认只有进入和退出 layer group 的时候，才需要和外部的 Global Memory 做 Load/Store 操作去交互。把需要的数据 load 进来，将结果数据 store 出去。layer group 减少了 Load/Store 操作，同时 layer group 也是后续 tiling、调度等操作的基本单元，降低了问题复杂度。&lt;/p&gt;
&lt;h1 id=&#34;动态规划搜索&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#动态规划搜索&#34;&gt;#&lt;/a&gt; 动态规划搜索&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736856461365.jpg&#34; alt=&#34;如图1所示&#34;&gt;&lt;/p&gt;
&lt;p&gt;还是这张图，如何划分 layer group 呢？图里面只有四个算子，可以 1,2 划分一组，3,4 划分一组；也可以 1 划为一组，2,3,4 划分另一组；甚至 1,2,3,4 全部划分为一组。不同的划分方式，最后得到的 Cost 也不同。&lt;br&gt;
为了找到最优的划分方式，不得不搜索所有划分方案。&lt;/p&gt;
&lt;p&gt;假设网络有 n 层，则 n 层中间有 n-1 个间隔。选取 0 个间隔，也就是 n 层全部划分为一组，是 C (n-1, 0)；选取 1 个间隔，也就是分成两组，是 C (n-1,1)；选取 2 个间隔，也就是划分三组，是 C (n-1,2)；以此类推， C (n-1, 0)+C (n-1,1)+C (n-1,2)+……+C (n-1,n-1)，根据二项式定理，需要 2^(n-1) 次搜索。&lt;/p&gt;
&lt;p&gt;指数式增长计算量太大，是不可接受的。&lt;/p&gt;
&lt;p&gt;可以采用动态规划的思想来减少搜索次数。&lt;/p&gt;
&lt;p&gt;我们以 100 层网络为例。设 &lt;code&gt;f(n)&lt;/code&gt;  为从第 0 层到达第 n 层最短用时。令  &lt;code&gt;f(-1) = 0&lt;/code&gt;  ， &lt;code&gt;cost(x,y)&lt;/code&gt;  为从 x 层到 y 层作为一个 group 的开销。则：&lt;/p&gt;
&lt;figure class=&#34;highlight cpp&#34;&gt;&lt;figcaption data-lang=&#34;C++&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;99&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;99&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;99&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;98&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cost&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;99&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;99&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这就需要提前计算一个 cost table。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736856527679.jpg&#34; alt=&#34;如图2所示&#34;&gt;&lt;/p&gt;
&lt;p&gt;cost table 如图所示。我们要做的就是以从 x 层到 y 层作为一个 group，计算开销 &lt;code&gt;cost(x,y)&lt;/code&gt;  并保存下来。对于一个 n 层的网络来说，需要搜索的 layer group 数量为： 1+2+…+n = n (n+1)/2 。搜索次数从原来的 2^(n-1) 指数函数，已经降到幂函数。&lt;/p&gt;
&lt;p&gt;但是当网络层数过多时，搜索量还是很大，可以限制最大搜索长度，例如最长支持 50 层，那么 cost table 就变成下面这个样子：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1736856542354.jpg&#34; alt=&#34;如图3所示&#34;&gt;&lt;/p&gt;
&lt;p&gt;现在对于 n 层网络，搜索数量降到 50*n 。&lt;/p&gt;
&lt;p&gt;还可以启发式的剪枝优化，优化掉明显不会有收益的分支，进一步降低搜索的时间消耗。&lt;/p&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！global_info)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;runtime_mod&lt;/code&gt;&lt;/p&gt;
</content>
        <category term="compile" />
        <category term="AI" />
        <updated>2025-01-14T12:09:18.353Z</updated>
    </entry>
    <entry>
        <id>https://forcheetah.github.io/2024/10/18/aicompile01/</id>
        <title>【AI编译】Tiling操作能优化什么时间</title>
        <link rel="alternate" href="https://forcheetah.github.io/2024/10/18/aicompile01/"/>
        <content type="html">&lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;本篇讲解 Tiling 操作为什么能够优化神经网络推理。&lt;/p&gt;
&lt;p&gt;也可以参考 &lt;a href=&#34;https://www.hiascend.com/developer/techArticles/20240920-1?envFlag=1&#34;&gt;《Ascend C 算子优化实用技巧 04——Tiling 优化》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作为初学者，错误在所难免，还望不吝赐教。&lt;/p&gt;
&lt;h1 id=&#34;什么是tiling&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#什么是tiling&#34;&gt;#&lt;/a&gt; 什么是 tiling&lt;/h1&gt;
&lt;p&gt;无法完整的容纳算子的输入与输出，需要每次搬运一部分输入进行计算然后搬出，再搬运下一部分输入进行计算，直到得到完整的最终结果，这个数据切分、分块计算的过程称之为 Tiling，切分数据的算法称为 Tiling 算法或者 Tiling 策略。&lt;/p&gt;
&lt;h1 id=&#34;tile算子和tiling的区别&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#tile算子和tiling的区别&#34;&gt;#&lt;/a&gt; tile 算子和 tiling 的区别&lt;/h1&gt;
&lt;p&gt;我们先问一问语言大模型两者的区别：&lt;/p&gt;
&lt;h2 id=&#34;神经网络推理中的tile算子&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#神经网络推理中的tile算子&#34;&gt;#&lt;/a&gt; 神经网络推理中的 Tile 算子&lt;/h2&gt;
&lt;p&gt;在神经网络中，会发现 tile 作为一个节点算子出现。Tile 算子（或称为 Tiling 操作）是一种张量操作，它的功能是将输入张量沿着指定的维度重复一定次数。该算子需要指定两个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.reps&lt;br&gt;
（重复次数）：这是一个整数列表，定义了每个维度上的重复次数。列表的长度必须与输入张量的维度相匹配，或者至少与你想要扩展的那些维度相匹配。如果对于某个维度你不希望进行复制，可以设置为 1。&lt;/li&gt;
&lt;li&gt;2.axis&lt;br&gt;
（轴 / 维度）：虽然某些框架可能不需要显式指定轴，因为它们可以通过 reps 的结构来推断，但有些情况下需要明确指出哪些维度应该被复制。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如，假设有一个形状为 (2, 3) 的二维张量，并且你想沿第一个维度（行方向）重复两次，沿第二个维度（列方向）重复三次，那么你可以使用 Tile 算子并设置 reps=[2, 3]。这样操作后，输出张量将会是一个形状为 (4, 9) 的新张量，其中原始张量的内容被按照指定的方式进行了复制。&lt;/p&gt;
&lt;p&gt;在不同的深度学习库中，Tile 算子的实现可能会有所不同。例如，在 TensorFlow 中，它是 tf.tile () 函数；而在 PyTorch 中，则对应的是 torch.tile () 或者 .repeat () 方法。每种实现都有其特定的语法和用法，但核心概念是一致的。&lt;/p&gt;
&lt;h2 id=&#34;ai编译优化中的tiling操作&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ai编译优化中的tiling操作&#34;&gt;#&lt;/a&gt; AI 编译优化中的 tiling 操作&lt;/h2&gt;
&lt;p&gt;在 AI 编译领域，特别是针对深度学习模型的优化过程中，“tiling”（平铺）操作是指一种将计算任务分解成更小、更易于管理的子任务的技术。这种技术通常用于提高计算效率和内存使用效率，尤其是在处理大规模数据集或高维度张量时。&lt;/p&gt;
&lt;p&gt;Tiling 的主要目的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1. 减少内存访问开销：通过将大块数据划分为较小的 “瓦片”，可以将这些小块数据加载到高速缓存中，从而减少对外部存储器的访问次数。这有助于利用 CPU 或 GPU 的高速缓存来加速计算过程。&lt;/li&gt;
&lt;li&gt;2. 并行化处理：每个 “瓦片” 可以独立处理，这意味着它们可以在多核处理器上并行执行，或者在 GPU 等并行计算架构上高效地分发给不同的线程或流处理器。&lt;/li&gt;
&lt;li&gt;3. 更好地利用硬件资源：通过适当调整瓦片大小，可以确保计算单元能够被充分利用，同时避免因单个任务过大导致的资源浪费。&lt;/li&gt;
&lt;li&gt;4. 改善局部性：合理设置的瓦片尺寸可以帮助保持良好的空间局部性和时间局部性，使得数据在被处理前已经被预取到了更快的存储层次中。&lt;/li&gt;
&lt;li&gt;5. 降低峰值内存需求：对于某些运算来说，一次性加载整个输入可能需要大量的内存。通过 tilling，我们可以仅处理当前所需的那部分数据，从而降低了对系统内存的需求。&lt;br&gt;
例如，在卷积神经网络 (CNN) 中，一个常见的做法是将图像分割成多个小块，并且只在这些小块上进行卷积运算，而不是在整个图像上进行。这样做不仅可以减少每次计算所需的数据量，还能促进更好的并行化。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tiling 策略的选择取决于具体的硬件特性以及所处理数据的特点。在实际应用中，通常需要根据目标平台和具体应用场景来精心设计合适的 tilling 方案。一些自动化的工具和框架如 TensorFlow XLA、TVM (Tensor Virtual Machine) 等提供了高级别的 API 来帮助开发者实现高效的 tilling 优化。&lt;/p&gt;
&lt;h2 id=&#34;两个tile是完全不同的概念&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#两个tile是完全不同的概念&#34;&gt;#&lt;/a&gt; 两个 tile 是完全不同的概念&lt;/h2&gt;
&lt;p&gt;两个 “tile” 概念虽然名字相似，但它们在不同的上下文中使用，并且具有不同的含义和用途：&lt;/p&gt;
&lt;p&gt;1. 神经网络中的 Tile 算子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在这个上下文中，Tile 是指一种张量操作，它将输入张量沿着指定维度重复一定次数。&lt;/li&gt;
&lt;li&gt;它主要用于数据处理阶段，例如当你需要复制或扩展张量以匹配某些特定的尺寸要求时。&lt;/li&gt;
&lt;li&gt;例子包括 TensorFlow 中的 tf.tile () 函数或 PyTorch 中的.repeat () 方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.AI 编译领域的 Tiling（平铺）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这里的 Tiling 是一种优化技术，用于将计算任务分解成更小的、可管理的部分，以便于更好地利用硬件资源，如缓存和并行计算能力。&lt;/li&gt;
&lt;li&gt;它通常是在模型优化阶段使用的，目的是提高计算效率、减少内存访问开销、促进并行化处理等。&lt;/li&gt;
&lt;li&gt;Tiling 可以应用于各种类型的运算，比如卷积运算中将图像分割成小块来处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结来说，尽管两者都涉及到了 “复制” 或 “分块” 的概念，但是神经网络中的 Tile 算子更多地关注于数据结构的操作，而 AI 编译领域的 Tiling 则是一个优化策略，旨在提升程序执行的性能。这两个概念分别属于数据处理和性能优化的不同领域。&lt;/p&gt;
&lt;h1 id=&#34;案例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#案例&#34;&gt;#&lt;/a&gt; 案例&lt;/h1&gt;
&lt;p&gt;一台电脑的内存很大，现在主流配置 16GB，甚至 32GB，虽然空间很大，但是它的速度比较慢，我们把它叫做 Global Memory。&lt;/p&gt;
&lt;p&gt;专用的 NPU 芯片用来做神经网络推理，它的高速缓存速度很快，但是空间可能只有几 MB，我们把它叫做 Cache。&lt;/p&gt;
&lt;p&gt;为了获取更快的运算速度，tensor 需要加载到 Cache 中进行计算，但是当算子需要占用的空间超过 Cache 的空间时，需要不断的进行数据搬运，导致算子搬入或搬出数据变为算子整个运行过程的性能瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1729258859529.jpg&#34; alt=&#34;AI&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示：&lt;/p&gt;
&lt;p&gt;假设现在有 300 个数，需要连续经过三个 add 算子进行加操作，分别是 &lt;code&gt;add_1&lt;/code&gt; ， &lt;code&gt;add_2&lt;/code&gt;  和 &lt;code&gt;add_3&lt;/code&gt; ，最终仍然输出 300 个数。&lt;/p&gt;
&lt;p&gt;但是 Cache 只能够存放 100 个数。&lt;/p&gt;
&lt;p&gt;在没有 tiling 操作的情况下：计算 &lt;code&gt;add_1&lt;/code&gt;  时，需要将 (0,100) 个数 load 到 Cache，计算完毕后，需要将这 (0,100) store 回 global memory，为下一百个数腾出空间【接下来的计算 Cache 未命中】；然后加载 load (100,200) 的数据，继续计算 &lt;code&gt;add_1&lt;/code&gt; 。以此类推，在没有 tiling 操作的情况下，计算完 &lt;code&gt;add_1&lt;/code&gt; ， &lt;code&gt;add_2&lt;/code&gt;  和 &lt;code&gt;add_3&lt;/code&gt;  需要 load 和 store 操作的数据都是 900。&lt;/p&gt;
&lt;p&gt;在 tiling 的情况下，会提前将数据分块，分成 (0,100)，(100,200) 和 (200,300)。加载 (0,100)，接连计算 &lt;code&gt;add_1&lt;/code&gt; ， &lt;code&gt;add_2&lt;/code&gt;  和 &lt;code&gt;add_3&lt;/code&gt; 。计算 &lt;code&gt;add_2&lt;/code&gt;  时发现 Cache 中的数据正是所需要的数据【Cache 命中】。计算流程如图所示，整个计算下来，load 和 store 操作的数据都是 300。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;a.jpg&#34; alt=&#34;图片&#34;&gt;&lt;/p&gt;
&lt;p&gt;tiling 操作提高了 cache 的命中率，避免了频繁搬运带来的时间损耗。&lt;br&gt;
从图上看，同一 group 中包含的超出 cache 算子越多，tiling 带来的收益越大。&lt;/p&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
</content>
        <category term="compile" />
        <category term="AI" />
        <updated>2024-10-18T13:44:56.351Z</updated>
    </entry>
</feed>

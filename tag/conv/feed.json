{
    "version": "https://jsonfeed.org/version/1",
    "title": "Пусть этот камень будет более крепким, чем человек • All posts by \"conv\" tag",
    "description": "有自己的博客很帅，但是我很懒，要命！！！",
    "home_page_url": "https://forcheetah.github.io",
    "items": [
        {
            "id": "https://forcheetah.github.io/2024/05/24/tvm1/",
            "url": "https://forcheetah.github.io/2024/05/24/tvm1/",
            "title": "【TVM】根据例子走通代码库",
            "date_published": "2024-05-24T14:49:36.319Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>最近开始学习 TVM。感觉 TVM 英文文档中 <a href=\"https://tvm.apache.org/docs/dev/tutorial/codebase_walkthrough.html\">TVM Codebase Walkthrough by Example</a>    一节对于理解 TVM 工程非常有用。本篇文章只是翻译，可以直接跳转查看英文全文。</p>\n<blockquote>\n<p>这个时代有这么多愿意开源并将技术介绍给我们的行业大牛，真是我们的荣幸，膜拜！<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"codebase-structure-overview\"><a class=\"anchor\" href=\"#codebase-structure-overview\">#</a> Codebase Structure Overview</h1>\n<p>在 TVM 存储库的根目录中，我们有以下子目录，它们共同构成了大部分代码库。</p>\n<ul>\n<li><strong>src</strong><br>\nC++ code for operator compilation and deployment runtimes.<br>\n 算子编译 、 runtime 部署 的 C++ 代码</li>\n<li><strong>src/relay</strong><br>\nImplementation of Relay, a new functional IR for deep learning framework.<br>\nRelay IR 的实现      算子的映射关系在 src/relay/op</li>\n<li><strong>python</strong><br>\nPython frontend that wraps C++ functions and objects implemented in src.<br>\npython 前端</li>\n<li><strong>src/topi</strong><br>\nCompute definitions and backend schedules for standard neural network operators.<br>\n 计算标准神经网络算子的定义和后端调度</li>\n</ul>\n<p>TVM 中 Python 和 C++ 的互操作性不是单向的。尽管在 TVM 中 C++ 完成繁重的内部执行工作，Python 完成用户接口， TVM 中也存在 C++ 调用 Python 的情况：For example, the convolution operator is implemented in Python, and its implementation is invoked from C++ code in Relay.（Relay 中的 C++ 调用 Python 实现的卷积算子）</p>\n<h1 id=\"vector-add-example\"><a class=\"anchor\" href=\"#vector-add-example\">#</a> Vector Add Example</h1>\n<p>使用 vector add 的例子来查看底层 TVM API.</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>n <span class=\"token operator\">=</span> <span class=\"token number\">1024</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>A <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">'A'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>B <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">'B'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>C <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>compute<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> <span class=\"token keyword\">lambda</span> i<span class=\"token punctuation\">:</span> A<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> B<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"C\"</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这里面 A、B、C 都是  <code>tvm.tensor.Tensor</code>   其 Python 定义位于 <code>python/tvm/te/tensor.py</code> . 支撑的 C++ 定义位于 <code>include/tvm/te/tensor.h</code>  和 <code>src/te/tensor.cc</code>  所有的 Python 类型定义都能找到对应的相同名字的 C++ 定义。</p>\n<p>Python 对 C++ 的包装位于  <code>python/tvm/_ffi/</code> 。</p>\n<p>一个 Tensor 包含一个 Operation 类，定义于 python/tvm/te/tensor.py，对应的 C++ 实现位于 <code>include/tvm/te/operation.h</code>  和 <code>src/tvm/te/operation</code>  。 <code>Tensor</code>  是  <code>Operation</code>  类的输出。</p>\n<p>我们将输出张量 C 对应的操作传递给 <code>tvm.te.create_schedule()</code>  函数 （来自于 <code>python/tvm/te/schedule.py</code> 。）</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>s <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>create_schedule<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">.</span>op<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这个函数映射到 C++ 函数 <code>include/tvm/schedule.h</code> 。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>inline Schedule create_schedule<span class=\"token punctuation\">(</span>Array<span class=\"token operator\">&lt;</span>Operation<span class=\"token operator\">></span> ops<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>  <span class=\"token keyword\">return</span> Schedule<span class=\"token punctuation\">(</span>ops<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><code>Schedule</code>  包含 <code>Stage</code>  输出  <code>Operation</code>  的集合。</p>\n<p><code>Stage</code>  对应于一个操作 <code>Operation</code> 。上面的 vector add 操作中有两个 placeholder ops 和一个 compute op. 所以 <code>Schedule s</code>  有三个状态  <code>Stage</code> ，每个 <code>Stage</code>  持有以下信息： 循环嵌套结构、每个循环的类型（ <code>Parallel，Vectorized，Unrolled</code> ）、以及在下一个循环嵌套 <code>Stage</code>  中在哪里执行它自己的计算。</p>\n<p><code>Schedule</code>  和 <code>Stage</code>  本身定义在 <code>tvm/python/te/schedule.py</code> ，  <code>include/tvm/te/schedule.h</code> ， 和 <code>src/te/schedule/schedule_ops.cc</code> 。</p>\n<p>为简单起见，我们使用 <code>tvm.build(...)</code>  处理上方 <code>create_schedule()</code>  函数创建的默认 <code>Schedule s</code>  和 &lt;em&gt;。我们必须添加必要的线程绑定，来使得其能在 GPU 上运行：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>target <span class=\"token operator\">=</span> <span class=\"token string\">\"cuda\"</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>bx<span class=\"token punctuation\">,</span> tx <span class=\"token operator\">=</span> s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">.</span>op<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> factor<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span>bx<span class=\"token punctuation\">,</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>thread_axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"blockIdx.x\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span>tx<span class=\"token punctuation\">,</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>thread_axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"threadIdx.x\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>fadd <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>build<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>A<span class=\"token punctuation\">,</span> B<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> target<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p><code>tvm.build(...)</code> ，定义在 <code>python/tvm/driver/build_module.py</code> ， 需要输入一个 <code>Schedule</code> ;  <code>input</code> , <code>output Tensor</code> ; 以及一个 <code>target</code> 。返回一个 <code>tvm.runtime.Module</code> 。</p>\n<p>整个 <code>tvm.build(...)</code>  过程可以分成两步：</p>\n<ul>\n<li>\n<p>i. 降级 高级的、初始的循环嵌套结构被转换为 最终的、低级的 IR</p>\n</li>\n<li>\n<p>ii. 代码生成 low level IR 生成目标机器码</p>\n</li>\n</ul>\n<p>降级是通过 <code>tvm.lower()</code>  函数完成的，它定义在 <code>python/tvm/build\\_module.py</code> 。第一，指定绑定推理，一个最初的循环嵌套结构就创建好了。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">def</span> <span class=\"token function\">lower</span><span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>          args<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>          name<span class=\"token operator\">=</span><span class=\"token string\">\"default_function\"</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>          binds<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>          simple_mode<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>   <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>   bounds <span class=\"token operator\">=</span> schedule<span class=\"token punctuation\">.</span>InferBound<span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>   stmt <span class=\"token operator\">=</span> schedule<span class=\"token punctuation\">.</span>ScheduleOps<span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">,</span> bounds<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>   <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr></table></figure><p>边界推断是推断所有循环边界和中间缓冲区大小的过程。如果你的目标是 CUDA，且你用了 share memory，它需要的最小 size 在此处确定。绑定推理时在 <code>src/te/schedule/bound.cc，src/te/schedule/graph.cc </code>  和  <code>src/te/schedule/message\\_passing.cc</code>  中实现的。</p>\n<p><code>stmt</code> ， <code>ScheduleOps()</code>  的输出，表示一个初识的循环嵌套结构。如果在 schedule 中已经应用了 <code>reorder</code>  和 <code>split</code>  原语，那么初始的循环嵌套结构已经反映了这些变化。 <code>ScheduleOps()</code>  定义在 <code>rc/te/schedule/schedule_ops.cc</code> 。</p>\n<p>接下来应用一些 lowering passes to  <code>stmt</code>  . 这些 passes 在 <code>src/tir/pass</code>  子文件夹下实现。举个例子，如果在你的 <code>schedule</code>  中应用了 <code>vectorize</code>  或者 <code>unroll</code>  原语，他们会被应用到循环 vectorization 和 unrolling passes。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>stmt <span class=\"token operator\">=</span> ir_pass<span class=\"token punctuation\">.</span>VectorizeLoop<span class=\"token punctuation\">(</span>stmt<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>stmt <span class=\"token operator\">=</span> ir_pass<span class=\"token punctuation\">.</span>UnrollLoop<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    stmt<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_step<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_depth<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_extent<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>unroll_explicit<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr></table></figure><p>在降级 lowering 结束后， <code>build()</code>  函数生成目标机器代码。如果你的设备是 X86, 这个代码可能包含 SSE 或者 AVX 指令；如果是 CUDA 设备，将包含 PTX 指令。 此外，除了目标特定的机器代码之外，TVM 还生成负责内存管理、内核启动等的主机端代码。</p>\n<p><code>build\\_module()</code>  函数完成代码生成，定义在 <code>python/tvm/target/codegen.py</code> 。在 C++ 端代码生成定义在 <code>src/target/codegen</code> 。 <code>build\\_module()</code>  Python 函数会搜索在 <code>src/target/codegen/codegen.cc</code>  中的 <code>build()</code>  函数。</p>\n<p><code>build()</code>  函数 <code>PackedFunc</code>  注册表中为目标设备查找代码生成器，并调用找到的函数。例如， <code>codegen.build\\_cuda</code>  函数注册在 <code>src/codegen/build_cuda_on.cc</code> ，就像这样：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>TVM_REGISTER_GLOBAL<span class=\"token punctuation\">(</span><span class=\"token string\">\"codegen.build_cuda\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token punctuation\">.</span>set_body<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>TVMArgs args<span class=\"token punctuation\">,</span> TVMRetValue<span class=\"token operator\">*</span> rv<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token operator\">*</span>rv <span class=\"token operator\">=</span> BuildCUDA<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  <span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>上方的 <code>BuildCUDA()</code>  函数使用定义在 <code>src/codegen/codegen_cuda.cc</code>  的 <code>CodeGenCUDA</code>  类，从 lowered IR 生成 CUDA kernel source，并使用 NVRTC 编译 kernel。如果你的目标设备使用 LLVM，包括 X86、ARM、NVPTX 和 AMDGPU，代码可由定义在 <code>src/codegen/llvm/codegen_llvm.cc</code>  的 <code>CodeGenLLVM</code>  来生成。 <code>CodeGenLLVM</code>  将 TVM IR 转换成 LLVM IR，运行一些 LLVM 优化 passes，以及生成目标机器码。</p>\n<p>在 <code>src/codegen/codegen.cc</code>  中的 <code>Build()</code>  函数会返回一个 <code>runtime::Module</code>  类，它定义在 <code>include/tvm/runtime/module.h</code>  和 <code>src/runtime/module.cc</code> 。一个 <code>Module</code>  类是一个潜在目标 设备的特定 <code>ModuleNode</code>  的容器。</p>\n<p>每个后端都实现一个 <code>ModuleNode</code>  的子类，来添加目标特定的 runtime API 调用。 例如，CUDA 后端在 <code>src/runtime/cuda/cuda_module.cc</code>  实现 <code>CUDAModuleNode</code>  类，来管理 CUDA 驱动 API。上方的 <code>BuildCUDA()</code>  函数用 <code>runtime::Module</code>  包装了 <code>CUDAModuleNode</code> ，并包装到 Python 端。LLVM 后端在 <code>src/codegen/llvm/llvm_module.cc</code>  实现了 <code>LLVMModuleNode</code> ，处理了 JIT 执行和编译代码。其他对应各个后端的 <code>ModuleNode</code>  子类可以在 <code>src/runtime</code>  子文件夹找到。<br>\n返回的 <code>module</code> ，可以被认作编译函数和设备 API 的组合，可以被 TVM 的 NDArray objects 调用。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>dev <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span>target<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>a <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>uniform<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>b <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>uniform<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>B<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>c <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>C<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>fadd<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>output <span class=\"token operator\">=</span> c<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>在幕后，TVM 会自动分配设备内存并管理内存传输。为了实现这个目标，每个后端都需要继承在 <code>include/tvm/runtime/device_api.h</code>  定义的 <code>DeviceAPI</code>  类，使用设备特定的 API 重写里面的内存管理方法。例如，CUDA 后端在 <code>src/runtime/cuda/cuda_device_api.cc</code>  使用 <code>cudaMalloc</code> 、 <code>cudaMemcpy</code>  实现了 <code>CUDADeviceAPI</code> .</p>\n<p>第一次使用 <code>fadd(a, b, c)</code>  调用编译后的模块时，会调用  <code>ModuleNode</code>  的  <code>GetFunction()</code>  方法来获取可用于内核调用的  <code>PackedFunc</code> 。例如，在 <code>src/runtime/cuda/cuda_module.cc</code>  CUDA 后端实现了 <code>CUDAModuleNode::GetFunction()</code>  函数如下：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>PackedFunc CUDAModuleNode<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>GetFunction<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>      const std<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>string<span class=\"token operator\">&amp;</span> name<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>      const std<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>shared_ptr<span class=\"token operator\">&lt;</span>ModuleNode<span class=\"token operator\">></span><span class=\"token operator\">&amp;</span> sptr_to_self<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  auto it <span class=\"token operator\">=</span> fmap_<span class=\"token punctuation\">.</span>find<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>  const FunctionInfo<span class=\"token operator\">&amp;</span> info <span class=\"token operator\">=</span> it<span class=\"token operator\">-</span><span class=\"token operator\">></span>second<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>  CUDAWrappedFunc f<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>  f<span class=\"token punctuation\">.</span>Init<span class=\"token punctuation\">(</span>this<span class=\"token punctuation\">,</span> sptr_to_self<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>arg_types<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>launch_param_tags<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>  <span class=\"token keyword\">return</span> PackFuncVoidAddr<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>arg_types<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><code>PackedFunc</code>  的重载函数 <code>operator()</code>  会被调用。从而会调用定义在 <code>src/runtime/cuda/cuda_module.cc</code>  的 <code>CUDAWrappedFunc</code>  的 <code>operator()</code>  函数，最终我们会看到 <code>cuLaunchKernel</code>  驱动会调用：</p>\n<pre><code>class CUDAWrappedFunc &#123;\n public:\n  void Init(...)\n  ...\n  void operator()(TVMArgs args,\n                  TVMRetValue* rv,\n                  void** void_args) const &#123;\n    int device_id;\n    CUDA_CALL(cudaGetDevice(&amp;device_id));\n    if (fcache_[device_id] == nullptr) &#123;\n      fcache_[device_id] = m_-&gt;GetFunc(device_id, func_name_);\n    &#125;\n    CUstream strm = static_cast&lt;CUstream&gt;(CUDAThreadEntry::ThreadLocal()-&gt;stream);\n    ThreadWorkLoad wl = launch_param_config_.Extract(args);\n    CUresult result = cuLaunchKernel(\n        fcache_[device_id],\n        wl.grid_dim(0),\n        wl.grid_dim(1),\n        wl.grid_dim(2),\n        wl.block_dim(0),\n        wl.block_dim(1),\n        wl.block_dim(2),\n        0, strm, void_args, 0);\n  &#125;\n&#125;;\n</code></pre>\n<p>本文概括了 TVM 如何编译和执行函数。 虽然本文没有详细说明 TOPI 或 Relay，但最终所有神经网络算子都会经历与上述相同的编译过程。</p>\n",
            "tags": [
                "accelerate",
                "conv",
                "tvm"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/24/category/",
            "url": "https://forcheetah.github.io/2024/05/24/category/",
            "title": "博客汇总目录",
            "date_published": "2024-05-24T13:25:53.973Z",
            "content_html": "<h1 id=\"暮冬z羡慕-的博客-文章汇总\"><a class=\"anchor\" href=\"#暮冬z羡慕-的博客-文章汇总\">#</a> 暮冬 Z 羡慕 的博客  文章汇总</h1>\n<h1 id=\"卷积加速算法\"><a class=\"anchor\" href=\"#卷积加速算法\">#</a> 卷积加速算法</h1>\n<ul>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/23/conv1/\">【Im2Col】卷积加速算法【1】 NCHW</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/23/conv2/\">【Im2Col】卷积加速算法【2】NHWC</a></p>\n</li>\n<li></li>\n</ul>\n<h1 id=\"ai推理引擎\"><a class=\"anchor\" href=\"#ai推理引擎\">#</a> AI 推理引擎</h1>\n<h1 id=\"ai编译优化\"><a class=\"anchor\" href=\"#ai编译优化\">#</a> AI 编译优化</h1>\n<ul>\n<li><a href=\"https://forcheetah.github.io/2024/05/24/tvm1/\">【TVM】根据例子走通代码库</a></li>\n</ul>\n<h1 id=\"问题解决\"><a class=\"anchor\" href=\"#问题解决\">#</a> 问题解决</h1>\n<ul>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/15/openBlas/\">openBlas 库的安装与简单使用</a></p>\n</li>\n<li>\n<p><a href=\"https://forcheetah.github.io/2024/05/14/cpplib/\">C 语言工程调用 Cpp 库解决方案</a></p>\n</li>\n</ul>\n<h1 id=\"杂谈\"><a class=\"anchor\" href=\"#杂谈\">#</a> 杂谈</h1>\n<blockquote>\n<p>持续更新中 ...</p>\n</blockquote>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/23/conv2/",
            "url": "https://forcheetah.github.io/2024/05/23/conv2/",
            "title": "【Im2Col】卷积加速算法【2】NHWC",
            "date_published": "2024-05-23T12:12:27.275Z",
            "content_html": "<p>本文为最基本的 Im2Col 算法的原理及实现。</p>\n<p><a href=\"https://forcheetah.github.io/2024/05/23/conv1/\">【Im2Col】卷积加速算法 NHWC</a> 【1】中已经讲了在输入和输出都是 nchw 排布下 Im2Col 算法的实现方式。常见的 tensor 输入有 NCHW 和 NHWC 两种内存排布方式，不同的排布方式各有优劣。排布方式不同，Im2Col 也有区别，本篇主要是在 NHWC 内存排布情况下的 Im2Col 算法原理和基本实现。</p>\n<blockquote>\n<p>慌乱的时候全是破绽，冷静下来，能够找到对方的破绽。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"两种内存排布\"><a class=\"anchor\" href=\"#两种内存排布\">#</a> 两种内存排布</h1>\n<p>卷积神经网络（CNN）的输入数据布局主要有两种标准：NCHW（通道、高度、宽度）和 NHWC（高度、宽度、通道）。主要深度学习框架对这两种布局的支持情况如下：</p>\n<p>PyTorch：主要采用 NCHW 格式。这是 PyTorch 在大多数情况下的默认布局，尤其是在涉及 GPU 计算时。</p>\n<p>Caffe：采用 NCHW 格式。Caffe 框架倾向于使用这种通道优先的布局。</p>\n<p>TensorFlow：默认使用 NHWC 格式，特别是在早期版本中，这是由于 TensorFlow 最初设计时主要针对 CPU 进行优化，NHWC 布局在这种场景下有更好的内存访问局部性。<br>\nKeras：Keras 本身是一个高级 API，集成到 Tensorflow 之后跟随 tensorflow 的内存排布方式。</p>\n<h1 id=\"内存排布的优缺点\"><a class=\"anchor\" href=\"#内存排布的优缺点\">#</a> 内存排布的优缺点</h1>\n<p><strong>NCHW 格式的优点：</strong></p>\n<ul>\n<li>\n<p>1. 在 GPU 中计算卷积时，比 NHWC 要快 2.5 倍左右。这是因为在 GPU 中，NCHW 格式的数据布局更符合 GPU 的内存访问模式和计算方式。</p>\n</li>\n<li>\n<p>2.NCHW 格式更适合那些需要对每个通道单独做运算的操作，比如 “MaxPooling”。这是因为 NCHW 格式的同一通道的像素值连续排布，使得对每个通道的数据可以更高效地进行运算。</p>\n</li>\n</ul>\n<p><strong>NCHW 格式的缺点：</strong></p>\n<ul>\n<li>\n<p>1.NCHW 格式需要把所有通道的数据都读取到，才能进行运算，因此在计算时需要的存储更多。这可能会限制其在一些具有限制的硬件环境下的应用。</p>\n</li>\n<li>\n<p>2.NCHW 格式的访存与计算的控制逻辑相对简单，这使得在一些需要精细控制访存和计算的场景下，可能不是最佳的选择。</p>\n</li>\n</ul>\n<p><strong>NHWC 格式的优点：</strong></p>\n<ul>\n<li>\n<p>1.NHWC 格式的访存局部性更好。这意味着每三个输入像素就可以得到一个输出像素，因此在一些特定的计算操作中，可以更高效地利用硬件资源。</p>\n</li>\n<li>\n<p>2.NHWC 格式更适合那些需要对不同通道的同一像素做某种运算的操作，比如 “Conv1x1”。这是因为 NHWC 格式的不同通道中的同一位置元素顺序存储，使得对不同通道的数据可以进行更高效的运算。</p>\n</li>\n<li>\n<p>3.NHWC 格式在早期的 CPU 开发中应用较多，因此对于主要基于 CPU 开发的深度学习框架和算法，NHWC 格式可能更受欢迎。</p>\n</li>\n</ul>\n<p><strong>NHWC 格式的缺点：</strong></p>\n<ul>\n<li>\n<p>1. 在使用 GPU 进行计算加速时，NHWC 格式不如 NCHW 格式高效。这是因为 NCHW 格式更符合 GPU 的内存访问模式和计算方式。</p>\n</li>\n<li>\n<p>2. 对于一些需要精细控制访存和计算的场景，NHWC 格式的控制逻辑可能相对复杂一些。</p>\n</li>\n</ul>\n<h1 id=\"im2col变换\"><a class=\"anchor\" href=\"#im2col变换\">#</a> Im2Col 变换</h1>\n<p>Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 input_2D 放在前面，也就是 (input_2D * kernel_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。</p>\n<p>依然采用下面这个简单的卷积样例，输入 tensor 按照 nhwc 排布，所以是一个 3 通道 5*5 的 input tensor。卷积核有 9 个，pad 为 1，因此输出是【1，5，5，9】。</p>\n<p><img loading=\"lazy\" data-src=\"1716464754440.jpg\" alt=\"卷积样例\"></p>\n<p>直接展开 2D 形式，两个矩阵形式如下。</p>\n<p><img loading=\"lazy\" data-src=\"1716464821580.jpg\" alt=\"Im2Col 2D示意图\"></p>\n<p><strong>上图右边是权重 kernel_2D</strong></p>\n<ul>\n<li>\n<p>1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序展成一列，也就是第一列绿色部分，共有 27 个数。</p>\n</li>\n<li>\n<p>2. 依次将余下 8 个 kernel 按照相同的方式展成一列，就得到了 kernel_2D。</p>\n</li>\n<li>\n<p>3.kernel_2D 维度为【27，9】</p>\n</li>\n</ul>\n<p>从内存排布上来看，需要将原本的 kernel_4D 进行数据重排。</p>\n<p><strong>上图左边是输入 input_2D</strong></p>\n<ul>\n<li>\n<p>1. 矩阵乘是 行 * 列；kernel_2D 一列代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一行是 “一个卷积核滑动窗口” 对应的数据，也就是 27 个。</p>\n</li>\n<li>\n<p>2. 第一行橘黄色部分是第一个滑动窗口对应的数据，未填的数代表 Pad。</p>\n</li>\n<li>\n<p>3. 滑动窗口需要纵移 5 次，每次纵移需要横移 5 次，因此有 5*5 行数据。</p>\n</li>\n<li>\n<p>4. 例如第 5 行蓝色是滑动窗口在图一中向右移动到第 5 格，蓝色格子时对应的数据。由于 Input_2D 的维度是 NHWC，也就是说图一中 “0，25，50” 三个数在内存中是相邻的（分别位于同一个 HW 位置的第一、第二、第三通道）。为了减少数据的搬运，这些连续的数被搬运到一起。即：图一中 3*3 的蓝色滑动窗口中前 3 个都对应 Pad，所以图二第 5 行蓝色行先有 3*3 (channel) 个 Pad 数据。图一蓝色滑动窗口在第 4 和第 5 个格子出现数据，所以图二 9 个 Pad 后面跟着 3，28，53；4，29，54。紧接着又出现了 Pad，以此类推。</p>\n</li>\n<li>\n<p>5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一行 27 个数据。input_2D 的维度为【25，27】</p>\n</li>\n<li>\n<p>6. 总结完 input_2D 的数据排布，那么 kernel_2D 每一列的数据具体怎么排也就清楚了，需要和 input_2D 的每一行一一对应。</p>\n</li>\n</ul>\n<p><strong>输出 output</strong></p>\n<p>input_2D【25，27】* kernel_2D 【27，9】得到结果 output_2D 【25，9】，刚好是输出 output【1，5，5，9】的内存排布方式。因此输出也不需要额外的内存转换。</p>\n<p><strong>具体实现</strong></p>\n<p>这里没有专门写 Im2Col 在 NHWC 排布情况下的代码，可以在卷积加速算法模拟下载完整的测试代码。并参考 “TestIm2FlavorConvLayer ();” 函数及文章<a href=\"https://forcheetah.github.io/2024/05/15/accelerate1/\"> im2col 卷积加速算法 NHWC</a>，完成 NHWC 内存排布情况下的算法。</p>\n<h1 id=\"矩阵乘先后顺序的影响\"><a class=\"anchor\" href=\"#矩阵乘先后顺序的影响\">#</a> 矩阵乘先后顺序的影响</h1>\n<p>以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。</p>\n<p><img loading=\"lazy\" data-src=\"1716466223424.jpg\" alt=\"两种排布情况下矩阵乘先后顺序不同对数据变换的影响\"></p>\n<p>已经讲解了简单的 im2col 算法在 NCHW 排布<a href=\"https://forcheetah.github.io/2024/05/15/accelerate1/\">上一篇文章</a>和 NHWC 排布情况下的 2D 内存排布情况。上图总结了两种排布情况下 矩阵乘先后顺序不同对数据变换的影响。</p>\n<ul>\n<li>NCHW 排布  kernel 在前  只需要对 input 做 im2col 变换</li>\n<li>NCHW 排布  input 在前  多出来对权重的转置变换 和输出的转置变换</li>\n<li>NHWC 排布  kernel 在前  需要对权重数据进行重排 以及输出进行转置</li>\n<li>NHWC 排布  input 在前  需要对权重进行转置</li>\n</ul>\n<p>当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。</p>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/23/conv1/",
            "url": "https://forcheetah.github.io/2024/05/23/conv1/",
            "title": "【Im2Col】卷积加速算法【1】 NCHW",
            "date_published": "2024-05-23T11:31:49.363Z",
            "content_html": "<p>本文为最基本的 Im2Col 算法的原理及实现。<br>\n加速算法道阻且长，想要选择最优的算法，需要通盘考虑现实需求、软件算法、硬件支持，这就是 “坚持理论联系实际”。 所以这里只是对 Im2Col 算法最基本的原理探讨。<br>\n探索本就是由正确和错误交织而成，还望各位不吝赐教！</p>\n<blockquote>\n<p>现在想起来，光是遇到你这个家伙，就感觉自己赚到了。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"卷积算法\"><a class=\"anchor\" href=\"#卷积算法\">#</a> 卷积算法</h1>\n<p>将 Im2col 算法之前，不得不再提一下卷积。卷积是一种运算，在神经网络中是提取特征的过程，具体的操作过程是在输入特征中不断滑动卷积核大小的窗口，与卷积核做乘加运算，得到输出结果。</p>\n<p>先定义一下维度的符号： 卷积核 ：【C_out, C_in, Hk, Wk】   输入 ： 【B, C_in, H, W】  输出：【B, C_out, Ho, Wo】</p>\n<p><img loading=\"lazy\" data-src=\"1715770544973.jpg\" alt=\"卷积运算流程\"></p>\n<p>上图是维度非常小的一个卷积运算的图示，左边卷积核维度为【9，3，3，3】，即个数为 9，通道数为 3（个数和通道数均未在图中展示出来）的 3*3 卷积核。中间为 Input 的维度，周围方格蓝色代表 tensor 的 pad，中间黄色代表维度为【1，3，5，5】的 Input，即通道为 3，长宽 5*5。</p>\n<p>进行卷积过程中，卷积核（3*3，通道为 3）先横向滑动（5 次），再纵向滑动（5 次）；每到一个位置计算 kernel 与 Input 对应位置的乘积和，因此得到 output 5*5 的结果。9 个卷积核依次进行，得到输出大小【1，9，5，5】</p>\n<p>想要理解卷积乃至实现卷积加速算法，不仅要知道计算过程，还要格外关注数据在内存中的排布顺序。在内存中所有的数据都是一维存储的，例如 Input【1，3，5，5】，在内存中只有一个连续的、大小为 75 的数组，【1，3，5，5】只是它的逻辑维度；最后一个维度的 5 个数字是连续的（0~4），紧接着是下一行（5~9）......</p>\n<p>图中仅画出了平面的大小，通道方向就要靠大家的想象了，上图中每个格子后面还有 2 个格子（通道数为 3）。<br>\n简单的卷积更容易看清 Im2col 算法的流程，本文以及后续系列文章都将以这个例子进行。</p>\n<h1 id=\"im2col算法\"><a class=\"anchor\" href=\"#im2col算法\">#</a> Im2Col 算法</h1>\n<p>为什么要对卷积算法进行加速呢？</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> batch <span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> batch<span class=\"token operator\">&lt;</span>in_n<span class=\"token punctuation\">;</span> batch<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>        <span class=\"token keyword\">while</span> <span class=\"token punctuation\">(</span>s <span class=\"token operator\">&lt;</span> out_c<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> out_row <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> out_row <span class=\"token operator\">&lt;</span> out_h<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>out_row<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>                <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> out_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> out_col <span class=\"token operator\">&lt;</span> out_w<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>out_col<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>                    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> imap <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> imap <span class=\"token operator\">&lt;</span> in_c<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>imap<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kr <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kr <span class=\"token operator\">&lt;</span> ker_size<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>kr<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>                            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kc <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kc <span class=\"token operator\">&lt;</span> ker_size<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>kc<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>                                dosomething<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>                <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>上方是个典型的卷积算法，要实现一个卷积，不仅要遍历 batch、C_out、Ho、Wo，还要遍历整个卷积核的维度，整个循环达到了 7 层之多。当输入数据维度变大时，整个卷积占用的资源让人难以接受。提高神经网络的推理速度是推理引擎和 AI 芯片设计者不断的追求，而卷积又占了神经网络推理的大部分时间，因此卷积的加速是重中之重。<br>\nIm2Col 算法的原理就是将卷积运算<br>\n转化为矩阵运算，这一转换带来以下好处：</p>\n<ul>\n<li>\n<p>易于优化：由于 GEMM 操作在计算库中被广泛研究和优化，开发者可以利用这些库的最新进展提高效率。</p>\n</li>\n<li>\n<p>并行化和硬件支持：矩阵乘法天然适合并行处理；大多硬件平台都会充分利用其硬件资源，对矩阵乘法进行深度优化。通过使用如 CUDA、cuDNN、OpenBLAS 等库，可以实现并行计算，极大地加速卷积运算。</p>\n</li>\n<li>\n<p>内存管理灵活性：虽然 im2col 需要额外的内存来存储展开后的矩阵，但通过调整实现策略（如分块处理），可以在内存使用和计算效率之间找到平衡。</p>\n</li>\n</ul>\n<p>总之，尽管 Im2col 算法没有减少任何计算量，甚至还给内存管理带来挑战，但是矩阵的高效运算、硬件、并行等仍然能够在很多场景下提高卷积计算效率。</p>\n<h1 id=\"im2col变换\"><a class=\"anchor\" href=\"#im2col变换\">#</a> Im2Col 变换</h1>\n<p>Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 kernel_2D 放在前面，也就是 (kernel_2D * input_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。</p>\n<p>再看一遍下面这个简单的卷积样例，输入 tensor 按照 nchw 排布。</p>\n<p><img loading=\"lazy\" data-src=\"1715770588007.jpg\" alt=\"卷积样例\"></p>\n<p>直接展开 2D 形式，两个矩阵形式如下。</p>\n<p><img loading=\"lazy\" data-src=\"1715770625347.jpg\" alt=\"Im2Col 2D示意图\"></p>\n<p><strong>图 “Im2Col 2D 示意图” 左边是 kernel_2D</strong></p>\n<ul>\n<li>\n<p>1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序完全展平，也就是第一行绿色部分，共有 28 个数。</p>\n</li>\n<li>\n<p>2. 依次将余下 8 个 kernel 按照相同的方式展平，就得到了 kernel_2D。</p>\n</li>\n<li>\n<p>3.kernel_2D 维度为【9，27】</p>\n</li>\n</ul>\n<p>惊喜地发现，kernel_2D 的内存排布和 kernel_4D 完全一致，不需要任何内存搬运！</p>\n<p><strong>图 “Im2Col 2D 示意图” 右边是 Input_2D</strong></p>\n<ul>\n<li>\n<p>1. 矩阵乘是 行 * 列；kernel_2D 中，一行代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一列是 “一个卷积核滑动窗口” 对应的数据。</p>\n</li>\n<li>\n<p>2. 右图中，前半部分空数据代表 pad,  后面的大面积空白只是懒得填上数字。</p>\n</li>\n<li>\n<p>3. 例如，右图中蓝色的一列数据，代表图 2 中，卷积核滑动到蓝色窗口时对应的 input 数据。即第一个 channel：(pad,pad,pad;3,4,pad;8,9,pad)； 第二个 channel：(pad,pad,pad;28,29,pad;33,34,pad)；第三个 channel：(pad,pad,pad;53,54,pad;58,59,pad)</p>\n</li>\n<li>\n<p>4. 那么 input_2D 一行数据代表什么呢？代表 kernel 在 input 中窗口的横向移动和纵向移动。在本例子中，窗口需要纵向滑动 5 次，每次纵向滑动都要横向滑动 5 次，一共产生 25 次窗口滑动。</p>\n</li>\n<li>\n<p>5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一列 27 个数据。input_2D 的维度为【27，25】</p>\n</li>\n</ul>\n<p><strong>输出 output</strong></p>\n<p>kernel_2D 【9，27】* input_2D【27，25】得到结果 output_2D 【9，25】，刚好是输出 output【1，9，5，5】的内存排布方式。因此输出也不需要额外的内存转换。</p>\n<p><strong>具体实现</strong></p>\n<p>这里是将 input 由 nchw 转为 2D 排布的代码。如果感兴趣，可以在<a href=\"https://github.com/ForCheetah/ConvAccelerate\">卷积加速算法模拟</a> 下载完整的测试代码。并通过 “TestIm2FlavorConvLayer ();” 函数进行测试。同时函数 “TestIm2ColConvIMW” 是 NCHW 排布下 Input_2D 在前，kernel_2D 在后的 Im2Col 算法实现。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">void</span> <span class=\"token function\">Im2Col</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>data_im<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> channels<span class=\"token punctuation\">,</span><span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> height<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> width<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> kernel_h<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>        <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> kernel_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> pad_h<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> pad_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> stride_h<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> stride_w<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>data_col<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> output_h <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>height <span class=\"token operator\">+</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> pad_h <span class=\"token operator\">-</span> kernel_h <span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> stride_h <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> output_w <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>width <span class=\"token operator\">+</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> pad_w <span class=\"token operator\">-</span> kernel_w<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> stride_w <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> channel_size <span class=\"token operator\">=</span> height <span class=\"token operator\">*</span> width<span class=\"token punctuation\">;</span> </pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> channel <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> channel <span class=\"token operator\">&lt;</span> channels<span class=\"token punctuation\">;</span> channel<span class=\"token operator\">++</span><span class=\"token punctuation\">,</span> data_im <span class=\"token operator\">+=</span> channel_size<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kernel_row <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kernel_row <span class=\"token operator\">&lt;</span> kernel_h<span class=\"token punctuation\">;</span> kernel_row<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>            <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> kernel_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> kernel_col <span class=\"token operator\">&lt;</span> kernel_w<span class=\"token punctuation\">;</span> kernel_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>                <span class=\"token keyword\">int</span> input_row <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>pad_h <span class=\"token operator\">+</span> kernel_row<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>                <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_rows <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_rows<span class=\"token operator\">&lt;</span>output_h<span class=\"token punctuation\">;</span> output_rows<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>                    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">!</span><span class=\"token function\">is_a_ge_zero_and_a_lt_b</span><span class=\"token punctuation\">(</span>input_row<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_cols <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_cols<span class=\"token operator\">&lt;</span>output_w<span class=\"token punctuation\">;</span> output_cols<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>                            <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>                        <span class=\"token keyword\">int</span> input_col <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>pad_w <span class=\"token operator\">+</span> kernel_col<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>                        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> output_col <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> output_col<span class=\"token operator\">&lt;</span>output_w<span class=\"token punctuation\">;</span> output_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>                            <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token function\">is_a_ge_zero_and_a_lt_b</span><span class=\"token punctuation\">(</span>input_col<span class=\"token punctuation\">,</span> width<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>                                <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> data_im<span class=\"token punctuation\">[</span>input_row <span class=\"token operator\">*</span> width <span class=\"token operator\">+</span> input_col<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>                                <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>data_col<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>                            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>                            input_col <span class=\"token operator\">+=</span> stride_w<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>                        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre>                    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>                    input_row <span class=\"token operator\">+=</span> stride_h<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"27\"></td><td><pre>                <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre>            <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"30\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"31\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><h1 id=\"矩阵乘先后顺序的影响\"><a class=\"anchor\" href=\"#矩阵乘先后顺序的影响\">#</a> 矩阵乘先后顺序的影响</h1>\n<p>以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。</p>\n<p><img loading=\"lazy\" data-src=\"1715770640439.jpg\" alt=\"Kernel_2D * Input_2D\"><br>\n 在输入为 nchw 排布，输出也是 nchw 排布情况下，kernel_2D 在前，Input_2D 在后，只需要对 Input 进行 Im2Col 变换。</p>\n<p><img loading=\"lazy\" data-src=\"1715770657999.jpg\" alt=\"Input_2D * Kernel_2D\"><br>\n 在输入为 nchw 排布，输出也是 nchw 排布情况下，Input_2D 在前，kernel_2D 在后，多出了对 kernel 和 output 的转置操作。</p>\n<p>当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。</p>\n",
            "tags": [
                "accelerate",
                "conv"
            ]
        }
    ]
}
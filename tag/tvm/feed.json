{
    "version": "https://jsonfeed.org/version/1",
    "title": "Пусть этот камень будет более крепким, чем человек • All posts by \"tvm\" tag",
    "description": "有自己的博客很帅，但是我很懒，要命！！！",
    "home_page_url": "https://forcheetah.github.io",
    "items": [
        {
            "id": "https://forcheetah.github.io/2024/10/10/tvm01/",
            "url": "https://forcheetah.github.io/2024/10/10/tvm01/",
            "title": "【TVM】通过代码学习编译流程【1】必要知识",
            "date_published": "2024-10-10T12:12:43.910Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本系列文章将从代码和流程图入手，详细介绍 TVM AI 编译器的编译流程。本文章为第一篇，取名为 “必要知识”，约等于 “大杂烩”。食之无味，弃之可惜。但基于我个人的主观喜好和偏见，这些知识值得在整个代码讲解之前声明。</p>\n<p>作为初学者，错误在所难免，还望不吝赐教。</p>\n<h1 id=\"工程结构\"><a class=\"anchor\" href=\"#工程结构\">#</a> 工程结构</h1>\n<p><a href=\"https://github.com/apache/tvm\">TVM</a> 主要由 C++ 语言和 Python 语言实现。</p>\n<ul>\n<li>\n<p>a.TVM C++ 后端核心代码   位于 /home/user/tvm/src<br>\nC++ 后端核心代码的功能是生成 <a href=\"http://libtvm.so\">libtvm.so</a> libtvm_runtime.so，分别是模型编译过程中的需要的 libtvm.so 库，和模型部署运行时需要的 libtvm_runtime.so 库。对应的头文件位于 /home/user/tvm/include.</p>\n<ul>\n<li>1. 其中 src/runtime 是模型部署相关 CPP 文件\n<ul>\n<li>src/runtime/graph_executor   最基本的图执行器</li>\n<li>src/runtime/module.cpp          运行时 Module 基类</li>\n<li>src/runtime/contrib                  BYOC 第三方设备运行及实现  包括 dnnl cblas cudnn ethosu 等等</li>\n<li>srd/runtime/cuda                    CUDA 运行时 Module</li>\n<li>还有其他设备的运行时 Module  如 opencl  hexagon   metal 等，以及一些接口文件</li>\n</ul>\n</li>\n<li>2.src/relay 是 TVM 高级中间表示 IR 的相关 CPP 文件</li>\n<li>3.src/relax TVM 正在发展的高级中间表示 IR，功能上等同于 Relay</li>\n<li>4.src/te      TVM 的 TE 表达式</li>\n<li>5.src/topi   算子的计算定义和后端调度</li>\n<li>6.src/tir      TVM 低级中介表示 IR，接近于硬件表示</li>\n<li>7.src/auto_shedule  TVM 自动优化调度相关</li>\n</ul>\n</li>\n<li>\n<p>b. TVM Python 前端代码      位于 /home/user/tvm/python<br>\nPython 前端代码为 C++ API 和执行编译的驱动代码，提供 Python 绑定。</p>\n</li>\n</ul>\n<p>表面上看 TVM 工程应当是 Python 前端调用 C++ 后端生成的 <a href=\"http://libtvm.so\">libtvm.so</a> libtvm_runtime.so ，实际上 TVM 支持 Python 和 C++ 的相互调用。例如一些算子的 compute 计算公式和 schedule 调度位于 Python 代码中，由 C++ 进行调用。</p>\n<p>在编译阶段，开发者编写 Python 代码，调用 TVM Python 前端，而 TVM 的 Python 前端又依赖于 C++ 后端生成的 libtvm.so libtvm_runtime.so 动态库。</p>\n<p>部署运行阶段即可以编写 Python 来执行，也可以直接编译 C++ 为可执行文件，参考<a href=\"https://github.com/ForCheetah/TvmCppDeploy\"> TVM C++ 部署</a></p>\n<h1 id=\"编译\"><a class=\"anchor\" href=\"#编译\">#</a> 编译</h1>\n<p>编译过程可以参考 TVM 英文文档和中文文档。文档中内容全面，这里只摘必要步骤，是最简单的部署安装方式。</p>\n<p>1. 编译 <a href=\"http://libtvm.so\">libtvm.so</a> libtvm_runtime.so</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build  <span class=\"token comment\"># 下载工程后进入工程目录 创新文件夹</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token function\">cp</span> cmake/config.cmake ./build</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token comment\"># 将 cmake 配置文件拷贝到 build 目录，可以更改需要的额配置选项</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    </pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token function\">make</span> <span class=\"token parameter variable\">-j32</span></pre></td></tr></table></figure><p>2. 配置临时运行环境</p>\n<p>个人喜欢用临时环境运行项目，可以避免 .bashrc 文件中添加过多环境路径；特别是开发人员有多个 TVM 项目的时候，可以避免项目的混淆。</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># 指定 tvm 库路径</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\"><span class=\"token environment constant\">PATH</span></span><span class=\"token operator\">=</span>/home/user/tvm/build:<span class=\"token environment constant\">$PATH</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># 指定 tvm Python 路径</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">PYTHONPATH</span><span class=\"token operator\">=</span>/home/user/tvm/python:<span class=\"token variable\">$&#123;PYTHONPATH&#125;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\"># 运行</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>python3 ***.py</pre></td></tr></table></figure><h1 id=\"tvm-基类\"><a class=\"anchor\" href=\"#tvm-基类\">#</a> TVM 基类</h1>\n<p>大概从事工程项目开发的人员对 Object, ObjectPtr, ObjectRef 等写法相对熟悉，但是没有相关经验的算法研究人员，还是先认识一下 TVM 中的这三个基类。</p>\n<p>TVM 中大部分类都继承自三大基础类   Object, ObjectPtr, ObjectRef。想多了解一下的话，可以学习博客<a href=\"https://zhuanlan.zhihu.com/p/656435385\"> TVM: Object, ObjectPtr, ObjectRef (以 tir.sch 为例)</a> 。</p>\n<p>可能比较粗显的总结一下：TVM 中以 &quot;Node&quot; 结尾的类是 Object 的派生类，去掉 &quot;Node&quot; 的类是 ObjectRef 的派生类，Object 的派生类负责 &quot;做事&quot;，ObjectRef 的派生类方便程序员引用，ObjectPtr 是桥梁。但总之这三者是一个东西（刚学时我就被这搞晕了：(  ）。</p>\n<p>举例说明如何使用 TVM 的三大基础类构造对象:</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">Anode</span><span class=\"token operator\">:</span> <span class=\"token base-clause\"><span class=\"token keyword\">public</span> tvm<span class=\"token double-colon punctuation\">::</span>runtime<span class=\"token double-colon punctuation\">::</span><span class=\"token class-name\">Object</span></span><span class=\"token punctuation\">&#123;</span>   </pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token keyword\">public</span><span class=\"token operator\">:</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>        <span class=\"token function\">Anode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>        <span class=\"token operator\">~</span><span class=\"token function\">Anode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>        std<span class=\"token double-colon punctuation\">::</span>string <span class=\"token function\">a</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span><span class=\"token keyword\">return</span> <span class=\"token string\">\"instance\"</span><span class=\"token punctuation\">;</span><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">A</span><span class=\"token operator\">:</span> <span class=\"token base-clause\"><span class=\"token keyword\">public</span> tvm<span class=\"token double-colon punctuation\">::</span>runtime<span class=\"token double-colon punctuation\">::</span><span class=\"token class-name\">ObjectRef</span></span><span class=\"token punctuation\">&#123;</span>       </pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    <span class=\"token keyword\">public</span><span class=\"token operator\">:</span>    </pre></td></tr><tr><td data-num=\"10\"></td><td><pre>    <span class=\"token function\">TVM_DEFINE_MUTABLE_OBJECT_REF_METHODS</span><span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">,</span> tvm<span class=\"token double-colon punctuation\">::</span>runtime<span class=\"token double-colon punctuation\">::</span>ObjectRef<span class=\"token punctuation\">,</span> Anode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span> </pre></td></tr><tr><td data-num=\"12\"></td><td><pre></pre></td></tr><tr><td data-num=\"13\"></td><td><pre><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">&#123;</span>    </pre></td></tr><tr><td data-num=\"14\"></td><td><pre>   tvm<span class=\"token double-colon punctuation\">::</span>runtime<span class=\"token double-colon punctuation\">::</span>ObjectPtr<span class=\"token operator\">&lt;</span>Anode<span class=\"token operator\">></span> n <span class=\"token operator\">=</span> tvm<span class=\"token double-colon punctuation\">::</span>runtime<span class=\"token double-colon punctuation\">::</span><span class=\"token generic-function\"><span class=\"token function\">make_object</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span>Anode<span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>  <span class=\"token comment\">// 桥梁 Ptr = make_object (本体 node)    </span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>   A <span class=\"token function\">ref</span><span class=\"token punctuation\">(</span>std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">move</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>  <span class=\"token comment\">// 指针</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>   ref<span class=\"token operator\">-></span><span class=\"token function\">a</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>          <span class=\"token comment\">// Anode 指针，可以引用 a ()</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>   <span class=\"token comment\">//ref.get ()->a (); // 基类 Object 指针，error Object has no member a ()</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><img loading=\"lazy\" data-src=\"1728562036320.jpg\" alt=\"Object 应用例子\"></p>\n<p>可以再看一下 TVM 工程中的例子，这段代码创建了本体 <code>CallNode</code> ，通过 make_object 生成桥梁 <code>ObjectPtr&lt;CallNode&gt;</code> ，最后赋值给指针 <code>Call</code>  的 <code>data_</code> 字段。</p>\n<h1 id=\"python和c相互调用\"><a class=\"anchor\" href=\"#python和c相互调用\">#</a> Python 和 C++ 相互调用</h1>\n<p>TVM 实现了 FFI (Foreign Function Interface) 机制，目标是使得任意语言下定义的函数都可以被任意其他语言调用。</p>\n<p>宏 TVM_REGISTER_GLOBAL</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">/*!</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre> * \\brief Register a function globally.</pre></td></tr><tr><td data-num=\"3\"></td><td><pre> * \\code</pre></td></tr><tr><td data-num=\"4\"></td><td><pre> *   TVM_REGISTER_GLOBAL(\"MyPrint\")</pre></td></tr><tr><td data-num=\"5\"></td><td><pre> *   .set_body([](TVMArgs args, TVMRetValue* rv) &#123;</pre></td></tr><tr><td data-num=\"6\"></td><td><pre> *   &#125;);</pre></td></tr><tr><td data-num=\"7\"></td><td><pre> * \\endcode</pre></td></tr><tr><td data-num=\"8\"></td><td><pre> */</pre></td></tr><tr><td data-num=\"9\"></td><td><pre> <span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">define</span> <span class=\"token macro-name function\">TVM_REGISTER_GLOBAL</span><span class=\"token expression\"><span class=\"token punctuation\">(</span>OpName<span class=\"token punctuation\">)</span> </span><span class=\"token punctuation\">\\</span></span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>  <span class=\"token expression\"><span class=\"token function\">TVM_STR_CONCAT</span><span class=\"token punctuation\">(</span>TVM_FUNC_REG_VAR_DEF<span class=\"token punctuation\">,</span> __COUNTER__<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token double-colon punctuation\">::</span>tvm<span class=\"token double-colon punctuation\">::</span>runtime<span class=\"token double-colon punctuation\">::</span><span class=\"token class-name\">Registry</span><span class=\"token double-colon punctuation\">::</span><span class=\"token function\">Register</span><span class=\"token punctuation\">(</span>OpName<span class=\"token punctuation\">)</span></span></pre></td></tr></table></figure><p>在 C++ 后端同过 “TVM_REGISTER_GLOBAL” 这个宏将函数注册为全局函数，该全局函数的类型为 PackedFunc。</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">TVM_REGISTER_GLOBAL</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"tvm.graph_executor_factory.create\"</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// 注册为全局函数 PackedFunc</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token punctuation\">.</span><span class=\"token function\">set_body</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>TVMArgs args<span class=\"token punctuation\">,</span> TVMRetValue<span class=\"token operator\">*</span> rv<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span>      </pre></td></tr><tr><td data-num=\"3\"></td><td><pre>        <span class=\"token operator\">*</span>rv <span class=\"token operator\">=</span> <span class=\"token function\">Module</span><span class=\"token punctuation\">(</span>exec<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>在 Python 前端可以通过 get_global_func () 函数来获取全局函数 PackedFunc 并执行</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>fcreate <span class=\"token operator\">=</span> get_global_func<span class=\"token punctuation\">(</span><span class=\"token string\">\"tvm.graph_executor_facttory.create\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>self<span class=\"token punctuation\">.</span>module <span class=\"token operator\">=</span> fcreate<span class=\"token punctuation\">(</span>graph_json_str<span class=\"token punctuation\">,</span> libmod<span class=\"token punctuation\">,</span> libmod_name<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>在 C++ 端也可以通过名字获取 全局函数 PackedFunc</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>PackedFunc pf <span class=\"token operator\">=</span> it<span class=\"token punctuation\">.</span><span class=\"token function\">GetFunction</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>python 端有大量如下类似代码，将有相同后缀的 PackedFunc 注册到 Python 端</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token triple-quoted-string string\">\"\"\"FFI APIs for tvm.ir\"\"\"</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">import</span> tvm<span class=\"token punctuation\">.</span>_ffi</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>tvm<span class=\"token punctuation\">.</span>_ffi<span class=\"token punctuation\">.</span>_init_api<span class=\"token punctuation\">(</span><span class=\"token string\">\"ir\"</span><span class=\"token punctuation\">,</span> __name__<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>Python 端获取并执行 PackedFunc</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>_ffi_api<span class=\"token punctuation\">.</span>Module_UpdateGlobalInfo<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">,</span> global_info<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！global_info)</p>\n",
            "tags": [
                "tvm",
                "compile"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/06/18/deployTVMPython/",
            "url": "https://forcheetah.github.io/2024/06/18/deployTVMPython/",
            "title": "【TVM】Python脚本实现模型编译和保存",
            "date_published": "2024-06-18T13:01:39.606Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇博客提供简单的 Python 脚本代码，实现 onnx 模型转换编译，保存为 TVM 的  <code>.so .params .json</code>  文件 。</p>\n<blockquote>\n<p>望长城内外，惟余莽莽；大河上下，顿失滔滔。<br>\n--------------- 教员<br>\n ------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"python脚本实现模型编译和保存\"><a class=\"anchor\" href=\"#python脚本实现模型编译和保存\">#</a> Python 脚本实现模型编译和保存</h1>\n<p>脚本中需要修改的就一些路径，很容易看明白，就不再过多介绍了。</p>\n<pre><code class=\"language-Python\">\nimport onnx\nfrom tvm.contrib.download import download_testdata\nfrom PIL import Image\nimport numpy as np\nimport tvm.relay as relay\nimport tvm\nfrom tvm.contrib import graph_executor\n\n\n# 图片\nimg_path = &quot;../image/imagenet_cat.png&quot;\n# img_url = &quot;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&quot;\n# img_path = download_testdata(img_url, &quot;../image/imagenet_cat.png&quot;, module=&quot;data&quot;)\n\n# 重设大小为 224x224\nresized_image = Image.open(img_path).resize((224, 224))\nimg_data = np.array(resized_image).astype(&quot;float32&quot;)\n\n# 输入图像是 HWC 布局，而 ONNX 需要 CHW 输入，所以转换数组\nimg_data = np.transpose(img_data, (2, 0, 1))\n\n# 根据 ImageNet 输入规范进行归一化\nimagenet_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\nimagenet_stddev = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\nnorm_img_data = (img_data / 255 - imagenet_mean) / imagenet_stddev\n\n\n# 添加 batch 维度，期望 4 维输入：NCHW。\nimg_data = np.expand_dims(norm_img_data, axis=0)\n# 保存为 bin 文件  \nnorm_img_data.astype(&quot;float32&quot;).tofile(&quot;../image/imagenet_cat.bin&quot;)\n\n\n# 目标设备配置\ntarget = 'llvm'  # 以CPU为例\n\ninput_name = &quot;data&quot;\nshape_dict = &#123;input_name: img_data.shape&#125;\n\nonnx_model = onnx.load(&quot;../model/simple.onnx&quot;)\n\nmod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n\nwith tvm.transform.PassContext(opt_level=3):\n    lib = relay.build(mod, target=target, params=params)\n\n\n# 运行相关\ndev = tvm.device(str(target), 0)\nmodule = graph_executor.GraphModule(lib[&quot;default&quot;](dev))\n\n# 保存库文件\nlib_fname = &quot;../lib/mod.so&quot;\nlib.export_library(lib_fname)\n\n# 保存模型参数\nparams_fname = &quot;../lib/mod.params&quot;\nwith open(params_fname, &quot;wb&quot;) as param_file:\n    param_file.write(relay.save_param_dict(lib.get_params()))\n\n# 保存JSON格式的计算图\njson_fname = &quot;../lib/mod.json&quot;\nwith open(json_fname, &quot;w&quot;) as json_file:\n    json_file.write(lib.get_executor_config())\n\ndtype = &quot;float32&quot;\nmodule.set_input(input_name, img_data)\nmodule.run()\noutput_shape = (1, 10)\ntvm_output = module.get_output(0, tvm.nd.empty(output_shape)).numpy()\n\nfrom scipy.special import softmax\n\n# 下载标签列表\nlabels_url = &quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;\nlabels_path = download_testdata(labels_url, &quot;synset.txt&quot;, module=&quot;data&quot;)\n\nwith open(labels_path, &quot;r&quot;) as f:\n    labels = [l.rstrip() for l in f]\n\n# 打开输出文件并读取输出张量\nscores = softmax(tvm_output)    #   直接输出模型结果\nscores = np.squeeze(tvm_output)\nranks = np.argsort(scores)[::-1]\nfor rank in ranks[0:5]:\n    print(&quot;class='%s' with probability=%f&quot; % (labels[rank], scores[rank]))\n\n</code></pre>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "tvm",
                "cmake",
                "runtime"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/06/10/deployTVM/",
            "url": "https://forcheetah.github.io/2024/06/10/deployTVM/",
            "title": "【TVM】C++部署运行TVM",
            "date_published": "2024-06-10T11:47:15.090Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>本篇博客主要介绍如何通过 G++ 编译器编译 C++ 代码，部署 TVM。</p>\n<blockquote>\n<p>总感觉，属于我们的时代还没开始，就要结束了呢。<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"现状\"><a class=\"anchor\" href=\"#现状\">#</a> 现状</h1>\n<p>TVM 官方文档:<a href=\"https://tvm.apache.org/docs\"> 英文文档</a> <a href=\"https://tvm.hyper.ai/\">中文文档</a> 主要介绍了通过 Python 脚本和 Python 命令行 tvmc 来编译和部署 TVM。但是以这两种方式部署，部署设备还需要安装 Python 运行环境，带来额外空间占用和开销。显然不能以这种方式部署。</p>\n<p>TVM 项目的 <a href=\"https://github.com/apache/tvm/tree/main/apps/howto_deploy\">howto_deploy</a> 目录下提供了 G++ 编译 C++ 代码部署 TVM 的方式。遗憾的是给的例子没有包含模型的权重.params 和图结构.json 的加载，也没有输入图片的加载。</p>\n<p>因此本博客提供了一个简单的 C++ 部署 TVM 工程，可以在 <a href=\"https://github.com/ForCheetah/TvmCppDeploy\">TvmCppDeploy 项目</a> 找到并下载，用于你的 TVM 项目部署。</p>\n<p>该项目没有使用 <a href=\"https://github.com/apache/tvm/tree/main/apps/howto_deploy\">TVM 项目 howto_deploy</a> 中的 Makefile，而是重写了 CMakeLists.txt 文件，更方便读懂和修改。</p>\n<h1 id=\"使用方式\"><a class=\"anchor\" href=\"#使用方式\">#</a> 使用方式</h1>\n<p>下载 <a href=\"https://github.com/ForCheetah/TvmCppDeploy\">TvmCppDeploy 项目</a> 到你的本地，可以通过下载 zip 文件后解压缩，也可以直接 git：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">git</span> clone https://github.com/ForCheetah/TvmCppDeploy.git</pre></td></tr></table></figure><p>进入项目根目录，进行必要的路径修改。和 <a href=\"https://github.com/apache/tvm/tree/main/apps/howto_deploy\">TVM 项目 howto_deploy</a> 一样，本项目也提供了两种部署方式，所需要修改的内容也有些不同。</p>\n<h1 id=\"重新编译-tvm_runtime\"><a class=\"anchor\" href=\"#重新编译-tvm_runtime\">#</a> 重新编译 tvm_runtime</h1>\n<p>重新编译 tvm_runtime，和个人的 C++ 文件编译在一起，编译好的可执行文件可独立执行。</p>\n<ul>\n<li>第 1 步：打开  <code>src/Resnet50_deploy.cc </code> 文件，找到 81 行  <code>const std::string artifacts_folder(&quot;/home/xiamu/whs/temp/resnet50-tvm/&quot;);</code>  ，将其中的 <code>/home/xiamu/whs/temp/resnet50-tvm/</code>  修改为自己的已经编译好的模型路径，该路径下应该存在有  <code>mod.so, mod.params, mod.json</code> 。</li>\n<li>第 2 步：还是 <code>src/Resnet50_deploy.cc </code> 文件， 找到 132 行，将其中的图片路径 <code>/home/xiamu/whs/python/remote_tvm/imagenet_cat.bin</code>  改为自己的图片路径，该 bin 文件应当是已经转换好的 float 格式文件。</li>\n<li>第 3 步：打开   <code>src/tvm\\_runtime\\_pack.cc</code> ， 将文件中所有的路径中的  <code>/home/xiamu/tvm</code>   修改为你本地 TVM 工程的根目录路径。 修改完一定要检查一下对应的目录中是否有相应的文件。</li>\n<li>第 4 步：打开  <code>CMakeLists.txt</code> , 找到第 10 行  <code>set(TVM_ROOT /home/xianmu/tvm)</code> ，将其中的 <code>/home/xianmu/tvm</code>  改成你本地 TVM 工程的根目录路径。</li>\n<li>为防止编译报错，可以将 <code>部署方式二： tvm_runtime.so 作为动态链接库编译</code> 对应的代码（43 至 63 行） 注释掉（当前可能还没有对其进行修改）。</li>\n<li>第 5 步：编译和执行，在根目录下：</li>\n</ul>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span> <span class=\"token parameter variable\">-j4</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>./MyRunnable</pre></td></tr></table></figure><h1 id=\"tvm_runtimeso-作为动态链接库编译\"><a class=\"anchor\" href=\"#tvm_runtimeso-作为动态链接库编译\">#</a> tvm_runtime.so 作为动态链接库编译</h1>\n<p>tvm_runtime.so 作为动态链接库，仅编译个人的 C++ 文件，运行时需要链接 libtvm_runtime.so</p>\n<p>这种方式的修改与第一种方式略有不同，修改如下：</p>\n<ul>\n<li>第 1 步：打开  <code>src/Resnet50_deploy.cc </code> 文件，找到 81 行  <code>const std::string artifacts_folder(&quot;/home/xiamu/whs/temp/resnet50-tvm/&quot;);</code>  ，将其中的 <code>/home/xiamu/whs/temp/resnet50-tvm/</code>  修改为自己的已经编译好的模型路径，该路径下应该存在有  <code>mod.so, mod.params, mod.json</code> 。</li>\n<li>第 2 步：还是 <code>src/Resnet50_deploy.cc </code> 文件， 找到 132 行，将其中的图片路径 <code>/home/xiamu/whs/python/remote_tvm/imagenet_cat.bin</code>  改为自己的图片路径，该 bin 文件应当是已经转换好的 float 格式文件。</li>\n<li>第 3 步：打开  <code>CMakeLists.txt</code> , 找到第 10 行  <code>set(TVM_ROOT /home/xianmu/tvm)</code> ，将其中的 <code>/home/xianmu/tvm</code>  改成你本地 TVM 工程的根目录路径。将 62 行的 <code>$&#123;TVM_ROOT&#125;/build</code>  libtvm_runtime.so 路径修改为你存放 libtvm_runtime.so 库的路径。</li>\n<li>为防止编译报错，可以将 <code>部署方式一： 重新编译 tvm_runtime</code>  对应的代码（17 至 38 行）注释掉（当前可能还没有对其进行修改）。</li>\n<li>第 5 步：编译和执行，在根目录下：</li>\n</ul>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token function\">mkdir</span> build <span class=\"token operator\">&amp;&amp;</span> <span class=\"token builtin class-name\">cd</span> build</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>cmake <span class=\"token punctuation\">..</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token function\">make</span> <span class=\"token parameter variable\">-j4</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>./MyExcute</pre></td></tr></table></figure><h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "tvm",
                "cmake",
                "runtime"
            ]
        },
        {
            "id": "https://forcheetah.github.io/2024/05/24/tvm1/",
            "url": "https://forcheetah.github.io/2024/05/24/tvm1/",
            "title": "【TVM】根据例子走通代码库",
            "date_published": "2024-05-24T14:49:36.319Z",
            "content_html": "<h1 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h1>\n<p>最近开始学习 TVM。感觉 TVM 英文文档中 <a href=\"https://tvm.apache.org/docs/dev/tutorial/codebase_walkthrough.html\">TVM Codebase Walkthrough by Example</a>    一节对于理解 TVM 工程非常有用。本篇文章只是翻译，可以直接跳转查看英文全文。</p>\n<blockquote>\n<p>这个时代有这么多愿意开源并将技术介绍给我们的行业大牛，真是我们的荣幸，膜拜！<br>\n------   大家好啊    我是   暮冬 Z 羡慕</p>\n</blockquote>\n<h1 id=\"codebase-structure-overview\"><a class=\"anchor\" href=\"#codebase-structure-overview\">#</a> Codebase Structure Overview</h1>\n<p>在 TVM 存储库的根目录中，我们有以下子目录，它们共同构成了大部分代码库。</p>\n<ul>\n<li><strong>src</strong><br>\nC++ code for operator compilation and deployment runtimes.<br>\n 算子编译 、 runtime 部署 的 C++ 代码</li>\n<li><strong>src/relay</strong><br>\nImplementation of Relay, a new functional IR for deep learning framework.<br>\nRelay IR 的实现      算子的映射关系在 src/relay/op</li>\n<li><strong>python</strong><br>\nPython frontend that wraps C++ functions and objects implemented in src.<br>\npython 前端</li>\n<li><strong>src/topi</strong><br>\nCompute definitions and backend schedules for standard neural network operators.<br>\n 计算标准神经网络算子的定义和后端调度</li>\n</ul>\n<p>TVM 中 Python 和 C++ 的互操作性不是单向的。尽管在 TVM 中 C++ 完成繁重的内部执行工作，Python 完成用户接口， TVM 中也存在 C++ 调用 Python 的情况：For example, the convolution operator is implemented in Python, and its implementation is invoked from C++ code in Relay.（Relay 中的 C++ 调用 Python 实现的卷积算子）</p>\n<h1 id=\"vector-add-example\"><a class=\"anchor\" href=\"#vector-add-example\">#</a> Vector Add Example</h1>\n<p>使用 vector add 的例子来查看底层 TVM API.</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>n <span class=\"token operator\">=</span> <span class=\"token number\">1024</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>A <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">'A'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>B <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">'B'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>C <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>compute<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> <span class=\"token keyword\">lambda</span> i<span class=\"token punctuation\">:</span> A<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> B<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"C\"</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这里面 A、B、C 都是  <code>tvm.tensor.Tensor</code>   其 Python 定义位于 <code>python/tvm/te/tensor.py</code> . 支撑的 C++ 定义位于 <code>include/tvm/te/tensor.h</code>  和 <code>src/te/tensor.cc</code>  所有的 Python 类型定义都能找到对应的相同名字的 C++ 定义。</p>\n<p>Python 对 C++ 的包装位于  <code>python/tvm/_ffi/</code> 。</p>\n<p>一个 Tensor 包含一个 Operation 类，定义于 python/tvm/te/tensor.py，对应的 C++ 实现位于 <code>include/tvm/te/operation.h</code>  和 <code>src/tvm/te/operation</code>  。 <code>Tensor</code>  是  <code>Operation</code>  类的输出。</p>\n<p>我们将输出张量 C 对应的操作传递给 <code>tvm.te.create_schedule()</code>  函数 （来自于 <code>python/tvm/te/schedule.py</code> 。）</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>s <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>create_schedule<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">.</span>op<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这个函数映射到 C++ 函数 <code>include/tvm/schedule.h</code> 。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>inline Schedule create_schedule<span class=\"token punctuation\">(</span>Array<span class=\"token operator\">&lt;</span>Operation<span class=\"token operator\">></span> ops<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>  <span class=\"token keyword\">return</span> Schedule<span class=\"token punctuation\">(</span>ops<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><code>Schedule</code>  包含 <code>Stage</code>  输出  <code>Operation</code>  的集合。</p>\n<p><code>Stage</code>  对应于一个操作 <code>Operation</code> 。上面的 vector add 操作中有两个 placeholder ops 和一个 compute op. 所以 <code>Schedule s</code>  有三个状态  <code>Stage</code> ，每个 <code>Stage</code>  持有以下信息： 循环嵌套结构、每个循环的类型（ <code>Parallel，Vectorized，Unrolled</code> ）、以及在下一个循环嵌套 <code>Stage</code>  中在哪里执行它自己的计算。</p>\n<p><code>Schedule</code>  和 <code>Stage</code>  本身定义在 <code>tvm/python/te/schedule.py</code> ，  <code>include/tvm/te/schedule.h</code> ， 和 <code>src/te/schedule/schedule_ops.cc</code> 。</p>\n<p>为简单起见，我们使用 <code>tvm.build(...)</code>  处理上方 <code>create_schedule()</code>  函数创建的默认 <code>Schedule s</code>  和 &lt;em&gt;。我们必须添加必要的线程绑定，来使得其能在 GPU 上运行：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>target <span class=\"token operator\">=</span> <span class=\"token string\">\"cuda\"</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>bx<span class=\"token punctuation\">,</span> tx <span class=\"token operator\">=</span> s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">.</span>op<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> factor<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span>bx<span class=\"token punctuation\">,</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>thread_axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"blockIdx.x\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>s<span class=\"token punctuation\">[</span>C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span>tx<span class=\"token punctuation\">,</span> tvm<span class=\"token punctuation\">.</span>te<span class=\"token punctuation\">.</span>thread_axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"threadIdx.x\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>fadd <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>build<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>A<span class=\"token punctuation\">,</span> B<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> target<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p><code>tvm.build(...)</code> ，定义在 <code>python/tvm/driver/build_module.py</code> ， 需要输入一个 <code>Schedule</code> ;  <code>input</code> , <code>output Tensor</code> ; 以及一个 <code>target</code> 。返回一个 <code>tvm.runtime.Module</code> 。</p>\n<p>整个 <code>tvm.build(...)</code>  过程可以分成两步：</p>\n<ul>\n<li>\n<p>i. 降级 高级的、初始的循环嵌套结构被转换为 最终的、低级的 IR</p>\n</li>\n<li>\n<p>ii. 代码生成 low level IR 生成目标机器码</p>\n</li>\n</ul>\n<p>降级是通过 <code>tvm.lower()</code>  函数完成的，它定义在 <code>python/tvm/build\\_module.py</code> 。第一，指定绑定推理，一个最初的循环嵌套结构就创建好了。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">def</span> <span class=\"token function\">lower</span><span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>          args<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>          name<span class=\"token operator\">=</span><span class=\"token string\">\"default_function\"</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>          binds<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>          simple_mode<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>   <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>   bounds <span class=\"token operator\">=</span> schedule<span class=\"token punctuation\">.</span>InferBound<span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>   stmt <span class=\"token operator\">=</span> schedule<span class=\"token punctuation\">.</span>ScheduleOps<span class=\"token punctuation\">(</span>sch<span class=\"token punctuation\">,</span> bounds<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>   <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr></table></figure><p>边界推断是推断所有循环边界和中间缓冲区大小的过程。如果你的目标是 CUDA，且你用了 share memory，它需要的最小 size 在此处确定。绑定推理时在 <code>src/te/schedule/bound.cc，src/te/schedule/graph.cc </code>  和  <code>src/te/schedule/message\\_passing.cc</code>  中实现的。</p>\n<p><code>stmt</code> ， <code>ScheduleOps()</code>  的输出，表示一个初识的循环嵌套结构。如果在 schedule 中已经应用了 <code>reorder</code>  和 <code>split</code>  原语，那么初始的循环嵌套结构已经反映了这些变化。 <code>ScheduleOps()</code>  定义在 <code>rc/te/schedule/schedule_ops.cc</code> 。</p>\n<p>接下来应用一些 lowering passes to  <code>stmt</code>  . 这些 passes 在 <code>src/tir/pass</code>  子文件夹下实现。举个例子，如果在你的 <code>schedule</code>  中应用了 <code>vectorize</code>  或者 <code>unroll</code>  原语，他们会被应用到循环 vectorization 和 unrolling passes。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>stmt <span class=\"token operator\">=</span> ir_pass<span class=\"token punctuation\">.</span>VectorizeLoop<span class=\"token punctuation\">(</span>stmt<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>stmt <span class=\"token operator\">=</span> ir_pass<span class=\"token punctuation\">.</span>UnrollLoop<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    stmt<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_step<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_depth<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>auto_unroll_max_extent<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    cfg<span class=\"token punctuation\">.</span>unroll_explicit<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr></table></figure><p>在降级 lowering 结束后， <code>build()</code>  函数生成目标机器代码。如果你的设备是 X86, 这个代码可能包含 SSE 或者 AVX 指令；如果是 CUDA 设备，将包含 PTX 指令。 此外，除了目标特定的机器代码之外，TVM 还生成负责内存管理、内核启动等的主机端代码。</p>\n<p><code>build\\_module()</code>  函数完成代码生成，定义在 <code>python/tvm/target/codegen.py</code> 。在 C++ 端代码生成定义在 <code>src/target/codegen</code> 。 <code>build\\_module()</code>  Python 函数会搜索在 <code>src/target/codegen/codegen.cc</code>  中的 <code>build()</code>  函数。</p>\n<p><code>build()</code>  函数 <code>PackedFunc</code>  注册表中为目标设备查找代码生成器，并调用找到的函数。例如， <code>codegen.build\\_cuda</code>  函数注册在 <code>src/codegen/build_cuda_on.cc</code> ，就像这样：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>TVM_REGISTER_GLOBAL<span class=\"token punctuation\">(</span><span class=\"token string\">\"codegen.build_cuda\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token punctuation\">.</span>set_body<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>TVMArgs args<span class=\"token punctuation\">,</span> TVMRetValue<span class=\"token operator\">*</span> rv<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token operator\">*</span>rv <span class=\"token operator\">=</span> BuildCUDA<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  <span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>上方的 <code>BuildCUDA()</code>  函数使用定义在 <code>src/codegen/codegen_cuda.cc</code>  的 <code>CodeGenCUDA</code>  类，从 lowered IR 生成 CUDA kernel source，并使用 NVRTC 编译 kernel。如果你的目标设备使用 LLVM，包括 X86、ARM、NVPTX 和 AMDGPU，代码可由定义在 <code>src/codegen/llvm/codegen_llvm.cc</code>  的 <code>CodeGenLLVM</code>  来生成。 <code>CodeGenLLVM</code>  将 TVM IR 转换成 LLVM IR，运行一些 LLVM 优化 passes，以及生成目标机器码。</p>\n<p>在 <code>src/codegen/codegen.cc</code>  中的 <code>Build()</code>  函数会返回一个 <code>runtime::Module</code>  类，它定义在 <code>include/tvm/runtime/module.h</code>  和 <code>src/runtime/module.cc</code> 。一个 <code>Module</code>  类是一个潜在目标 设备的特定 <code>ModuleNode</code>  的容器。</p>\n<p>每个后端都实现一个 <code>ModuleNode</code>  的子类，来添加目标特定的 runtime API 调用。 例如，CUDA 后端在 <code>src/runtime/cuda/cuda_module.cc</code>  实现 <code>CUDAModuleNode</code>  类，来管理 CUDA 驱动 API。上方的 <code>BuildCUDA()</code>  函数用 <code>runtime::Module</code>  包装了 <code>CUDAModuleNode</code> ，并包装到 Python 端。LLVM 后端在 <code>src/codegen/llvm/llvm_module.cc</code>  实现了 <code>LLVMModuleNode</code> ，处理了 JIT 执行和编译代码。其他对应各个后端的 <code>ModuleNode</code>  子类可以在 <code>src/runtime</code>  子文件夹找到。<br>\n返回的 <code>module</code> ，可以被认作编译函数和设备 API 的组合，可以被 TVM 的 NDArray objects 调用。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>dev <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span>target<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>a <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>uniform<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>b <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>uniform<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>B<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>c <span class=\"token operator\">=</span> tvm<span class=\"token punctuation\">.</span>nd<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>C<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dev<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>fadd<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>output <span class=\"token operator\">=</span> c<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>在幕后，TVM 会自动分配设备内存并管理内存传输。为了实现这个目标，每个后端都需要继承在 <code>include/tvm/runtime/device_api.h</code>  定义的 <code>DeviceAPI</code>  类，使用设备特定的 API 重写里面的内存管理方法。例如，CUDA 后端在 <code>src/runtime/cuda/cuda_device_api.cc</code>  使用 <code>cudaMalloc</code> 、 <code>cudaMemcpy</code>  实现了 <code>CUDADeviceAPI</code> .</p>\n<p>第一次使用 <code>fadd(a, b, c)</code>  调用编译后的模块时，会调用  <code>ModuleNode</code>  的  <code>GetFunction()</code>  方法来获取可用于内核调用的  <code>PackedFunc</code> 。例如，在 <code>src/runtime/cuda/cuda_module.cc</code>  CUDA 后端实现了 <code>CUDAModuleNode::GetFunction()</code>  函数如下：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>PackedFunc CUDAModuleNode<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>GetFunction<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>      const std<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>string<span class=\"token operator\">&amp;</span> name<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>      const std<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>shared_ptr<span class=\"token operator\">&lt;</span>ModuleNode<span class=\"token operator\">></span><span class=\"token operator\">&amp;</span> sptr_to_self<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  auto it <span class=\"token operator\">=</span> fmap_<span class=\"token punctuation\">.</span>find<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>  const FunctionInfo<span class=\"token operator\">&amp;</span> info <span class=\"token operator\">=</span> it<span class=\"token operator\">-</span><span class=\"token operator\">></span>second<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>  CUDAWrappedFunc f<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>  f<span class=\"token punctuation\">.</span>Init<span class=\"token punctuation\">(</span>this<span class=\"token punctuation\">,</span> sptr_to_self<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>arg_types<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>launch_param_tags<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>  <span class=\"token keyword\">return</span> PackFuncVoidAddr<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">.</span>arg_types<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p><code>PackedFunc</code>  的重载函数 <code>operator()</code>  会被调用。从而会调用定义在 <code>src/runtime/cuda/cuda_module.cc</code>  的 <code>CUDAWrappedFunc</code>  的 <code>operator()</code>  函数，最终我们会看到 <code>cuLaunchKernel</code>  驱动会调用：</p>\n<figure class=\"highlight cpp\"><figcaption data-lang=\"C++\"><span>p</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">CUDAWrappedFunc</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre> <span class=\"token keyword\">public</span><span class=\"token operator\">:</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>  <span class=\"token keyword\">void</span> <span class=\"token function\">Init</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>  <span class=\"token keyword\">void</span> <span class=\"token keyword\">operator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>TVMArgs args<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>                  TVMRetValue<span class=\"token operator\">*</span> rv<span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>                  <span class=\"token keyword\">void</span><span class=\"token operator\">*</span><span class=\"token operator\">*</span> void_args<span class=\"token punctuation\">)</span> <span class=\"token keyword\">const</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token keyword\">int</span> device_id<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    <span class=\"token function\">CUDA_CALL</span><span class=\"token punctuation\">(</span><span class=\"token function\">cudaGetDevice</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>device_id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>fcache_<span class=\"token punctuation\">[</span>device_id<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>      fcache_<span class=\"token punctuation\">[</span>device_id<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> m_<span class=\"token operator\">-></span><span class=\"token function\">GetFunc</span><span class=\"token punctuation\">(</span>device_id<span class=\"token punctuation\">,</span> func_name_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    CUstream strm <span class=\"token operator\">=</span> <span class=\"token generic-function\"><span class=\"token function\">static_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span>CUstream<span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span><span class=\"token class-name\">CUDAThreadEntry</span><span class=\"token double-colon punctuation\">::</span><span class=\"token function\">ThreadLocal</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">-></span>stream<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>    ThreadWorkLoad wl <span class=\"token operator\">=</span> launch_param_config_<span class=\"token punctuation\">.</span><span class=\"token function\">Extract</span><span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    CUresult result <span class=\"token operator\">=</span> <span class=\"token function\">cuLaunchKernel</span><span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>        fcache_<span class=\"token punctuation\">[</span>device_id<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">grid_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">grid_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">grid_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">block_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">block_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>        wl<span class=\"token punctuation\">.</span><span class=\"token function\">block_dim</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>        <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> strm<span class=\"token punctuation\">,</span> void_args<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>  <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>本文概括了 TVM 如何编译和执行函数。 虽然本文没有详细说明 TOPI 或 Relay，但最终所有神经网络算子都会经历与上述相同的编译过程。</p>\n<h1 id=\"后记\"><a class=\"anchor\" href=\"#后记\">#</a> 后记</h1>\n<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a href=\"https://github.com/ForCheetah/ForCheetah.github.io\">github 项目</a> 或随便一个项目下提出 issue，或者<a href=\"https://www.zhihu.com/people/guai-dao-ji-de-3-50\">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>\n",
            "tags": [
                "accelerate",
                "conv",
                "tvm"
            ]
        }
    ]
}
<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://forcheetah.github.io</id>
    <title>Пусть этот камень будет более крепким, чем человек • Posts by &#34;tvm&#34; tag</title>
    <link href="https://forcheetah.github.io" />
    <updated>2024-05-24T14:49:36.319Z</updated>
    <category term="bar" />
    <category term="baz" />
    <category term="Linux" />
    <category term="openBlas" />
    <category term="lib" />
    <category term="accelerate" />
    <category term="conv" />
    <category term="tvm" />
    <entry>
        <id>https://forcheetah.github.io/2024/05/24/tvm1/</id>
        <title>【TVM】根据例子走通代码库</title>
        <link rel="alternate" href="https://forcheetah.github.io/2024/05/24/tvm1/"/>
        <content type="html">&lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;最近开始学习 TVM。感觉 TVM 英文文档中 &lt;a href=&#34;https://tvm.apache.org/docs/dev/tutorial/codebase_walkthrough.html&#34;&gt;TVM Codebase Walkthrough by Example&lt;/a&gt;    一节对于理解 TVM 工程非常有用。本篇文章只是翻译，可以直接跳转查看英文全文。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个时代有这么多愿意开源并将技术介绍给我们的行业大牛，真是我们的荣幸，膜拜！&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;codebase-structure-overview&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#codebase-structure-overview&#34;&gt;#&lt;/a&gt; Codebase Structure Overview&lt;/h1&gt;
&lt;p&gt;在 TVM 存储库的根目录中，我们有以下子目录，它们共同构成了大部分代码库。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;src&lt;/strong&gt;&lt;br&gt;
C++ code for operator compilation and deployment runtimes.&lt;br&gt;
 算子编译 、 runtime 部署 的 C++ 代码&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;src/relay&lt;/strong&gt;&lt;br&gt;
Implementation of Relay, a new functional IR for deep learning framework.&lt;br&gt;
Relay IR 的实现      算子的映射关系在 src/relay/op&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;python&lt;/strong&gt;&lt;br&gt;
Python frontend that wraps C++ functions and objects implemented in src.&lt;br&gt;
python 前端&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;src/topi&lt;/strong&gt;&lt;br&gt;
Compute definitions and backend schedules for standard neural network operators.&lt;br&gt;
 计算标准神经网络算子的定义和后端调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TVM 中 Python 和 C++ 的互操作性不是单向的。尽管在 TVM 中 C++ 完成繁重的内部执行工作，Python 完成用户接口， TVM 中也存在 C++ 调用 Python 的情况：For example, the convolution operator is implemented in Python, and its implementation is invoked from C++ code in Relay.（Relay 中的 C++ 调用 Python 实现的卷积算子）&lt;/p&gt;
&lt;h1 id=&#34;vector-add-example&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#vector-add-example&#34;&gt;#&lt;/a&gt; Vector Add Example&lt;/h1&gt;
&lt;p&gt;使用 vector add 的例子来查看底层 TVM API.&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;n &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;1024&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;A &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;placeholder&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#39;A&#39;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;B &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;placeholder&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#39;B&#39;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;C &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;compute&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;shape&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;lambda&lt;/span&gt; i&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt; A&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;C&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这里面 A、B、C 都是  &lt;code&gt;tvm.tensor.Tensor&lt;/code&gt;   其 Python 定义位于 &lt;code&gt;python/tvm/te/tensor.py&lt;/code&gt; . 支撑的 C++ 定义位于 &lt;code&gt;include/tvm/te/tensor.h&lt;/code&gt;  和 &lt;code&gt;src/te/tensor.cc&lt;/code&gt;  所有的 Python 类型定义都能找到对应的相同名字的 C++ 定义。&lt;/p&gt;
&lt;p&gt;Python 对 C++ 的包装位于  &lt;code&gt;python/tvm/_ffi/&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;一个 Tensor 包含一个 Operation 类，定义于 python/tvm/te/tensor.py，对应的 C++ 实现位于 &lt;code&gt;include/tvm/te/operation.h&lt;/code&gt;  和 &lt;code&gt;src/tvm/te/operation&lt;/code&gt;  。 &lt;code&gt;Tensor&lt;/code&gt;  是  &lt;code&gt;Operation&lt;/code&gt;  类的输出。&lt;/p&gt;
&lt;p&gt;我们将输出张量 C 对应的操作传递给 &lt;code&gt;tvm.te.create_schedule()&lt;/code&gt;  函数 （来自于 &lt;code&gt;python/tvm/te/schedule.py&lt;/code&gt; 。）&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;create_schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;op&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这个函数映射到 C++ 函数 &lt;code&gt;include/tvm/schedule.h&lt;/code&gt; 。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;inline Schedule create_schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;Array&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;Operation&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt; ops&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;return&lt;/span&gt; Schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;ops&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;Schedule&lt;/code&gt;  包含 &lt;code&gt;Stage&lt;/code&gt;  输出  &lt;code&gt;Operation&lt;/code&gt;  的集合。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Stage&lt;/code&gt;  对应于一个操作 &lt;code&gt;Operation&lt;/code&gt; 。上面的 vector add 操作中有两个 placeholder ops 和一个 compute op. 所以 &lt;code&gt;Schedule s&lt;/code&gt;  有三个状态  &lt;code&gt;Stage&lt;/code&gt; ，每个 &lt;code&gt;Stage&lt;/code&gt;  持有以下信息： 循环嵌套结构、每个循环的类型（ &lt;code&gt;Parallel，Vectorized，Unrolled&lt;/code&gt; ）、以及在下一个循环嵌套 &lt;code&gt;Stage&lt;/code&gt;  中在哪里执行它自己的计算。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Schedule&lt;/code&gt;  和 &lt;code&gt;Stage&lt;/code&gt;  本身定义在 &lt;code&gt;tvm/python/te/schedule.py&lt;/code&gt; ，  &lt;code&gt;include/tvm/te/schedule.h&lt;/code&gt; ， 和 &lt;code&gt;src/te/schedule/schedule_ops.cc&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;为简单起见，我们使用 &lt;code&gt;tvm.build(...)&lt;/code&gt;  处理上方 &lt;code&gt;create_schedule()&lt;/code&gt;  函数创建的默认 &lt;code&gt;Schedule s&lt;/code&gt;  和 &amp;lt;em&amp;gt;。我们必须添加必要的线程绑定，来使得其能在 GPU 上运行：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;target &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token string&#34;&gt;&#34;cuda&#34;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;bx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tx &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;split&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;op&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;axis&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; factor&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;bind&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;bx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;thread_axis&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;blockIdx.x&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;bind&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;tx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;thread_axis&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;threadIdx.x&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;fadd &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;build&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;s&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; target&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;tvm.build(...)&lt;/code&gt; ，定义在 &lt;code&gt;python/tvm/driver/build_module.py&lt;/code&gt; ， 需要输入一个 &lt;code&gt;Schedule&lt;/code&gt; ;  &lt;code&gt;input&lt;/code&gt; , &lt;code&gt;output Tensor&lt;/code&gt; ; 以及一个 &lt;code&gt;target&lt;/code&gt; 。返回一个 &lt;code&gt;tvm.runtime.Module&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;整个 &lt;code&gt;tvm.build(...)&lt;/code&gt;  过程可以分成两步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;i. 降级 高级的、初始的循环嵌套结构被转换为 最终的、低级的 IR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ii. 代码生成 low level IR 生成目标机器码&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;降级是通过 &lt;code&gt;tvm.lower()&lt;/code&gt;  函数完成的，它定义在 &lt;code&gt;python/tvm/build\_module.py&lt;/code&gt; 。第一，指定绑定推理，一个最初的循环嵌套结构就创建好了。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;default_function&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          binds&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token boolean&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          simple_mode&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token boolean&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   bounds &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; schedule&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;InferBound&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; schedule&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;ScheduleOps&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; bounds&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;边界推断是推断所有循环边界和中间缓冲区大小的过程。如果你的目标是 CUDA，且你用了 share memory，它需要的最小 size 在此处确定。绑定推理时在 &lt;code&gt;src/te/schedule/bound.cc，src/te/schedule/graph.cc &lt;/code&gt;  和  &lt;code&gt;src/te/schedule/message\_passing.cc&lt;/code&gt;  中实现的。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;stmt&lt;/code&gt; ， &lt;code&gt;ScheduleOps()&lt;/code&gt;  的输出，表示一个初识的循环嵌套结构。如果在 schedule 中已经应用了 &lt;code&gt;reorder&lt;/code&gt;  和 &lt;code&gt;split&lt;/code&gt;  原语，那么初始的循环嵌套结构已经反映了这些变化。 &lt;code&gt;ScheduleOps()&lt;/code&gt;  定义在 &lt;code&gt;rc/te/schedule/schedule_ops.cc&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;接下来应用一些 lowering passes to  &lt;code&gt;stmt&lt;/code&gt;  . 这些 passes 在 &lt;code&gt;src/tir/pass&lt;/code&gt;  子文件夹下实现。举个例子，如果在你的 &lt;code&gt;schedule&lt;/code&gt;  中应用了 &lt;code&gt;vectorize&lt;/code&gt;  或者 &lt;code&gt;unroll&lt;/code&gt;  原语，他们会被应用到循环 vectorization 和 unrolling passes。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; ir_pass&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;VectorizeLoop&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;stmt&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; ir_pass&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;UnrollLoop&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    stmt&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_step&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_depth&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_extent&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;unroll_explicit&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;在降级 lowering 结束后， &lt;code&gt;build()&lt;/code&gt;  函数生成目标机器代码。如果你的设备是 X86, 这个代码可能包含 SSE 或者 AVX 指令；如果是 CUDA 设备，将包含 PTX 指令。 此外，除了目标特定的机器代码之外，TVM 还生成负责内存管理、内核启动等的主机端代码。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;build\_module()&lt;/code&gt;  函数完成代码生成，定义在 &lt;code&gt;python/tvm/target/codegen.py&lt;/code&gt; 。在 C++ 端代码生成定义在 &lt;code&gt;src/target/codegen&lt;/code&gt; 。 &lt;code&gt;build\_module()&lt;/code&gt;  Python 函数会搜索在 &lt;code&gt;src/target/codegen/codegen.cc&lt;/code&gt;  中的 &lt;code&gt;build()&lt;/code&gt;  函数。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;build()&lt;/code&gt;  函数 &lt;code&gt;PackedFunc&lt;/code&gt;  注册表中为目标设备查找代码生成器，并调用找到的函数。例如， &lt;code&gt;codegen.build\_cuda&lt;/code&gt;  函数注册在 &lt;code&gt;src/codegen/build_cuda_on.cc&lt;/code&gt; ，就像这样：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;TVM_REGISTER_GLOBAL&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;codegen.build_cuda&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;set_body&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;TVMArgs args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; TVMRetValue&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; rv&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;rv &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; BuildCUDA&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;args&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;上方的 &lt;code&gt;BuildCUDA()&lt;/code&gt;  函数使用定义在 &lt;code&gt;src/codegen/codegen_cuda.cc&lt;/code&gt;  的 &lt;code&gt;CodeGenCUDA&lt;/code&gt;  类，从 lowered IR 生成 CUDA kernel source，并使用 NVRTC 编译 kernel。如果你的目标设备使用 LLVM，包括 X86、ARM、NVPTX 和 AMDGPU，代码可由定义在 &lt;code&gt;src/codegen/llvm/codegen_llvm.cc&lt;/code&gt;  的 &lt;code&gt;CodeGenLLVM&lt;/code&gt;  来生成。 &lt;code&gt;CodeGenLLVM&lt;/code&gt;  将 TVM IR 转换成 LLVM IR，运行一些 LLVM 优化 passes，以及生成目标机器码。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;src/codegen/codegen.cc&lt;/code&gt;  中的 &lt;code&gt;Build()&lt;/code&gt;  函数会返回一个 &lt;code&gt;runtime::Module&lt;/code&gt;  类，它定义在 &lt;code&gt;include/tvm/runtime/module.h&lt;/code&gt;  和 &lt;code&gt;src/runtime/module.cc&lt;/code&gt; 。一个 &lt;code&gt;Module&lt;/code&gt;  类是一个潜在目标 设备的特定 &lt;code&gt;ModuleNode&lt;/code&gt;  的容器。&lt;/p&gt;
&lt;p&gt;每个后端都实现一个 &lt;code&gt;ModuleNode&lt;/code&gt;  的子类，来添加目标特定的 runtime API 调用。 例如，CUDA 后端在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  实现 &lt;code&gt;CUDAModuleNode&lt;/code&gt;  类，来管理 CUDA 驱动 API。上方的 &lt;code&gt;BuildCUDA()&lt;/code&gt;  函数用 &lt;code&gt;runtime::Module&lt;/code&gt;  包装了 &lt;code&gt;CUDAModuleNode&lt;/code&gt; ，并包装到 Python 端。LLVM 后端在 &lt;code&gt;src/codegen/llvm/llvm_module.cc&lt;/code&gt;  实现了 &lt;code&gt;LLVMModuleNode&lt;/code&gt; ，处理了 JIT 执行和编译代码。其他对应各个后端的 &lt;code&gt;ModuleNode&lt;/code&gt;  子类可以在 &lt;code&gt;src/runtime&lt;/code&gt;  子文件夹找到。&lt;br&gt;
返回的 &lt;code&gt;module&lt;/code&gt; ，可以被认作编译函数和设备 API 的组合，可以被 TVM 的 NDArray objects 调用。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;dev &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;device&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;target&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;a &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;random&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;uniform&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;size&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;astype&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;b &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;random&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;uniform&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;size&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;astype&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;B&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;c &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;zeros&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dtype&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;fadd&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;a&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; b&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; c&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;output &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; c&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;numpy&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;在幕后，TVM 会自动分配设备内存并管理内存传输。为了实现这个目标，每个后端都需要继承在 &lt;code&gt;include/tvm/runtime/device_api.h&lt;/code&gt;  定义的 &lt;code&gt;DeviceAPI&lt;/code&gt;  类，使用设备特定的 API 重写里面的内存管理方法。例如，CUDA 后端在 &lt;code&gt;src/runtime/cuda/cuda_device_api.cc&lt;/code&gt;  使用 &lt;code&gt;cudaMalloc&lt;/code&gt; 、 &lt;code&gt;cudaMemcpy&lt;/code&gt;  实现了 &lt;code&gt;CUDADeviceAPI&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;第一次使用 &lt;code&gt;fadd(a, b, c)&lt;/code&gt;  调用编译后的模块时，会调用  &lt;code&gt;ModuleNode&lt;/code&gt;  的  &lt;code&gt;GetFunction()&lt;/code&gt;  方法来获取可用于内核调用的  &lt;code&gt;PackedFunc&lt;/code&gt; 。例如，在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  CUDA 后端实现了 &lt;code&gt;CUDAModuleNode::GetFunction()&lt;/code&gt;  函数如下：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;PackedFunc CUDAModuleNode&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;GetFunction&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      const std&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;string&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; name&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      const std&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;shared_ptr&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;ModuleNode&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; sptr_to_self&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  auto it &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; fmap_&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;find&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;name&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  const FunctionInfo&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; info &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; it&lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;second&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  CUDAWrappedFunc f&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  f&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;Init&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;this&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; sptr_to_self&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;arg_types&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;size&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;launch_param_tags&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;return&lt;/span&gt; PackFuncVoidAddr&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;f&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;arg_types&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;PackedFunc&lt;/code&gt;  的重载函数 &lt;code&gt;operator()&lt;/code&gt;  会被调用。从而会调用定义在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  的 &lt;code&gt;CUDAWrappedFunc&lt;/code&gt;  的 &lt;code&gt;operator()&lt;/code&gt;  函数，最终我们会看到 &lt;code&gt;cuLaunchKernel&lt;/code&gt;  驱动会调用：&lt;/p&gt;
&lt;figure class=&#34;highlight cpp&#34;&gt;&lt;figcaption data-lang=&#34;C++&#34;&gt;&lt;span&gt;p&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;token class-name&#34;&gt;CUDAWrappedFunc&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt; &lt;span class=&#34;token keyword&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;Init&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;operator&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;TVMArgs args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                  TVMRetValue&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; rv&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                  &lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; void_args&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; device_id&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token function&#34;&gt;CUDA_CALL&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;cudaGetDevice&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;fcache_&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;nullptr&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      fcache_&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; m_&lt;span class=&#34;token operator&#34;&gt;-&gt;&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;GetFunc&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; func_name_&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    CUstream strm &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token generic-function&#34;&gt;&lt;span class=&#34;token function&#34;&gt;static_cast&lt;/span&gt;&lt;span class=&#34;token generic class-name&#34;&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;CUstream&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token class-name&#34;&gt;CUDAThreadEntry&lt;/span&gt;&lt;span class=&#34;token double-colon punctuation&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;ThreadLocal&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;-&gt;&lt;/span&gt;stream&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    ThreadWorkLoad wl &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; launch_param_config_&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;Extract&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;args&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    CUresult result &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;cuLaunchKernel&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;16&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        fcache_&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;device_id&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;17&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;grid_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;18&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;grid_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;19&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;grid_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;20&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;block_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;21&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;block_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;22&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        wl&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;block_dim&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;23&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; strm&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; void_args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;24&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;25&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;本文概括了 TVM 如何编译和执行函数。 虽然本文没有详细说明 TOPI 或 Relay，但最终所有神经网络算子都会经历与上述相同的编译过程。&lt;/p&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
</content>
        <category term="accelerate" />
        <category term="conv" />
        <category term="tvm" />
        <updated>2024-05-24T14:49:36.319Z</updated>
    </entry>
</feed>

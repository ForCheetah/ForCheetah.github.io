<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://forcheetah.github.io</id>
    <title>Пусть этот камень будет более крепким, чем человек • Posts by &#34;accelerate&#34; tag</title>
    <link href="https://forcheetah.github.io" />
    <updated>2024-05-26T08:19:13.185Z</updated>
    <category term="bar" />
    <category term="baz" />
    <category term="Linux" />
    <category term="openBlas" />
    <category term="lib" />
    <category term="accelerate" />
    <category term="conv" />
    <category term="tvm" />
    <entry>
        <id>https://forcheetah.github.io/2024/05/26/zatan3D/</id>
        <title>【3D建模】IS-7攻城锤流纹岩皮肤展示</title>
        <link rel="alternate" href="https://forcheetah.github.io/2024/05/26/zatan3D/"/>
        <content type="html">&lt;h1 id=&#34;坦克世界皮肤图片&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#坦克世界皮肤图片&#34;&gt;#&lt;/a&gt; 坦克世界皮肤图片&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1024x.jpg&#34; alt=&#34;洪水猛兽&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;24x.jpg&#34; alt=&#34;突击重坦&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;2x.jpg&#34; alt=&#34;钢铁之盾&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;2024x.jpg&#34; alt=&#34;胜利之师&#34;&gt;&lt;/p&gt;
&lt;p&gt;IS-7 皮肤台词：&amp;quot; 原来是这样的： 如果有人回到基地，至少一辆坦克，那就算是一次营救任务。如果无人返回，那就是一次侦察任务。那就是他们在总部报告中对我们的分类方式，而我就是参加了一次营救任务。事情是这样的：我们整个小队一起移动，一些奇怪的大雾导致能见度为 0，坦克只能摸索着前行，就好像在牛奶中前进一样。然后坦克出现了。D 系的坦克。我记得它们的外形，也知道如何应对它们。你也知道，我以前经历过一些战事。接下来又遇到了没有沙子的沙尘暴… 我也不知道该怎么解释。我经历了持续的耳鸣，一些嗡嗡的声音。眼睛仿佛沾满了焦油。奇怪的低语。黑暗… 如同活物一般。我们摔得东倒西歪，被分散了。我害怕吗？当然不怕。起初我们都很困惑，但后来就适应了。没有什么区别，我们来都来了，还有什么好怕的？什么都改变不了。服从命令，祈祷可以回家。所以我就回来了… 一个人回来了。上尉，你的问题很奇怪。当小队重新集结时，我会回去吗？你已经知道答案，我们发过同样的誓言。而这只会发生一次。发了誓，就要遵守它。现在不是犹豫不决的时候了。我不希望加入一次... 侦察任务。当我们抵达那里的时候... 我们会挺过去的。&lt;/p&gt;
&lt;h1 id=&#34;一些制作过程&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#一些制作过程&#34;&gt;#&lt;/a&gt; 一些制作过程&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;0XGEG466R.png&#34; alt=&#34;solidworks 3D 建模&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;aaaaa.png&#34; alt=&#34;solidworks 3D 建模&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;6I6P.png&#34; alt=&#34;solidworks 3D 建模&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;194309.jpg&#34; alt=&#34;打印件出炉&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;120504.jpg&#34; alt=&#34;组装&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;120514.jpg&#34; alt=&#34;上色&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;最后&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#最后&#34;&gt;#&lt;/a&gt; 最后&lt;/h1&gt;
&lt;p&gt;该 IS-7 流纹岩皮肤模型还履带转动、炮塔旋转、炮管上下、夜视灯开关和发动机声音模拟功能。&lt;/p&gt;
&lt;p&gt;感兴趣的小伙伴可以到 bilibili 观看制作过程的视频呀！&lt;/p&gt;
&lt;p&gt;视频链接：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Tp4y1c719/&#34;&gt;【坦克模型】IS-7 攻城锤 坦克世界 is7 流纹岩 3D 皮肤模型&lt;/a&gt;&lt;/p&gt;
</content>
        <category term="accelerate" />
        <category term="conv" />
        <updated>2024-05-26T08:19:13.185Z</updated>
    </entry>
    <entry>
        <id>https://forcheetah.github.io/2024/05/24/tvm1/</id>
        <title>【TVM】根据例子走通代码库</title>
        <link rel="alternate" href="https://forcheetah.github.io/2024/05/24/tvm1/"/>
        <content type="html">&lt;h1 id=&#34;前言&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前言&#34;&gt;#&lt;/a&gt; 前言&lt;/h1&gt;
&lt;p&gt;最近开始学习 TVM。感觉 TVM 英文文档中 &lt;a href=&#34;https://tvm.apache.org/docs/dev/tutorial/codebase_walkthrough.html&#34;&gt;TVM Codebase Walkthrough by Example&lt;/a&gt;    一节对于理解 TVM 工程非常有用。本篇文章只是翻译，可以直接跳转查看英文全文。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个时代有这么多愿意开源并将技术介绍给我们的行业大牛，真是我们的荣幸，膜拜！&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;codebase-structure-overview&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#codebase-structure-overview&#34;&gt;#&lt;/a&gt; Codebase Structure Overview&lt;/h1&gt;
&lt;p&gt;在 TVM 存储库的根目录中，我们有以下子目录，它们共同构成了大部分代码库。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;src&lt;/strong&gt;&lt;br&gt;
C++ code for operator compilation and deployment runtimes.&lt;br&gt;
 算子编译 、 runtime 部署 的 C++ 代码&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;src/relay&lt;/strong&gt;&lt;br&gt;
Implementation of Relay, a new functional IR for deep learning framework.&lt;br&gt;
Relay IR 的实现      算子的映射关系在 src/relay/op&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;python&lt;/strong&gt;&lt;br&gt;
Python frontend that wraps C++ functions and objects implemented in src.&lt;br&gt;
python 前端&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;src/topi&lt;/strong&gt;&lt;br&gt;
Compute definitions and backend schedules for standard neural network operators.&lt;br&gt;
 计算标准神经网络算子的定义和后端调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TVM 中 Python 和 C++ 的互操作性不是单向的。尽管在 TVM 中 C++ 完成繁重的内部执行工作，Python 完成用户接口， TVM 中也存在 C++ 调用 Python 的情况：For example, the convolution operator is implemented in Python, and its implementation is invoked from C++ code in Relay.（Relay 中的 C++ 调用 Python 实现的卷积算子）&lt;/p&gt;
&lt;h1 id=&#34;vector-add-example&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#vector-add-example&#34;&gt;#&lt;/a&gt; Vector Add Example&lt;/h1&gt;
&lt;p&gt;使用 vector add 的例子来查看底层 TVM API.&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;n &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;1024&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;A &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;placeholder&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#39;A&#39;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;B &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;placeholder&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#39;B&#39;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;C &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;compute&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;shape&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;lambda&lt;/span&gt; i&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt; A&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;C&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这里面 A、B、C 都是  &lt;code&gt;tvm.tensor.Tensor&lt;/code&gt;   其 Python 定义位于 &lt;code&gt;python/tvm/te/tensor.py&lt;/code&gt; . 支撑的 C++ 定义位于 &lt;code&gt;include/tvm/te/tensor.h&lt;/code&gt;  和 &lt;code&gt;src/te/tensor.cc&lt;/code&gt;  所有的 Python 类型定义都能找到对应的相同名字的 C++ 定义。&lt;/p&gt;
&lt;p&gt;Python 对 C++ 的包装位于  &lt;code&gt;python/tvm/_ffi/&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;一个 Tensor 包含一个 Operation 类，定义于 python/tvm/te/tensor.py，对应的 C++ 实现位于 &lt;code&gt;include/tvm/te/operation.h&lt;/code&gt;  和 &lt;code&gt;src/tvm/te/operation&lt;/code&gt;  。 &lt;code&gt;Tensor&lt;/code&gt;  是  &lt;code&gt;Operation&lt;/code&gt;  类的输出。&lt;/p&gt;
&lt;p&gt;我们将输出张量 C 对应的操作传递给 &lt;code&gt;tvm.te.create_schedule()&lt;/code&gt;  函数 （来自于 &lt;code&gt;python/tvm/te/schedule.py&lt;/code&gt; 。）&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;create_schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;op&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这个函数映射到 C++ 函数 &lt;code&gt;include/tvm/schedule.h&lt;/code&gt; 。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;inline Schedule create_schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;Array&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;Operation&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt; ops&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;return&lt;/span&gt; Schedule&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;ops&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;Schedule&lt;/code&gt;  包含 &lt;code&gt;Stage&lt;/code&gt;  输出  &lt;code&gt;Operation&lt;/code&gt;  的集合。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Stage&lt;/code&gt;  对应于一个操作 &lt;code&gt;Operation&lt;/code&gt; 。上面的 vector add 操作中有两个 placeholder ops 和一个 compute op. 所以 &lt;code&gt;Schedule s&lt;/code&gt;  有三个状态  &lt;code&gt;Stage&lt;/code&gt; ，每个 &lt;code&gt;Stage&lt;/code&gt;  持有以下信息： 循环嵌套结构、每个循环的类型（ &lt;code&gt;Parallel，Vectorized，Unrolled&lt;/code&gt; ）、以及在下一个循环嵌套 &lt;code&gt;Stage&lt;/code&gt;  中在哪里执行它自己的计算。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Schedule&lt;/code&gt;  和 &lt;code&gt;Stage&lt;/code&gt;  本身定义在 &lt;code&gt;tvm/python/te/schedule.py&lt;/code&gt; ，  &lt;code&gt;include/tvm/te/schedule.h&lt;/code&gt; ， 和 &lt;code&gt;src/te/schedule/schedule_ops.cc&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;为简单起见，我们使用 &lt;code&gt;tvm.build(...)&lt;/code&gt;  处理上方 &lt;code&gt;create_schedule()&lt;/code&gt;  函数创建的默认 &lt;code&gt;Schedule s&lt;/code&gt;  和 &amp;lt;em&amp;gt;。我们必须添加必要的线程绑定，来使得其能在 GPU 上运行：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;target &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token string&#34;&gt;&#34;cuda&#34;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;bx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tx &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;split&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;op&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;axis&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; factor&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;bind&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;bx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;thread_axis&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;blockIdx.x&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;s&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;bind&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;tx&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;te&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;thread_axis&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;threadIdx.x&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;fadd &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;build&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;s&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; C&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; target&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;tvm.build(...)&lt;/code&gt; ，定义在 &lt;code&gt;python/tvm/driver/build_module.py&lt;/code&gt; ， 需要输入一个 &lt;code&gt;Schedule&lt;/code&gt; ;  &lt;code&gt;input&lt;/code&gt; , &lt;code&gt;output Tensor&lt;/code&gt; ; 以及一个 &lt;code&gt;target&lt;/code&gt; 。返回一个 &lt;code&gt;tvm.runtime.Module&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;整个 &lt;code&gt;tvm.build(...)&lt;/code&gt;  过程可以分成两步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;i. 降级 高级的、初始的循环嵌套结构被转换为 最终的、低级的 IR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ii. 代码生成 low level IR 生成目标机器码&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;降级是通过 &lt;code&gt;tvm.lower()&lt;/code&gt;  函数完成的，它定义在 &lt;code&gt;python/tvm/build\_module.py&lt;/code&gt; 。第一，指定绑定推理，一个最初的循环嵌套结构就创建好了。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          name&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;default_function&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          binds&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token boolean&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;          simple_mode&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token boolean&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   bounds &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; schedule&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;InferBound&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; schedule&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;ScheduleOps&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;sch&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; bounds&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;   &lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;边界推断是推断所有循环边界和中间缓冲区大小的过程。如果你的目标是 CUDA，且你用了 share memory，它需要的最小 size 在此处确定。绑定推理时在 &lt;code&gt;src/te/schedule/bound.cc，src/te/schedule/graph.cc &lt;/code&gt;  和  &lt;code&gt;src/te/schedule/message\_passing.cc&lt;/code&gt;  中实现的。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;stmt&lt;/code&gt; ， &lt;code&gt;ScheduleOps()&lt;/code&gt;  的输出，表示一个初识的循环嵌套结构。如果在 schedule 中已经应用了 &lt;code&gt;reorder&lt;/code&gt;  和 &lt;code&gt;split&lt;/code&gt;  原语，那么初始的循环嵌套结构已经反映了这些变化。 &lt;code&gt;ScheduleOps()&lt;/code&gt;  定义在 &lt;code&gt;rc/te/schedule/schedule_ops.cc&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;接下来应用一些 lowering passes to  &lt;code&gt;stmt&lt;/code&gt;  . 这些 passes 在 &lt;code&gt;src/tir/pass&lt;/code&gt;  子文件夹下实现。举个例子，如果在你的 &lt;code&gt;schedule&lt;/code&gt;  中应用了 &lt;code&gt;vectorize&lt;/code&gt;  或者 &lt;code&gt;unroll&lt;/code&gt;  原语，他们会被应用到循环 vectorization 和 unrolling passes。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; ir_pass&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;VectorizeLoop&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;stmt&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;stmt &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; ir_pass&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;UnrollLoop&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    stmt&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_step&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_depth&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;auto_unroll_max_extent&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    cfg&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;unroll_explicit&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;在降级 lowering 结束后， &lt;code&gt;build()&lt;/code&gt;  函数生成目标机器代码。如果你的设备是 X86, 这个代码可能包含 SSE 或者 AVX 指令；如果是 CUDA 设备，将包含 PTX 指令。 此外，除了目标特定的机器代码之外，TVM 还生成负责内存管理、内核启动等的主机端代码。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;build\_module()&lt;/code&gt;  函数完成代码生成，定义在 &lt;code&gt;python/tvm/target/codegen.py&lt;/code&gt; 。在 C++ 端代码生成定义在 &lt;code&gt;src/target/codegen&lt;/code&gt; 。 &lt;code&gt;build\_module()&lt;/code&gt;  Python 函数会搜索在 &lt;code&gt;src/target/codegen/codegen.cc&lt;/code&gt;  中的 &lt;code&gt;build()&lt;/code&gt;  函数。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;build()&lt;/code&gt;  函数 &lt;code&gt;PackedFunc&lt;/code&gt;  注册表中为目标设备查找代码生成器，并调用找到的函数。例如， &lt;code&gt;codegen.build\_cuda&lt;/code&gt;  函数注册在 &lt;code&gt;src/codegen/build_cuda_on.cc&lt;/code&gt; ，就像这样：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;TVM_REGISTER_GLOBAL&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34;codegen.build_cuda&#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;set_body&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;TVMArgs args&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; TVMRetValue&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; rv&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;rv &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; BuildCUDA&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;args&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;上方的 &lt;code&gt;BuildCUDA()&lt;/code&gt;  函数使用定义在 &lt;code&gt;src/codegen/codegen_cuda.cc&lt;/code&gt;  的 &lt;code&gt;CodeGenCUDA&lt;/code&gt;  类，从 lowered IR 生成 CUDA kernel source，并使用 NVRTC 编译 kernel。如果你的目标设备使用 LLVM，包括 X86、ARM、NVPTX 和 AMDGPU，代码可由定义在 &lt;code&gt;src/codegen/llvm/codegen_llvm.cc&lt;/code&gt;  的 &lt;code&gt;CodeGenLLVM&lt;/code&gt;  来生成。 &lt;code&gt;CodeGenLLVM&lt;/code&gt;  将 TVM IR 转换成 LLVM IR，运行一些 LLVM 优化 passes，以及生成目标机器码。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;src/codegen/codegen.cc&lt;/code&gt;  中的 &lt;code&gt;Build()&lt;/code&gt;  函数会返回一个 &lt;code&gt;runtime::Module&lt;/code&gt;  类，它定义在 &lt;code&gt;include/tvm/runtime/module.h&lt;/code&gt;  和 &lt;code&gt;src/runtime/module.cc&lt;/code&gt; 。一个 &lt;code&gt;Module&lt;/code&gt;  类是一个潜在目标 设备的特定 &lt;code&gt;ModuleNode&lt;/code&gt;  的容器。&lt;/p&gt;
&lt;p&gt;每个后端都实现一个 &lt;code&gt;ModuleNode&lt;/code&gt;  的子类，来添加目标特定的 runtime API 调用。 例如，CUDA 后端在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  实现 &lt;code&gt;CUDAModuleNode&lt;/code&gt;  类，来管理 CUDA 驱动 API。上方的 &lt;code&gt;BuildCUDA()&lt;/code&gt;  函数用 &lt;code&gt;runtime::Module&lt;/code&gt;  包装了 &lt;code&gt;CUDAModuleNode&lt;/code&gt; ，并包装到 Python 端。LLVM 后端在 &lt;code&gt;src/codegen/llvm/llvm_module.cc&lt;/code&gt;  实现了 &lt;code&gt;LLVMModuleNode&lt;/code&gt; ，处理了 JIT 执行和编译代码。其他对应各个后端的 &lt;code&gt;ModuleNode&lt;/code&gt;  子类可以在 &lt;code&gt;src/runtime&lt;/code&gt;  子文件夹找到。&lt;br&gt;
返回的 &lt;code&gt;module&lt;/code&gt; ，可以被认作编译函数和设备 API 的组合，可以被 TVM 的 NDArray objects 调用。&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;dev &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;device&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;target&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;a &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;random&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;uniform&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;size&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;astype&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;b &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;random&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;uniform&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;size&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;astype&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;B&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;c &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; tvm&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;nd&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;array&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;np&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;zeros&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;n&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dtype&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;dtype&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; dev&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;fadd&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;a&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; b&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; c&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;output &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; c&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;numpy&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;在幕后，TVM 会自动分配设备内存并管理内存传输。为了实现这个目标，每个后端都需要继承在 &lt;code&gt;include/tvm/runtime/device_api.h&lt;/code&gt;  定义的 &lt;code&gt;DeviceAPI&lt;/code&gt;  类，使用设备特定的 API 重写里面的内存管理方法。例如，CUDA 后端在 &lt;code&gt;src/runtime/cuda/cuda_device_api.cc&lt;/code&gt;  使用 &lt;code&gt;cudaMalloc&lt;/code&gt; 、 &lt;code&gt;cudaMemcpy&lt;/code&gt;  实现了 &lt;code&gt;CUDADeviceAPI&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;第一次使用 &lt;code&gt;fadd(a, b, c)&lt;/code&gt;  调用编译后的模块时，会调用  &lt;code&gt;ModuleNode&lt;/code&gt;  的  &lt;code&gt;GetFunction()&lt;/code&gt;  方法来获取可用于内核调用的  &lt;code&gt;PackedFunc&lt;/code&gt; 。例如，在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  CUDA 后端实现了 &lt;code&gt;CUDAModuleNode::GetFunction()&lt;/code&gt;  函数如下：&lt;/p&gt;
&lt;figure class=&#34;highlight python&#34;&gt;&lt;figcaption data-lang=&#34;python&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;PackedFunc CUDAModuleNode&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;GetFunction&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      const std&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;string&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; name&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;      const std&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;:&lt;/span&gt;shared_ptr&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;ModuleNode&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; sptr_to_self&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  auto it &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; fmap_&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;find&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;name&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  const FunctionInfo&lt;span class=&#34;token operator&#34;&gt;&amp;amp;&lt;/span&gt; info &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; it&lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;second&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  CUDAWrappedFunc f&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  f&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;Init&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;this&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; sptr_to_self&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; name&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;arg_types&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;size&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;launch_param_tags&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;  &lt;span class=&#34;token keyword&#34;&gt;return&lt;/span&gt; PackFuncVoidAddr&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;f&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; info&lt;span class=&#34;token punctuation&#34;&gt;.&lt;/span&gt;arg_types&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;PackedFunc&lt;/code&gt;  的重载函数 &lt;code&gt;operator()&lt;/code&gt;  会被调用。从而会调用定义在 &lt;code&gt;src/runtime/cuda/cuda_module.cc&lt;/code&gt;  的 &lt;code&gt;CUDAWrappedFunc&lt;/code&gt;  的 &lt;code&gt;operator()&lt;/code&gt;  函数，最终我们会看到 &lt;code&gt;cuLaunchKernel&lt;/code&gt;  驱动会调用：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class CUDAWrappedFunc &amp;#123;
 public:
  void Init(...)
  ...
  void operator()(TVMArgs args,
                  TVMRetValue* rv,
                  void** void_args) const &amp;#123;
    int device_id;
    CUDA_CALL(cudaGetDevice(&amp;amp;device_id));
    if (fcache_[device_id] == nullptr) &amp;#123;
      fcache_[device_id] = m_-&amp;gt;GetFunc(device_id, func_name_);
    &amp;#125;
    CUstream strm = static_cast&amp;lt;CUstream&amp;gt;(CUDAThreadEntry::ThreadLocal()-&amp;gt;stream);
    ThreadWorkLoad wl = launch_param_config_.Extract(args);
    CUresult result = cuLaunchKernel(
        fcache_[device_id],
        wl.grid_dim(0),
        wl.grid_dim(1),
        wl.grid_dim(2),
        wl.block_dim(0),
        wl.block_dim(1),
        wl.block_dim(2),
        0, strm, void_args, 0);
  &amp;#125;
&amp;#125;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;本文概括了 TVM 如何编译和执行函数。 虽然本文没有详细说明 TOPI 或 Relay，但最终所有神经网络算子都会经历与上述相同的编译过程。&lt;/p&gt;
</content>
        <category term="accelerate" />
        <category term="conv" />
        <category term="tvm" />
        <updated>2024-05-24T14:49:36.319Z</updated>
    </entry>
    <entry>
        <id>https://forcheetah.github.io/2024/05/24/category/</id>
        <title>博客汇总目录</title>
        <link rel="alternate" href="https://forcheetah.github.io/2024/05/24/category/"/>
        <content type="html">&lt;h1 id=&#34;暮冬z羡慕-的博客-文章汇总&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#暮冬z羡慕-的博客-文章汇总&#34;&gt;#&lt;/a&gt; 暮冬 Z 羡慕 的博客  文章汇总&lt;/h1&gt;
&lt;h1 id=&#34;卷积加速算法&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#卷积加速算法&#34;&gt;#&lt;/a&gt; 卷积加速算法&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://forcheetah.github.io/2024/05/23/conv1/&#34;&gt;【Im2Col】卷积加速算法【1】 NCHW&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://forcheetah.github.io/2024/05/23/conv2/&#34;&gt;【Im2Col】卷积加速算法【2】NHWC&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ai推理引擎&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ai推理引擎&#34;&gt;#&lt;/a&gt; AI 推理引擎&lt;/h1&gt;
&lt;h1 id=&#34;ai编译优化&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ai编译优化&#34;&gt;#&lt;/a&gt; AI 编译优化&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://forcheetah.github.io/2024/05/24/tvm1/&#34;&gt;【TVM】根据例子走通代码库&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;问题解决&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#问题解决&#34;&gt;#&lt;/a&gt; 问题解决&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://forcheetah.github.io/2024/05/15/openBlas/&#34;&gt;openBlas 库的安装与简单使用&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://forcheetah.github.io/2024/05/14/cpplib/&#34;&gt;C 语言工程调用 Cpp 库解决方案&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;杂谈&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#杂谈&#34;&gt;#&lt;/a&gt; 杂谈&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;持续更新中 ...&lt;/p&gt;
&lt;/blockquote&gt;
</content>
        <category term="accelerate" />
        <category term="conv" />
        <updated>2024-05-24T13:25:53.973Z</updated>
    </entry>
    <entry>
        <id>https://forcheetah.github.io/2024/05/23/conv2/</id>
        <title>【Im2Col】卷积加速算法【2】NHWC</title>
        <link rel="alternate" href="https://forcheetah.github.io/2024/05/23/conv2/"/>
        <content type="html">&lt;p&gt;本文为最基本的 Im2Col 算法的原理及实现。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://forcheetah.github.io/2024/05/23/conv1/&#34;&gt;【Im2Col】卷积加速算法 NHWC&lt;/a&gt; 【1】中已经讲了在输入和输出都是 nchw 排布下 Im2Col 算法的实现方式。常见的 tensor 输入有 NCHW 和 NHWC 两种内存排布方式，不同的排布方式各有优劣。排布方式不同，Im2Col 也有区别，本篇主要是在 NHWC 内存排布情况下的 Im2Col 算法原理和基本实现。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;慌乱的时候全是破绽，冷静下来，能够找到对方的破绽。&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;两种内存排布&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#两种内存排布&#34;&gt;#&lt;/a&gt; 两种内存排布&lt;/h1&gt;
&lt;p&gt;卷积神经网络（CNN）的输入数据布局主要有两种标准：NCHW（通道、高度、宽度）和 NHWC（高度、宽度、通道）。主要深度学习框架对这两种布局的支持情况如下：&lt;/p&gt;
&lt;p&gt;PyTorch：主要采用 NCHW 格式。这是 PyTorch 在大多数情况下的默认布局，尤其是在涉及 GPU 计算时。&lt;/p&gt;
&lt;p&gt;Caffe：采用 NCHW 格式。Caffe 框架倾向于使用这种通道优先的布局。&lt;/p&gt;
&lt;p&gt;TensorFlow：默认使用 NHWC 格式，特别是在早期版本中，这是由于 TensorFlow 最初设计时主要针对 CPU 进行优化，NHWC 布局在这种场景下有更好的内存访问局部性。&lt;br&gt;
Keras：Keras 本身是一个高级 API，集成到 Tensorflow 之后跟随 tensorflow 的内存排布方式。&lt;/p&gt;
&lt;h1 id=&#34;内存排布的优缺点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#内存排布的优缺点&#34;&gt;#&lt;/a&gt; 内存排布的优缺点&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;NCHW 格式的优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 在 GPU 中计算卷积时，比 NHWC 要快 2.5 倍左右。这是因为在 GPU 中，NCHW 格式的数据布局更符合 GPU 的内存访问模式和计算方式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.NCHW 格式更适合那些需要对每个通道单独做运算的操作，比如 “MaxPooling”。这是因为 NCHW 格式的同一通道的像素值连续排布，使得对每个通道的数据可以更高效地进行运算。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NCHW 格式的缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1.NCHW 格式需要把所有通道的数据都读取到，才能进行运算，因此在计算时需要的存储更多。这可能会限制其在一些具有限制的硬件环境下的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.NCHW 格式的访存与计算的控制逻辑相对简单，这使得在一些需要精细控制访存和计算的场景下，可能不是最佳的选择。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NHWC 格式的优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1.NHWC 格式的访存局部性更好。这意味着每三个输入像素就可以得到一个输出像素，因此在一些特定的计算操作中，可以更高效地利用硬件资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.NHWC 格式更适合那些需要对不同通道的同一像素做某种运算的操作，比如 “Conv1x1”。这是因为 NHWC 格式的不同通道中的同一位置元素顺序存储，使得对不同通道的数据可以进行更高效的运算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.NHWC 格式在早期的 CPU 开发中应用较多，因此对于主要基于 CPU 开发的深度学习框架和算法，NHWC 格式可能更受欢迎。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NHWC 格式的缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 在使用 GPU 进行计算加速时，NHWC 格式不如 NCHW 格式高效。这是因为 NCHW 格式更符合 GPU 的内存访问模式和计算方式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 对于一些需要精细控制访存和计算的场景，NHWC 格式的控制逻辑可能相对复杂一些。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;im2col变换&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#im2col变换&#34;&gt;#&lt;/a&gt; Im2Col 变换&lt;/h1&gt;
&lt;p&gt;Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 input_2D 放在前面，也就是 (input_2D * kernel_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。&lt;/p&gt;
&lt;p&gt;依然采用下面这个简单的卷积样例，输入 tensor 按照 nhwc 排布，所以是一个 3 通道 5*5 的 input tensor。卷积核有 9 个，pad 为 1，因此输出是【1，5，5，9】。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1716464754440.jpg&#34; alt=&#34;卷积样例&#34;&gt;&lt;/p&gt;
&lt;p&gt;直接展开 2D 形式，两个矩阵形式如下。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1716464821580.jpg&#34; alt=&#34;Im2Col 2D示意图&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上图右边是权重 kernel_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序展成一列，也就是第一列绿色部分，共有 27 个数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 依次将余下 8 个 kernel 按照相同的方式展成一列，就得到了 kernel_2D。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.kernel_2D 维度为【27，9】&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从内存排布上来看，需要将原本的 kernel_4D 进行数据重排。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上图左边是输入 input_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 矩阵乘是 行 * 列；kernel_2D 一列代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一行是 “一个卷积核滑动窗口” 对应的数据，也就是 27 个。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 第一行橘黄色部分是第一个滑动窗口对应的数据，未填的数代表 Pad。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3. 滑动窗口需要纵移 5 次，每次纵移需要横移 5 次，因此有 5*5 行数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4. 例如第 5 行蓝色是滑动窗口在图一中向右移动到第 5 格，蓝色格子时对应的数据。由于 Input_2D 的维度是 NHWC，也就是说图一中 “0，25，50” 三个数在内存中是相邻的（分别位于同一个 HW 位置的第一、第二、第三通道）。为了减少数据的搬运，这些连续的数被搬运到一起。即：图一中 3*3 的蓝色滑动窗口中前 3 个都对应 Pad，所以图二第 5 行蓝色行先有 3*3 (channel) 个 Pad 数据。图一蓝色滑动窗口在第 4 和第 5 个格子出现数据，所以图二 9 个 Pad 后面跟着 3，28，53；4，29，54。紧接着又出现了 Pad，以此类推。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一行 27 个数据。input_2D 的维度为【25，27】&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;6. 总结完 input_2D 的数据排布，那么 kernel_2D 每一列的数据具体怎么排也就清楚了，需要和 input_2D 的每一行一一对应。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;输出 output&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;input_2D【25，27】* kernel_2D 【27，9】得到结果 output_2D 【25，9】，刚好是输出 output【1，5，5，9】的内存排布方式。因此输出也不需要额外的内存转换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具体实现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这里没有专门写 Im2Col 在 NHWC 排布情况下的代码，可以在卷积加速算法模拟下载完整的测试代码。并参考 “TestIm2FlavorConvLayer ();” 函数及文章&lt;a href=&#34;https://forcheetah.github.io/2024/05/15/accelerate1/&#34;&gt; im2col 卷积加速算法 NHWC&lt;/a&gt;，完成 NHWC 内存排布情况下的算法。&lt;/p&gt;
&lt;h1 id=&#34;矩阵乘先后顺序的影响&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#矩阵乘先后顺序的影响&#34;&gt;#&lt;/a&gt; 矩阵乘先后顺序的影响&lt;/h1&gt;
&lt;p&gt;以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1716466223424.jpg&#34; alt=&#34;两种排布情况下矩阵乘先后顺序不同对数据变换的影响&#34;&gt;&lt;/p&gt;
&lt;p&gt;已经讲解了简单的 im2col 算法在 NCHW 排布&lt;a href=&#34;https://forcheetah.github.io/2024/05/15/accelerate1/&#34;&gt;上一篇文章&lt;/a&gt;和 NHWC 排布情况下的 2D 内存排布情况。上图总结了两种排布情况下 矩阵乘先后顺序不同对数据变换的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NCHW 排布  kernel 在前  只需要对 input 做 im2col 变换&lt;/li&gt;
&lt;li&gt;NCHW 排布  input 在前  多出来对权重的转置变换 和输出的转置变换&lt;/li&gt;
&lt;li&gt;NHWC 排布  kernel 在前  需要对权重数据进行重排 以及输出进行转置&lt;/li&gt;
&lt;li&gt;NHWC 排布  input 在前  需要对权重进行转置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。&lt;/p&gt;
</content>
        <category term="accelerate" />
        <category term="conv" />
        <updated>2024-05-23T12:12:27.275Z</updated>
    </entry>
    <entry>
        <id>https://forcheetah.github.io/2024/05/23/conv1/</id>
        <title>【Im2Col】卷积加速算法【1】 NCHW</title>
        <link rel="alternate" href="https://forcheetah.github.io/2024/05/23/conv1/"/>
        <content type="html">&lt;p&gt;本文为最基本的 Im2Col 算法的原理及实现。&lt;br&gt;
加速算法道阻且长，想要选择最优的算法，需要通盘考虑现实需求、软件算法、硬件支持，这就是 “坚持理论联系实际”。 所以这里只是对 Im2Col 算法最基本的原理探讨。&lt;br&gt;
探索本就是由正确和错误交织而成，还望各位不吝赐教！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;现在想起来，光是遇到你这个家伙，就感觉自己赚到了。&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;卷积算法&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#卷积算法&#34;&gt;#&lt;/a&gt; 卷积算法&lt;/h1&gt;
&lt;p&gt;将 Im2col 算法之前，不得不再提一下卷积。卷积是一种运算，在神经网络中是提取特征的过程，具体的操作过程是在输入特征中不断滑动卷积核大小的窗口，与卷积核做乘加运算，得到输出结果。&lt;/p&gt;
&lt;p&gt;先定义一下维度的符号： 卷积核 ：【C_out, C_in, Hk, Wk】   输入 ： 【B, C_in, H, W】  输出：【B, C_out, Ho, Wo】&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770544973.jpg&#34; alt=&#34;卷积运算流程&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是维度非常小的一个卷积运算的图示，左边卷积核维度为【9，3，3，3】，即个数为 9，通道数为 3（个数和通道数均未在图中展示出来）的 3*3 卷积核。中间为 Input 的维度，周围方格蓝色代表 tensor 的 pad，中间黄色代表维度为【1，3，5，5】的 Input，即通道为 3，长宽 5*5。&lt;/p&gt;
&lt;p&gt;进行卷积过程中，卷积核（3*3，通道为 3）先横向滑动（5 次），再纵向滑动（5 次）；每到一个位置计算 kernel 与 Input 对应位置的乘积和，因此得到 output 5*5 的结果。9 个卷积核依次进行，得到输出大小【1，9，5，5】&lt;/p&gt;
&lt;p&gt;想要理解卷积乃至实现卷积加速算法，不仅要知道计算过程，还要格外关注数据在内存中的排布顺序。在内存中所有的数据都是一维存储的，例如 Input【1，3，5，5】，在内存中只有一个连续的、大小为 75 的数组，【1，3，5，5】只是它的逻辑维度；最后一个维度的 5 个数字是连续的（0~4），紧接着是下一行（5~9）......&lt;/p&gt;
&lt;p&gt;图中仅画出了平面的大小，通道方向就要靠大家的想象了，上图中每个格子后面还有 2 个格子（通道数为 3）。&lt;br&gt;
简单的卷积更容易看清 Im2col 算法的流程，本文以及后续系列文章都将以这个例子进行。&lt;/p&gt;
&lt;h1 id=&#34;im2col算法&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#im2col算法&#34;&gt;#&lt;/a&gt; Im2Col 算法&lt;/h1&gt;
&lt;p&gt;为什么要对卷积算法进行加速呢？&lt;/p&gt;
&lt;figure class=&#34;highlight c&#34;&gt;&lt;figcaption data-lang=&#34;c&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; batch &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; batch&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;in_n&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; batch&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token keyword&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;s &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; out_c&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; out_row &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; out_row &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; out_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;out_row&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; out_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; out_col &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; out_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;out_col&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; imap &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; imap &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; in_c&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;imap&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kr &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kr &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; ker_size&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;kr&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kc &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kc &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; ker_size&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;kc&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                                dosomething&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;上方是个典型的卷积算法，要实现一个卷积，不仅要遍历 batch、C_out、Ho、Wo，还要遍历整个卷积核的维度，整个循环达到了 7 层之多。当输入数据维度变大时，整个卷积占用的资源让人难以接受。提高神经网络的推理速度是推理引擎和 AI 芯片设计者不断的追求，而卷积又占了神经网络推理的大部分时间，因此卷积的加速是重中之重。&lt;br&gt;
Im2Col 算法的原理就是将卷积运算&lt;br&gt;
转化为矩阵运算，这一转换带来以下好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;易于优化：由于 GEMM 操作在计算库中被广泛研究和优化，开发者可以利用这些库的最新进展提高效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;并行化和硬件支持：矩阵乘法天然适合并行处理；大多硬件平台都会充分利用其硬件资源，对矩阵乘法进行深度优化。通过使用如 CUDA、cuDNN、OpenBLAS 等库，可以实现并行计算，极大地加速卷积运算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;内存管理灵活性：虽然 im2col 需要额外的内存来存储展开后的矩阵，但通过调整实现策略（如分块处理），可以在内存使用和计算效率之间找到平衡。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总之，尽管 Im2col 算法没有减少任何计算量，甚至还给内存管理带来挑战，但是矩阵的高效运算、硬件、并行等仍然能够在很多场景下提高卷积计算效率。&lt;/p&gt;
&lt;h1 id=&#34;im2col变换&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#im2col变换&#34;&gt;#&lt;/a&gt; Im2Col 变换&lt;/h1&gt;
&lt;p&gt;Im2col 算法是将卷积转换成两个 2D 矩阵乘运算。(kernel_2D * input_2D)  与 (input_2D * kernel_2D) 矩阵乘的顺序不同，所需要的 4D 到 2D 变换也不同，我们先将 kernel_2D 放在前面，也就是 (kernel_2D * input_2D)，最后再讨论 kernel_2D 放在前面还是后面的区别。&lt;/p&gt;
&lt;p&gt;再看一遍下面这个简单的卷积样例，输入 tensor 按照 nchw 排布。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770588007.jpg&#34; alt=&#34;卷积样例&#34;&gt;&lt;/p&gt;
&lt;p&gt;直接展开 2D 形式，两个矩阵形式如下。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770625347.jpg&#34; alt=&#34;Im2Col 2D示意图&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图 “Im2Col 2D 示意图” 左边是 kernel_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 原 kernel_4D 维度为【9，3，3，3】，拥有 9 个 kernel，将其第一个 kernel 按照【C_in, Kh, Kw】的顺序完全展平，也就是第一行绿色部分，共有 28 个数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 依次将余下 8 个 kernel 按照相同的方式展平，就得到了 kernel_2D。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.kernel_2D 维度为【9，27】&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;惊喜地发现，kernel_2D 的内存排布和 kernel_4D 完全一致，不需要任何内存搬运！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图 “Im2Col 2D 示意图” 右边是 Input_2D&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1. 矩阵乘是 行 * 列；kernel_2D 中，一行代表一个卷积核【C_in, Kh, Kw】；所以 input_2D 的一列是 “一个卷积核滑动窗口” 对应的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2. 右图中，前半部分空数据代表 pad,  后面的大面积空白只是懒得填上数字。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3. 例如，右图中蓝色的一列数据，代表图 2 中，卷积核滑动到蓝色窗口时对应的 input 数据。即第一个 channel：(pad,pad,pad;3,4,pad;8,9,pad)； 第二个 channel：(pad,pad,pad;28,29,pad;33,34,pad)；第三个 channel：(pad,pad,pad;53,54,pad;58,59,pad)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4. 那么 input_2D 一行数据代表什么呢？代表 kernel 在 input 中窗口的横向移动和纵向移动。在本例子中，窗口需要纵向滑动 5 次，每次纵向滑动都要横向滑动 5 次，一共产生 25 次窗口滑动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5. 总结：25 次窗口滑动，每次滑动形成窗口对应的一列 27 个数据。input_2D 的维度为【27，25】&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;输出 output&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;kernel_2D 【9，27】* input_2D【27，25】得到结果 output_2D 【9，25】，刚好是输出 output【1，9，5，5】的内存排布方式。因此输出也不需要额外的内存转换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具体实现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这里是将 input 由 nchw 转为 2D 排布的代码。如果感兴趣，可以在&lt;a href=&#34;https://github.com/ForCheetah/ConvAccelerate&#34;&gt;卷积加速算法模拟&lt;/a&gt; 下载完整的测试代码。并通过 “TestIm2FlavorConvLayer ();” 函数进行测试。同时函数 “TestIm2ColConvIMW” 是 NCHW 排布下 Input_2D 在前，kernel_2D 在后的 Im2Col 算法实现。&lt;/p&gt;
&lt;figure class=&#34;highlight cpp&#34;&gt;&lt;figcaption data-lang=&#34;C++&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;Im2Col&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;data_im&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; channels&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; height&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; width&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_h&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_w&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; pad_h&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; pad_w&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; stride_h&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; stride_w&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;data_col&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_h &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;height &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; pad_h &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt; kernel_h &lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;/&lt;/span&gt; stride_h &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_w &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;width &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; pad_w &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt; kernel_w&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;/&lt;/span&gt; stride_w &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; channel_size &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; height &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; width&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; &lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; channel &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; channel &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; channels&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; channel&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; data_im &lt;span class=&#34;token operator&#34;&gt;+=&lt;/span&gt; channel_size&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_row &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_row &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; kernel_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_row&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; kernel_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_col &lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt; kernel_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; kernel_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; input_row &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;pad_h &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; kernel_row&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_rows &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_rows&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;output_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_rows&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;is_a_ge_zero_and_a_lt_b&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;input_row&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; height&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_cols &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_cols&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;output_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_cols&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;data_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;16&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; input_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;-&lt;/span&gt;pad_w &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; kernel_col&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;17&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; output_col &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_col&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;output_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt; output_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;18&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token function&#34;&gt;is_a_ge_zero_and_a_lt_b&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;input_col&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; width&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;19&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                                &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;data_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; data_im&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;input_row &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt; width &lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt; input_col&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;20&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;21&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                                &lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;data_col&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;22&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;23&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                            input_col &lt;span class=&#34;token operator&#34;&gt;+=&lt;/span&gt; stride_w&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;24&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;25&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;26&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    input_row &lt;span class=&#34;token operator&#34;&gt;+=&lt;/span&gt; stride_h&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;27&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;28&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;29&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;30&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;31&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&#34;矩阵乘先后顺序的影响&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#矩阵乘先后顺序的影响&#34;&gt;#&lt;/a&gt; 矩阵乘先后顺序的影响&lt;/h1&gt;
&lt;p&gt;以严格按照矩阵标准的 行 * 列 运算为前提，权重和输入的先后顺序会影响其内存排布。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770640439.jpg&#34; alt=&#34;Kernel_2D * Input_2D&#34;&gt;&lt;br&gt;
 在输入为 nchw 排布，输出也是 nchw 排布情况下，kernel_2D 在前，Input_2D 在后，只需要对 Input 进行 Im2Col 变换。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1715770657999.jpg&#34; alt=&#34;Input_2D * Kernel_2D&#34;&gt;&lt;br&gt;
 在输入为 nchw 排布，输出也是 nchw 排布情况下，Input_2D 在前，kernel_2D 在后，多出了对 kernel 和 output 的转置操作。&lt;/p&gt;
&lt;p&gt;当然，在真正运算矩阵乘时，如果有行优先货列优先的调整，就需要具体问题具体分析了。&lt;/p&gt;
</content>
        <category term="accelerate" />
        <category term="conv" />
        <updated>2024-05-23T11:31:49.363Z</updated>
    </entry>
</feed>

<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Пусть этот камень будет более крепким, чем человек • Posts by &#34;openblas&#34; tag</title>
        <link>https://forcheetah.github.io</link>
        <description>有自己的博客很帅，但是我很懒，要命！！！</description>
        <language>en</language>
        <pubDate>Thu, 27 Jun 2024 23:21:34 +0800</pubDate>
        <lastBuildDate>Thu, 27 Jun 2024 23:21:34 +0800</lastBuildDate>
        <category>bar</category>
        <category>baz</category>
        <category>Linux</category>
        <category>openBlas</category>
        <category>lib</category>
        <category>accelerate</category>
        <category>conv</category>
        <category>tvm</category>
        <category>tengine</category>
        <category>ncnn</category>
        <category>cmake</category>
        <category>runtime</category>
        <category>tank</category>
        <category>zatan</category>
        <item>
            <guid isPermalink="true">https://forcheetah.github.io/2024/06/27/conv3/</guid>
            <title>【im2col】昇腾卷积加速算法</title>
            <link>https://forcheetah.github.io/2024/06/27/conv3/</link>
            <category>Linux</category>
            <category>openBlas</category>
            <pubDate>Thu, 27 Jun 2024 23:21:34 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;前置信息&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#前置信息&#34;&gt;#&lt;/a&gt; 前置信息&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;（1）本文讲解使用的例子&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以如下的卷积为例，进行昇腾 Im2Col 卷积过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input 输入维度为 NHWC ：【2，25，25，17】&lt;/li&gt;
&lt;li&gt;外圈蓝色代表 pad&lt;/li&gt;
&lt;li&gt;Kernal 维度为  CCHkWk  ：【34，17，3，3】&lt;/li&gt;
&lt;li&gt;操作为 3*3 卷积 pad=1, Group=1, Stride=1， 2D 卷积&lt;/li&gt;
&lt;li&gt;得到输出的维度 为 NHWC : 【22，25，25，18】&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从图上可以轻易看出相关信息。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500181052.jpg&#34; alt=&#34;例子&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;现在想起来，光是遇到你这个家伙，就感觉自己赚到了。&lt;br&gt;
------   大家好啊    我是   暮冬 Z 羡慕&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;（2）矩阵乘运算单元&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;昇腾达芬奇架构设计了 16*16 的矩阵乘运算单元，能够提供强大的并行乘加计算能力，可以以一条指令实现两个 16*16 的矩阵相乘的运算。所以昇腾 Im2Col 卷积的目的就是让卷积能够高效地利用 “矩阵乘运算单元” 进行计算。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500301302.jpg&#34; alt=&#34;davincii&#34;&gt;&lt;/p&gt;
&lt;p&gt;感兴趣的可以阅读昇腾架构介绍书籍。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;矩阵计算单元可以⽤⼀条指令完成两个 16×16 矩阵的相乘运算（标记为 16&lt;sup&gt;3，也是 Cube 这⼀名称的来历），等同于在极短时间内进⾏了 16&lt;/sup&gt;3＝4096 个乘加运算，并且可以实现 FP16 的运算精度。如图 3-7 所⽰，矩阵计算单元在完成 C＝A×B 的矩阵运算时，会事先将矩阵 A 按⾏存放在输⼊缓冲区中，同时将矩阵 B 按列存放在输⼊缓冲区中，通过矩阵计算单元计算后得到的结果矩阵 C 按⾏存放在输出缓冲区中。在矩阵相乘运算中，矩阵 C 的第⼀元素由矩阵 A 的第⼀⾏的 16 个元素和矩阵 B 的第⼀列的 16 个元素由矩阵计算单元⼦电路进⾏ 16 次乘法和 15 次加法运算得出。矩阵计算单元中共有 256 个矩阵计算⼦电路，可以由⼀条指令并⾏完成矩阵 C 的 256 个元素计算。                                          摘自《昇腾 AI 处理器架构与编程》&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;权重排布&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#权重排布&#34;&gt;#&lt;/a&gt; 权重排布&lt;/h1&gt;
&lt;p&gt;昇腾 Im2Col 五维卷积加速算法   基本流程：&lt;/p&gt;
&lt;p&gt;输入为 nhwc 输出为 nhwc&lt;/p&gt;
&lt;p&gt;权重维度变化： 权重的维度变化离线进行，不消耗神经网络推理时间。（神经网络推理大致分为 模型转换 量化 推理三个步骤，权重的维度转换可以在模型转换时进行，不占用推理的时间）。下面是权重变换的分步流程，代码实现可以一步完成，也可以分多步完成（因为不影响推理时间。）&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500423112.jpg&#34; alt=&#34;weight change&#34;&gt;&lt;/p&gt;
&lt;p&gt;上方的变换如果比较抽象的话，可以结合后面的流程来理解。&lt;/p&gt;
&lt;h2 id=&#34;权重-从kernel-4d变换到kernel-2d&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#权重-从kernel-4d变换到kernel-2d&#34;&gt;#&lt;/a&gt; 权重 从 kernel 4D 变换到 kernel 2D&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500512598.jpg&#34; alt=&#34;weight change&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500556415.jpg&#34; alt=&#34;weight change2&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是 Kernel 2D 的数据排布方式，维度为【2*3*3*16，34】，为了简便，跳过昇腾 5D 结构，直接从 4D 转到 2D。下面介绍 4D 数据和 2D 数据的一一对应关系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;D 图 ① 覆盖区域表示 一个卷积核【17，3，3】展开成 2D 中的一列。对应于 A 图中一整个卷积核。34 个卷积核将展开为 34 列。因此每列代表一个卷积核。&lt;/li&gt;
&lt;li&gt;B 图，卷积核通道数为 17，需要补零为 16 的倍数 32，并拆分成 2 块（分别是紫色、黄色）。E 图：每一列（每一个卷积核）的紫色部分②是卷积核通道方向拆分的第一块（B 图中的紫色），黄色部分③是拆分的第二块（B 图中的黄色）。&lt;/li&gt;
&lt;li&gt;拆分的每一块（比如紫色部分）又分成 3*3（kernel 行 * 列），F 图: ④覆盖的是 kernel 第一行 (对应于 C 图中的④的部分)，⑤覆盖的是 kernel 第二行（对应于 C 图中⑤的部分），相似的⑥覆盖的是 kernel 第三行（对应于 C 图中⑥的部分）。3*3 卷积核一共就三行&lt;/li&gt;
&lt;li&gt;每一个紫色的小方格代表通道方向的 16 个数。&lt;/li&gt;
&lt;li&gt;至此，kernel 4D 和 kernel 2D 所有的数据都一一对应了。例如 F 图中：⑦代表第 6 个卷积核、通道拆分的第二块、第一行、第二列、通道方向的 16 个数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过上述对应关系，我们不难得到维度为【2*3*3*16，34】的卷积核 2D 形式。由于昇腾卷积算法的 AI 计算核心是 16*16 的矩阵乘运算单元，同时为了取数方便，还需要将卷积核 2D 转换为大 Z 小 N 排布方式。&lt;/p&gt;
&lt;h2 id=&#34;权重-从kernel2d变换到大z小n&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#权重-从kernel2d变换到大z小n&#34;&gt;#&lt;/a&gt; 权重 从 kernel2D 变换到大 Z 小 N&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500684744.jpg&#34; alt=&#34;2d&#34;&gt;&lt;/p&gt;
&lt;p&gt;第一步，将 2D【2*3*3*16，34】中 34 补零为 16 的倍数，即 48，得到【2*3*3*16，48】。&lt;/p&gt;
&lt;p&gt;第二步，将其按照 16*16 的方格进行划分，得到【2*3*3，3】个【16，16】的小块。（图中画成了 4 个小块，实际应该是 3 个，示意图，见谅）&lt;/p&gt;
&lt;p&gt;第三步，将这些小块按照大 Z 小 N 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube8 这种 “Z” 字形排布；小 N 指的是内部按照列优先，即每个 16*16 的 Cube，先排第一列，然后是第二列...  详见最右边的彩色表示。&lt;/p&gt;
&lt;p&gt;多说一句，之所以专门按照 “小 N” 排布，是因为在矩阵运算中，权重作为矩阵乘的第二个参数，数据是按列取的。这就意味着在实际内存中要跳着取数（内存中都是按照行优先排序），自然效率低。提前将其按照列优先的方式进行排布，那么在矩阵乘运算中可以连续取数。至此，我们得到了 【2*3*3，3，16*16】的权重大 Z 小 N 排布形式，这种形式使得能够一次性取出 256 个数参与计算，效率很高。&lt;/p&gt;
&lt;p&gt;下面的代码一次性完成了 权重 4D nhwc  到权重大 Z 小 N 排布，仅供参考。还是那句话，权重的变换离线进行，不占用宝贵的推理时间，所以无须关心转换的效率。完整代码可以下载 &lt;a href=&#34;https://github.com/ForCheetah/ConvAccelerate&#34;&gt;加速算法模拟&lt;/a&gt;，并运行其中的  &lt;code&gt;TestAscendConvLayer();&lt;/code&gt;  函数。可以看到三个测试函数，它们的区别在于不同的输入排布方式。&lt;/p&gt;
&lt;figure class=&#34;highlight cpp&#34;&gt;&lt;figcaption data-lang=&#34;C++&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token comment&#34;&gt;//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;// TestAscendConvLayer();&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;// TestAscendConvLayerNCHW();&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;// TestAscendConvLayerNHWC();&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;pre&gt;&lt;code&gt;void WeightTrans_A(const float* filters, const TensorDim weight_dim, Ascend5Dim we_5D_dim, float* we_tran5D, 
            AscendTransform5Dim we_tran5D_dim, int CUBE_row, int CUBE_col)&amp;#123;
    int lastdim4 = we_tran5D_dim.move * we_tran5D_dim.channel * we_tran5D_dim.LW * we_tran5D_dim.cube;
    int lastdim3 = we_tran5D_dim.channel * we_tran5D_dim.LW * we_tran5D_dim.cube;
    int lastdim2 = we_tran5D_dim.LW * we_tran5D_dim.cube;
    int single_filter_num = weight_dim.c * weight_dim.h * weight_dim.w;
    int single_filter_channel = weight_dim.h * weight_dim.w;

    for(int ch_cube=0; ch_cube&amp;lt;we_tran5D_dim.batch; ch_cube++)&amp;#123;  //通道方向块   ch_cube
        int index_1 = ch_cube * lastdim4;
        for(int hk=0; hk&amp;lt;we_tran5D_dim.move; hk++)&amp;#123;  // filter 长  
            int index_2 = index_1 + hk * lastdim3;
            for(int wk=0; wk&amp;lt;we_tran5D_dim.channel; wk++)&amp;#123;  // filter 宽
                int index_3 = index_2 + wk * lastdim2;
                for(int cout_cube=0; cout_cube&amp;lt;we_tran5D_dim.LW; cout_cube++)&amp;#123; // cout方向块 
                    int index_4 = index_3 + cout_cube*we_tran5D_dim.cube;
                    for(int cube_row=0; cube_row&amp;lt;CUBE_row; cube_row++)&amp;#123;
                        for(int cube_col=0; cube_col&amp;lt;CUBE_col; cube_col++)&amp;#123;
                            int index = index_4 + cube_row*CUBE_col + cube_col;                       
                            if((cout_cube*CUBE_col+cube_row)&amp;gt;=weight_dim.n  || (ch_cube*CUBE_col+cube_col)&amp;gt;=weight_dim.c)&amp;#123;
                                we_tran5D[index] = 0;
                            &amp;#125;else&amp;#123;
                                // 第几个filter  第几个通道  第几行  第几列  还要注意 大Z小N排布方式     大Z小N排布方式（行变列，列变行）
                                int index_from = (cout_cube*CUBE_col+cube_row)*single_filter_num + (ch_cube*CUBE_col+cube_col)*single_filter_channel + hk*weight_dim.w+ wk;                                
                                we_tran5D[index] = filters[index_from];
                            &amp;#125;  
                        &amp;#125;
                    &amp;#125;
                &amp;#125;
            &amp;#125;
        &amp;#125;
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;输入排布&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#输入排布&#34;&gt;#&lt;/a&gt; 输入排布&lt;/h1&gt;
&lt;p&gt;输入 tensor 的内存排布为 nhwc 输出为 nhwc&lt;/p&gt;
&lt;p&gt;昇腾算法的维度详细变换如图下图所示。这里展示了输入 input 从 4D 维度转换到 昇腾 5D 结构，然后再转换到 2D 结构，最后转换到大 Z 小 Z 维度。写这么详细只是为了方便读者理解，而在实际操作中，由于 Input 的变换是在线进行，消耗宝贵的推理时间，所以如华为昇腾书中所说：input 先是从 4D 维度 通过软件算法转换为 昇腾 5D 维度（在模型推理过程中这一步可能不需要，因为中间层的 tensor 已经处于昇腾 5D 维度了），之后从昇腾 5D 维度通过 硬件直接转换到大 Z 小 Z 排布（模型推理过程肯定是边转换变计算，所以不会将整个 tensor 转换为大 Z 小 Z 之后，才进行矩阵运算阶段的。本博客为方便，将整个 tensor 完全转换到大 Z 小 Z，再进行后面计算。）&lt;/p&gt;
&lt;p&gt;说完这些，就可以介绍一下昇腾算法极致高效的输入的排布转换过程了！&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500920323.jpg&#34; alt=&#34;input&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;输入-从input-4d-到input-5d&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#输入-从input-4d-到input-5d&#34;&gt;#&lt;/a&gt; 输入 从 Input 4D 到 Input 5D&lt;/h2&gt;
&lt;p&gt;还是再强调一下，昇腾可以做到整个模型的中间层的 tensor 均保持昇腾 5D 的维度，所以思考一下，可能只有最初输入到模型的 tensor 需要 从 Input 4D 转 到 Input 5D，或者再数据预处理的时候就将数据处理为 5D 排布。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719500976081.jpg&#34; alt=&#34;trans6&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G 图是最原始的 Input4D 结构，当然，batch 维度 N=2 没有画，只画了一个。它的维度是【25，25，17】&lt;/li&gt;
&lt;li&gt;H 图为昇腾 5D 结构图，首先要将通道方向的 17 补齐为 16 的倍数 32，同时每 16 个进行一次拆分，拆成两组。&lt;/li&gt;
&lt;li&gt;最后注意一下数据的排布顺序就好了：注意 5D 结构中，K_cube 位于最内层，这些数据是连续的，所以先把 高 h=1, 宽 w=1 位置的 16 个数据排在一起。&lt;/li&gt;
&lt;li&gt;紧接着将宽度方向 25 个 K_cube 排在一起，变成 25*16&lt;/li&gt;
&lt;li&gt;然后再遍历高的方向。变成 25*25*16&lt;/li&gt;
&lt;li&gt;最后是遍历两组，得到昇腾的 5D 结构【2，25，25，16】&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此处数据搬运较为简单，可以参考代码&lt;a href=&#34;https://github.com/ForCheetah/ConvAccelerate&#34;&gt;加速算法模拟&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;输入-从input-5d-直接搬到-大z小z&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#输入-从input-5d-直接搬到-大z小z&#34;&gt;#&lt;/a&gt; 输入 从 Input 5D 直接搬到 大 Z 小 Z&lt;/h2&gt;
&lt;p&gt;昇腾通过专门设计的硬件，将 input 从 5D 格式直接搬到 大 Z 小 Z 排布。想要知道怎么搬以及为什么这么搬，还真不得不把其 2D 排布讲明白。  《昇腾 AI 处理器架构与编程》这本书中直接跳过了 2D 排布，导致晦涩难懂。&lt;/p&gt;
&lt;h3 id=&#34;input-5d-到-input-2d&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#input-5d-到-input-2d&#34;&gt;#&lt;/a&gt; Input 5D 到 Input 2D&lt;/h3&gt;
&lt;p&gt;所以我们直接看 Input2D 与 Weight 2D 的对应情况，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719501086578.jpg&#34; alt=&#34;trans5&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;J 图为 input2D 【25*25，2*3*3*16】   K 图为 Weight2D 【2*3*3*16，34】。再回忆一下 Weight2D 数据每一行和每一列的数据的意义，它的一列数据 2*3*3*16 代表什么呢？  2*3*3*16 代表一整个卷积核，2 代表该卷积核通道方向拆成两块，那么 3*3*16 就是每一块的 高 * 宽 * K_cube。&lt;/li&gt;
&lt;li&gt;好巧！Input2D 的一行也是 2*3*3*16！（废话，不一样就没法算了）。既然 weight2D 一列数据的意义一清二楚，那么对应的 Input2D 数据一行的意义也就呼之欲出啦！ Input2D 的一行 就是卷积核在某个滑动窗口位置对应的 input 数据。例如，Input2D 的第一行，就对应于 I 图 3*3 的彩色窗口数据（没有 Pad 的情况下）。&lt;/li&gt;
&lt;li&gt;也就可以推知，Input2D 的每一行绿色部分，就是 I 图通道方向拆分的第一块（拆分的绿色部分）；每一行的的蓝色部分，就是 I 图通道防线拆分的第二块（中间深蓝宽度 1，和补齐的浅蓝 15）&lt;/li&gt;
&lt;li&gt;那么，为什么 Input2D 有足足 625 行呢？因为滑动窗口纵向滑动 25 次，每次纵向滑动，都包含横向的 25 次，总共 625 次。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假如直接计算 Input2D 矩阵乘 Weight2D，卷积计算就得到最终结果啦！这就是普通的 Im2Col 算法，不清楚的小伙伴们还可以去读一下 &lt;a href=&#34;https://forcheetah.github.io/2024/05/23/conv1/&#34;&gt;Im2Col 算法 NCHW&lt;/a&gt; 和 &lt;a href=&#34;https://forcheetah.github.io/2024/05/23/conv2/&#34;&gt;Im2Col 算法 NHWC&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;从 2D 的角度来看，算法是不是很简单啊。&lt;/p&gt;
&lt;p&gt;不要高兴的太早，还没完呢。&lt;/p&gt;
&lt;h3 id=&#34;input-2d-到-大z小z&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#input-2d-到-大z小z&#34;&gt;#&lt;/a&gt; Input 2D 到 大 Z 小 Z&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719501234541.jpg&#34; alt=&#34;trans4&#34;&gt;&lt;/p&gt;
&lt;p&gt;接下来是将 Input2D 转换到大 Z 小 Z 排布&lt;/p&gt;
&lt;p&gt;第一步，将 Input2D【25*25，2*3*3*16】中 25*25 补零为 16 的倍数，即 640，得到【640，2*3*3*16】  ，如图 L。&lt;/p&gt;
&lt;p&gt;第二步，将其按照 16*16 的方格进行划分，即得到【40，18】个【16，16】的小块，如图 M。&lt;/p&gt;
&lt;p&gt;第三步，将这些小块按照大 Z 小 Z 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube720 这些块按照 “Z” 字形排布；像 N 图上方排成一行；小 Z 指的是内部也按照行优先，即每个 16*16 的 Cube，先排第一行，然后是第二行... 详见 N 图中的颜色表示。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719501296597.jpg&#34; alt=&#34;trans3&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图来自《昇腾 AI 处理器架构与编程》，矩阵 A 的排布为大 Z 小 Z，矩阵 B 的排布为大 Z 小 N，大家可以再理解一下。&lt;/p&gt;
&lt;p&gt;至此，Input 的大 Z 小 Z 排布已经实现，接下来就是 16*16 的矩阵乘了。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719501324297.jpg&#34; alt=&#34;trans2&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input 现在是【40，18】个【16，16】小块，如左图，当然，它现在处于大 Z 小 Z 的一维排布。&lt;/li&gt;
&lt;li&gt;Weight 现在是 【18，3】个【16，16】小块，如中间图，当然，它现在处于大 Z 小 N 的一维排布。&lt;/li&gt;
&lt;li&gt;不知道分块矩阵乘的小伙伴可以再搜索下 《线性代数》中的分块矩阵乘运算。&lt;/li&gt;
&lt;li&gt;内部，进行两个 16*16 块的矩阵乘运算，由于 weight 已经按照列优先进行排布，所以矩阵乘的顺序如上图最右边所示。&lt;/li&gt;
&lt;li&gt;外部，对【40，18】和【18，3】做矩阵乘运算。&lt;/li&gt;
&lt;li&gt;至此，我们得到了【640，18】的矩阵。&lt;/li&gt;
&lt;li&gt;然后将上图两图灰色部分对应的多余数据裁掉，就得到了卷积结果【25，25，34】 ，当然，还得遍历一下 batch，得到【2，25，25，34】&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;input5d搬到大z小z&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#input5d搬到大z小z&#34;&gt;#&lt;/a&gt; Input5D 搬到大 Z 小 Z&lt;/h3&gt;
&lt;p&gt;前两小节介绍了 Input5D 变换到 Input 2D，再变换到 大 Z 小 Z 的过程。而在昇腾芯片中，从 Input5D 到 Input2D 由硬件一步实现。&lt;/p&gt;
&lt;p&gt;如果前面两小节已经看明白了的话，那么搬运的秘密就呼之欲出了。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;1719501381618.jpg&#34; alt=&#34;trans1&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;看上图，左图是 Input 的 5D 维度排布【2，25，25，16】，右边是 Input 2D 排布【25*25，2*3*3*16】。中间是个滑动窗口示意图，3*3，因为本文中用的例子就是 3*3 卷积。&lt;/li&gt;
&lt;li&gt;回忆一下右边 2D 排布的数据的意义，每一个小格子是通道方向的 16 个数，每一行是滑动窗口每一个位置对应的 2*3*3*16 个数。滑动窗口纵向滑动 25 次，每次要横向滑动 25 次，所以有 625 行数据，再加上补齐的 15 行，才达到了 640 行数据。&lt;/li&gt;
&lt;li&gt;那么右图红色 1 的位置是滑动窗口 a 在第一个位置所对应的 16 个数字；红色 2 的位置是滑动窗口 a 横向滑动一次对应的 16 个数字；红色 3 的位置是滑动窗口 a 横向滑动第三次对应的 16 个数字；依次类推，红色 16 的位置是滑动窗口横向滑动第 16 次对应的 16 个数字。这 16 次滑动，滑动窗口的 a 在左图从 1 滑倒 16！&lt;/li&gt;
&lt;li&gt;也就是说，右图红色框的 1-16 与左图 1-16 一一对应！&lt;/li&gt;
&lt;li&gt;再来回忆一下，左图中 1-16 这 16*16 的数据是连续的吗？是！（不清楚的再回去看 Input 的维度变换）&lt;/li&gt;
&lt;li&gt;那么右图中的 1-16 这 16*16 个数据是连续的吗？它是！ 根据大 Z 小 Z 排布，这红色框中 16*16 的数据刚好被分到一个小 Cube 中！&lt;/li&gt;
&lt;li&gt;昇腾能够从 Input5D 中一次性拷贝 256 个数据到大 Z 小 Z 排布！&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;代码模拟&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#代码模拟&#34;&gt;#&lt;/a&gt; 代码模拟&lt;/h1&gt;
&lt;p&gt;当然，我猜测昇腾应该是设计了 16 个 DMA 组成的 DAM 队列，来实现一次 256 个数据的搬运。真的是相当高效了！&lt;/p&gt;
&lt;p&gt;我提供了 C 语言代码模拟整个昇腾的卷积运算流程。完整代码可以在 &lt;a href=&#34;https://github.com/ForCheetah/ConvAccelerate&#34;&gt;加速算法模拟&lt;/a&gt;下载，该工程提供了以下三个测试函数，它们的区别在于不同的输入排布方式。&lt;/p&gt;
&lt;figure class=&#34;highlight cpp&#34;&gt;&lt;figcaption data-lang=&#34;C++&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token comment&#34;&gt;//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;// TestAscendConvLayer();&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;// TestAscendConvLayerNCHW();&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token comment&#34;&gt;// TestAscendConvLayerNHWC();&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;还要再提一句，该工程中采用 C 语言函数 memcpy () 来模拟昇腾的批量数据拷贝功能。数据搬运中并不是所有的情况都是 256 个数据内存连续的，所以可以看到代码运行中分两次、三次才能拷贝完 256 个数据的情况。昇腾硬件中设计的 DMA 队列不会出现这种问题。此外，硬件肯定设计为边搬运边计算的工作模式，不会像我工程中完全得到 Input 大 Z 小 Z 排布再进行矩阵运算。&lt;/p&gt;
&lt;p&gt;文章好长啊！画了好多图！&lt;/p&gt;
&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://forcheetah.github.io/2024/05/15/openBlas/</guid>
            <title>openBlas库的安装与简单使用</title>
            <link>https://forcheetah.github.io/2024/05/15/openBlas/</link>
            <category>Linux</category>
            <category>openBlas</category>
            <pubDate>Wed, 15 May 2024 20:27:08 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;编译&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#编译&#34;&gt;#&lt;/a&gt; 编译&lt;/h1&gt;
&lt;p&gt;在官网上下载 zip 或通过 git 下载：&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;weget https://github.com/OpenMathLib/OpenBLAS.git&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;将文件解压后进入工程主目录&lt;/p&gt;
&lt;p&gt;编译 openblas 库&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;span&gt;h&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;make&lt;/span&gt; &lt;span class=&#34;token parameter variable&#34;&gt;-j32&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;导出 依赖库&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;make&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;token assign-left variable&#34;&gt;PREFIX&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;/path/to/your/admire&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&#34;cblas_sgemm函数&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#cblas_sgemm函数&#34;&gt;#&lt;/a&gt; cblas_sgemm 函数&lt;/h1&gt;
&lt;figure class=&#34;highlight c&#34;&gt;&lt;figcaption data-lang=&#34;c&#34;&gt;&lt;span&gt;c&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;cblas_sgemm&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;order&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; transA&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; transB&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; M&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; N&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; K&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; ALPHA&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; A&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; LDA&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; LDB&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; BETA&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; C&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; LDC&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;order &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; ClasRowMajow 行优先  ClasColMajow 列优先&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;transA &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; CblasTrans  转置     CblasNoTrans 不转置&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;transA &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; CblasTrans  转置     CblasNoTrans 不转置&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;M &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; 表示 A或C的行数。如果A转置，则表示转置后的行数    A的行数（transA处理之后）&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;N &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; 表示 B或C的列数。如果B转置，则表示转置后的列数    B的列数（transB处理之后）&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;K &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; K &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; A的列数 &lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt; B的行数&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;ALPHA &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;ALPHA&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;B&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;BETA&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;C  矩阵权重&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;A &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; 输入矩阵&lt;span class=&#34;token function&#34;&gt;A&lt;/span&gt;  &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;M&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;K&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;LDA &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; A所在矩阵的列数   有可能 LAD&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;K&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;B &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; 输入矩阵&lt;span class=&#34;token function&#34;&gt;B&lt;/span&gt;  &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;K&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;N&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;LDB &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; B所在矩阵的列数   有可能 LDB&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;N&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;BETA &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;ALPHA&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;A&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;B&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;BETA&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;C  偏置权重&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;C &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; 偏置C  同时也是输出&lt;span class=&#34;token function&#34;&gt;C&lt;/span&gt;  &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;M&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;N&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;16&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;LDC &lt;span class=&#34;token operator&#34;&gt;:&lt;/span&gt; C所在矩阵的列数   有可能 LDC&lt;span class=&#34;token operator&#34;&gt;&gt;&lt;/span&gt;N&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&#34;使用例子&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#使用例子&#34;&gt;#&lt;/a&gt; 使用例子&lt;/h1&gt;
&lt;figure class=&#34;highlight c&#34;&gt;&lt;figcaption data-lang=&#34;c&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token macro property&#34;&gt;&lt;span class=&#34;token directive-hash&#34;&gt;#&lt;/span&gt;&lt;span class=&#34;token directive keyword&#34;&gt;include&lt;/span&gt; &lt;span class=&#34;token string&#34;&gt;&amp;lt;vector&gt;&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token macro property&#34;&gt;&lt;span class=&#34;token directive-hash&#34;&gt;#&lt;/span&gt;&lt;span class=&#34;token directive keyword&#34;&gt;include&lt;/span&gt; &lt;span class=&#34;token string&#34;&gt;&amp;lt;iostream&gt;&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token macro property&#34;&gt;&lt;span class=&#34;token directive-hash&#34;&gt;#&lt;/span&gt;&lt;span class=&#34;token directive keyword&#34;&gt;include&lt;/span&gt; &lt;span class=&#34;token string&#34;&gt;&amp;lt;cblas.h&gt;&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;using namespace std&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;token function&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; M&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; N&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; K&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; alpha&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; beta&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; lda&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;K&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; ldb&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;N&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; ldc&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;N&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;15&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; A&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;M&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;K&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;16&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;K&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;N&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;17&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;float&lt;/span&gt; C&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;M&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;N&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;18&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;19&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token function&#34;&gt;cblas_sgemm&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;CblasRowMajor&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; CblasNoTrans&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; CblasNoTrans&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; M&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; N&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; K&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; alpha&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; A&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; lda&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; B&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; ldb&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; beta&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; C&lt;span class=&#34;token punctuation&#34;&gt;,&lt;/span&gt; ldc&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;20&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;21&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; i&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;i&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;M&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;i&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;22&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token keyword&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token keyword&#34;&gt;int&lt;/span&gt; j&lt;span class=&#34;token operator&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;token number&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;j&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&lt;/span&gt;N&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;j&lt;span class=&#34;token operator&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#123;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;23&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;            cout&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;C&lt;span class=&#34;token punctuation&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;token operator&#34;&gt;*&lt;/span&gt;N&lt;span class=&#34;token operator&#34;&gt;+&lt;/span&gt;j&lt;span class=&#34;token punctuation&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;token string&#34;&gt;&#34; &#34;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;24&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;   &lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;25&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;        cout&lt;span class=&#34;token operator&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;endl&lt;span class=&#34;token punctuation&#34;&gt;;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;26&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;    &lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;  &lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;27&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token punctuation&#34;&gt;&amp;#125;&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;对应的 CMakeLists.txt 文件&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;cmake_minimum_required &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;VERSION &lt;span class=&#34;token number&#34;&gt;3.5&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;project &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;demo&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;5&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;include_directories &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token variable&#34;&gt;$&amp;#123;PROJECT_SOURCE_DIR&amp;#125;&lt;/span&gt;/include&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;6&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;                    /path/to/your/admire/include&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;7&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;8&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;find_library&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;OpenBlas_Lib openblas HINTS /path/to/your/admire/lib&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;9&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;10&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;11&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;AUX_SOURCE_DIRECTORY&lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;token variable&#34;&gt;$&amp;#123;PROJECT_SOURCE_DIR&amp;#125;&lt;/span&gt;/src   SRC_FILE&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;12&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token builtin class-name&#34;&gt;set&lt;/span&gt; &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;SRC_FILE &lt;span class=&#34;token variable&#34;&gt;$&amp;#123;PROJECT_SOURCE_DIR&amp;#125;&lt;/span&gt;/src/test.cpp&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;13&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;add_executable &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;demo &lt;span class=&#34;token variable&#34;&gt;$&amp;#123;SRC_FILE&amp;#125;&lt;/span&gt;&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;14&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;target_link_libraries &lt;span class=&#34;token punctuation&#34;&gt;(&lt;/span&gt;demo &lt;span class=&#34;token variable&#34;&gt;$&amp;#123;OpenBlas_Lib&amp;#125;&lt;/span&gt; m&lt;span class=&#34;token punctuation&#34;&gt;)&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;在工程主目录&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;figcaption data-lang=&#34;bash&#34;&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td data-num=&#34;1&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;mkdir&lt;/span&gt; build &lt;span class=&#34;token operator&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;token builtin class-name&#34;&gt;cd&lt;/span&gt; build&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;2&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;cmake &lt;span class=&#34;token punctuation&#34;&gt;..&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;3&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;&lt;span class=&#34;token function&#34;&gt;make&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-num=&#34;4&#34;&gt;&lt;/td&gt;&lt;td&gt;&lt;pre&gt;./demo&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&#34;后记&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#后记&#34;&gt;#&lt;/a&gt; 后记&lt;/h1&gt;
&lt;p&gt;本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 &lt;a href=&#34;https://github.com/ForCheetah/ForCheetah.github.io&#34;&gt;github 项目&lt;/a&gt; 或随便一个项目下提出 issue，或者&lt;a href=&#34;https://www.zhihu.com/people/guai-dao-ji-de-3-50&#34;&gt;知乎&lt;/a&gt; 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>

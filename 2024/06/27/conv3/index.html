<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"><link rel="alternate" href="/rss.xml" title="Пусть этот камень будет более крепким, чем человек" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="Пусть этот камень будет более крепким, чем человек" type="application/atom+xml"><link rel="alternate" type="application/json" title="Пусть этот камень будет более крепким, чем человек" href="https://forcheetah.github.io/feed.json"><link rel="preconnect" href="https://lf9-cdn-tos.bytecdntp.com"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&amp;display=swap&amp;subset=latin,latin-ext" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/app.css?v=0.4.2"><link rel="modulepreload" href="/js/chunk-FJ7AJ5BW.js"><link rel="modulepreload" href="/js/chunk-MQTNP6EI.js"><link rel="modulepreload" href="/js/chunk-QAWHJ5B3.js"><link rel="modulepreload" href="/js/index.esm-SU253EAQ.js"><link rel="modulepreload" href="/js/post-SZ2V6ERD.js"><link rel="modulepreload" href="/js/quicklink-GO25OZIT.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo6.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo10.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/girl.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/gamersky.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo13.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo7.webp" as="image" fetchpriority="high"><meta name="keywords" content="Linux,"><meta name="description" content="有自己的博客很帅，但是我很懒，要命！！！"><link rel="canonical" href="https://forcheetah.github.io/2024/06/27/conv3/"><title>【im2col】AScend conv accelerate</title><meta name="generator" content="Hexo 7.0.0"></head><body itemscope="" itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">【im2col】AScend conv accelerate</h1><div class="meta"><span class="item" title="Created: 2024-06-27 23:21:34"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">Posted on</span><time itemprop="dateCreated datePublished" datetime="2024-06-27T23:21:34+08:00">2024-06-27</time></span><span class="item" title="Symbols count in article"><span class="icon"><i class="ic i-pen"></i></span><span class="text">Symbols count in article</span><span>9.2k</span><span class="text">words</span></span><span class="item" title="Reading time"><span class="icon"><i class="ic i-clock"></i></span><span class="text">Reading time</span><span>8 mins.</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">暮冬Z羡慕的博客</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><ul><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo6.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo10.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/girl.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/gamersky.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo13.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo7.webp&quot;);"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement="" itemscope="" itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">Home</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope="itemscope" itemtype="https://schema.org/ListItem"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" itemprop="item" rel="index" title="In卷积加速"><span itemprop="name">卷积加速<meta itemprop="position" content="0"></span></a></span></div><article class="post block" itemscope="itemscope" itemtype="http://schema.org/Article" lang="en"><link itemprop="mainEntityOfPage" href="https://forcheetah.github.io/2024/06/27/conv3/"><span hidden="hidden" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.webp"><meta itemprop="name" content="XianMu"><meta itemprop="description" content="神经网络推理、加速、AI编译。 我必须立刻开始挣扎！, 有自己的博客很帅，但是我很懒，要命！！！"></span><span hidden="hidden" itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="Пусть этот камень будет более крепким, чем человек"></span><div class="body md" itemprop="articleBody"><h1 id="前置信息"><a class="anchor" href="#前置信息">#</a> 前置信息</h1>
<p><strong>（1）本文讲解使用的例子</strong></p>
<p>以如下的卷积为例，进行昇腾 Im2Col 卷积过程：</p>
<ul>
<li>Input 输入维度为 NHWC ：【2，25，25，17】</li>
<li>外圈蓝色代表 pad</li>
<li>Kernal 维度为  CCHkWk  ：【34，17，3，3】</li>
<li>操作为 3*3 卷积 pad=1, Group=1, Stride=1， 2D 卷积</li>
<li>得到输出的维度 为 NHWC : 【22，25，25，18】</li>
</ul>
<p>从图上可以轻易看出相关信息。</p>
<p><img loading="lazy" data-src="1719500181052.webp" alt="例子"></p>
<blockquote>
<p>现在想起来，光是遇到你这个家伙，就感觉自己赚到了。<br>
------   大家好啊    我是   暮冬 Z 羡慕</p>
</blockquote>
<p><strong>（2）矩阵乘运算单元</strong></p>
<p>昇腾达芬奇架构设计了 16*16 的矩阵乘运算单元，能够提供强大的并行乘加计算能力，可以以一条指令实现两个 16*16 的矩阵相乘的运算。所以昇腾 Im2Col 卷积的目的就是让卷积能够高效地利用 “矩阵乘运算单元” 进行计算。</p>
<p><img loading="lazy" data-src="1719500301302.webp" alt="davincii"></p>
<p>感兴趣的可以阅读昇腾架构介绍书籍。</p>
<blockquote>
<p>矩阵计算单元可以⽤⼀条指令完成两个 16×16 矩阵的相乘运算（标记为 16<sup>3，也是 Cube 这⼀名称的来历），等同于在极短时间内进⾏了 16</sup>3＝4096 个乘加运算，并且可以实现 FP16 的运算精度。如图 3-7 所⽰，矩阵计算单元在完成 C＝A×B 的矩阵运算时，会事先将矩阵 A 按⾏存放在输⼊缓冲区中，同时将矩阵 B 按列存放在输⼊缓冲区中，通过矩阵计算单元计算后得到的结果矩阵 C 按⾏存放在输出缓冲区中。在矩阵相乘运算中，矩阵 C 的第⼀元素由矩阵 A 的第⼀⾏的 16 个元素和矩阵 B 的第⼀列的 16 个元素由矩阵计算单元⼦电路进⾏ 16 次乘法和 15 次加法运算得出。矩阵计算单元中共有 256 个矩阵计算⼦电路，可以由⼀条指令并⾏完成矩阵 C 的 256 个元素计算。                                          摘自《昇腾 AI 处理器架构与编程》</p>
</blockquote>
<h1 id="权重排布"><a class="anchor" href="#权重排布">#</a> 权重排布</h1>
<p>昇腾 Im2Col 五维卷积加速算法   基本流程：</p>
<p>输入为 nhwc 输出为 nhwc</p>
<p>权重维度变化： 权重的维度变化离线进行，不消耗神经网络推理时间。（神经网络推理大致分为 模型转换 量化 推理三个步骤，权重的维度转换可以在模型转换时进行，不占用推理的时间）。下面是权重变换的分步流程，代码实现可以一步完成，也可以分多步完成（因为不影响推理时间。）</p>
<p><img loading="lazy" data-src="1719500423112.webp" alt="weight change"></p>
<p>上方的变换如果比较抽象的话，可以结合后面的流程来理解。</p>
<h2 id="权重-从kernel-4d变换到kernel-2d"><a class="anchor" href="#权重-从kernel-4d变换到kernel-2d">#</a> 权重 从 kernel 4D 变换到 kernel 2D</h2>
<p><img loading="lazy" data-src="1719500512598.webp" alt="weight change"></p>
<p><img loading="lazy" data-src="1719500556415.webp" alt="weight change2"></p>
<p>上图是 Kernel 2D 的数据排布方式，维度为【2*3*3*16，34】，为了简便，跳过昇腾 5D 结构，直接从 4D 转到 2D。下面介绍 4D 数据和 2D 数据的一一对应关系。</p>
<ul>
<li>D 图 ① 覆盖区域表示 一个卷积核【17，3，3】展开成 2D 中的一列。对应于 A 图中一整个卷积核。34 个卷积核将展开为 34 列。因此每列代表一个卷积核。</li>
<li>B 图，卷积核通道数为 17，需要补零为 16 的倍数 32，并拆分成 2 块（分别是紫色、黄色）。E 图：每一列（每一个卷积核）的紫色部分②是卷积核通道方向拆分的第一块（B 图中的紫色），黄色部分③是拆分的第二块（B 图中的黄色）。</li>
<li>拆分的每一块（比如紫色部分）又分成 3*3（kernel 行 * 列），F 图: ④覆盖的是 kernel 第一行 (对应于 C 图中的④的部分)，⑤覆盖的是 kernel 第二行（对应于 C 图中⑤的部分），相似的⑥覆盖的是 kernel 第三行（对应于 C 图中⑥的部分）。3*3 卷积核一共就三行</li>
<li>每一个紫色的小方格代表通道方向的 16 个数。</li>
<li>至此，kernel 4D 和 kernel 2D 所有的数据都一一对应了。例如 F 图中：⑦代表第 6 个卷积核、通道拆分的第二块、第一行、第二列、通道方向的 16 个数。</li>
</ul>
<p>通过上述对应关系，我们不难得到维度为【2*3*3*16，34】的卷积核 2D 形式。由于昇腾卷积算法的 AI 计算核心是 16*16 的矩阵乘运算单元，同时为了取数方便，还需要将卷积核 2D 转换为大 Z 小 N 排布方式。</p>
<h2 id="权重-从kernel2d变换到大z小n"><a class="anchor" href="#权重-从kernel2d变换到大z小n">#</a> 权重 从 kernel2D 变换到大 Z 小 N</h2>
<p><img loading="lazy" data-src="1719500684744.webp" alt="2d"></p>
<p>第一步，将 2D【2*3*3*16，34】中 34 补零为 16 的倍数，即 48，得到【2*3*3*16，48】。</p>
<p>第二步，将其按照 16*16 的方格进行划分，得到【2*3*3，3】个【16，16】的小块。（图中画成了 4 个小块，实际应该是 3 个，示意图，见谅）</p>
<p>第三步，将这些小块按照大 Z 小 N 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube8 这种 “Z” 字形排布；小 N 指的是内部按照列优先，即每个 16*16 的 Cube，先排第一列，然后是第二列...  详见最右边的彩色表示。</p>
<p>多说一句，之所以专门按照 “小 N” 排布，是因为在矩阵运算中，权重作为矩阵乘的第二个参数，数据是按列取的。这就意味着在实际内存中要跳着取数（内存中都是按照行优先排序），自然效率低。提前将其按照列优先的方式进行排布，那么在矩阵乘运算中可以连续取数。至此，我们得到了 【2*3*3，3，16*16】的权重大 Z 小 N 排布形式，这种形式使得能够一次性取出 256 个数参与计算，效率很高。</p>
<p>下面的代码一次性完成了 权重 4D nhwc  到权重大 Z 小 N 排布，仅供参考。还是那句话，权重的变换离线进行，不占用宝贵的推理时间，所以无须关心转换的效率。完整代码可以下载 <a target="_blank" rel="noopener" href="https://github.com/ForCheetah/ConvAccelerate">加速算法模拟</a>，并运行其中的  <code>TestAscendConvLayer();</code>  函数。可以看到三个测试函数，它们的区别在于不同的输入排布方式。</p>
<figure class="highlight cpp"><figcaption data-lang="C++"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment">//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token comment">// TestAscendConvLayer();</span></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token comment">//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token comment">// TestAscendConvLayerNCHW();</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token comment">//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token comment">// TestAscendConvLayerNHWC();</span></pre></td></tr></tbody></table></figure><figure class="highlight cpp"><figcaption data-lang="C++"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">void</span> <span class="token function">WeightTrans_A</span><span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> filters<span class="token punctuation">,</span> <span class="token keyword">const</span> TensorDim weight_dim<span class="token punctuation">,</span> Ascend5Dim we_5D_dim<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> we_tran5D<span class="token punctuation">,</span> </pre></td></tr><tr><td data-num="2"></td><td><pre>            AscendTransform5Dim we_tran5D_dim<span class="token punctuation">,</span> <span class="token keyword">int</span> CUBE_row<span class="token punctuation">,</span> <span class="token keyword">int</span> CUBE_col<span class="token punctuation">)</span><span class="token punctuation">{</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">int</span> lastdim4 <span class="token operator">=</span> we_tran5D_dim<span class="token punctuation">.</span>move <span class="token operator">*</span> we_tran5D_dim<span class="token punctuation">.</span>channel <span class="token operator">*</span> we_tran5D_dim<span class="token punctuation">.</span>LW <span class="token operator">*</span> we_tran5D_dim<span class="token punctuation">.</span>cube<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token keyword">int</span> lastdim3 <span class="token operator">=</span> we_tran5D_dim<span class="token punctuation">.</span>channel <span class="token operator">*</span> we_tran5D_dim<span class="token punctuation">.</span>LW <span class="token operator">*</span> we_tran5D_dim<span class="token punctuation">.</span>cube<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token keyword">int</span> lastdim2 <span class="token operator">=</span> we_tran5D_dim<span class="token punctuation">.</span>LW <span class="token operator">*</span> we_tran5D_dim<span class="token punctuation">.</span>cube<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token keyword">int</span> single_filter_num <span class="token operator">=</span> weight_dim<span class="token punctuation">.</span>c <span class="token operator">*</span> weight_dim<span class="token punctuation">.</span>h <span class="token operator">*</span> weight_dim<span class="token punctuation">.</span>w<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token keyword">int</span> single_filter_channel <span class="token operator">=</span> weight_dim<span class="token punctuation">.</span>h <span class="token operator">*</span> weight_dim<span class="token punctuation">.</span>w<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> ch_cube<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> ch_cube<span class="token operator">&lt;</span>we_tran5D_dim<span class="token punctuation">.</span>batch<span class="token punctuation">;</span> ch_cube<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token comment">// 通道方向块   ch_cube</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        <span class="token keyword">int</span> index_1 <span class="token operator">=</span> ch_cube <span class="token operator">*</span> lastdim4<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> hk<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> hk<span class="token operator">&lt;</span>we_tran5D_dim<span class="token punctuation">.</span>move<span class="token punctuation">;</span> hk<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token comment">//filter 长  </span></pre></td></tr><tr><td data-num="12"></td><td><pre>            <span class="token keyword">int</span> index_2 <span class="token operator">=</span> index_1 <span class="token operator">+</span> hk <span class="token operator">*</span> lastdim3<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="13"></td><td><pre>            <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> wk<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> wk<span class="token operator">&lt;</span>we_tran5D_dim<span class="token punctuation">.</span>channel<span class="token punctuation">;</span> wk<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token comment">//filter 宽</span></pre></td></tr><tr><td data-num="14"></td><td><pre>                <span class="token keyword">int</span> index_3 <span class="token operator">=</span> index_2 <span class="token operator">+</span> wk <span class="token operator">*</span> lastdim2<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="15"></td><td><pre>                <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> cout_cube<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> cout_cube<span class="token operator">&lt;</span>we_tran5D_dim<span class="token punctuation">.</span>LW<span class="token punctuation">;</span> cout_cube<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment">//cout 方向块 </span></pre></td></tr><tr><td data-num="16"></td><td><pre>                    <span class="token keyword">int</span> index_4 <span class="token operator">=</span> index_3 <span class="token operator">+</span> cout_cube<span class="token operator">*</span>we_tran5D_dim<span class="token punctuation">.</span>cube<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="17"></td><td><pre>                    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> cube_row<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> cube_row<span class="token operator">&lt;</span>CUBE_row<span class="token punctuation">;</span> cube_row<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span></pre></td></tr><tr><td data-num="18"></td><td><pre>                        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> cube_col<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> cube_col<span class="token operator">&lt;</span>CUBE_col<span class="token punctuation">;</span> cube_col<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span></pre></td></tr><tr><td data-num="19"></td><td><pre>                            <span class="token keyword">int</span> index <span class="token operator">=</span> index_4 <span class="token operator">+</span> cube_row<span class="token operator">*</span>CUBE_col <span class="token operator">+</span> cube_col<span class="token punctuation">;</span>                       </pre></td></tr><tr><td data-num="20"></td><td><pre>                            <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token punctuation">(</span>cout_cube<span class="token operator">*</span>CUBE_col<span class="token operator">+</span>cube_row<span class="token punctuation">)</span><span class="token operator">&gt;=</span>weight_dim<span class="token punctuation">.</span>n  <span class="token operator">||</span> <span class="token punctuation">(</span>ch_cube<span class="token operator">*</span>CUBE_col<span class="token operator">+</span>cube_col<span class="token punctuation">)</span><span class="token operator">&gt;=</span>weight_dim<span class="token punctuation">.</span>c<span class="token punctuation">)</span><span class="token punctuation">{</span></pre></td></tr><tr><td data-num="21"></td><td><pre>                                we_tran5D<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="22"></td><td><pre>                            <span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span></pre></td></tr><tr><td data-num="23"></td><td><pre>                                <span class="token comment">// 第几个 filter  第几个通道  第几行  第几列  还要注意 大 Z 小 N 排布方式     大 Z 小 N 排布方式（行变列，列变行）</span></pre></td></tr><tr><td data-num="24"></td><td><pre>                                <span class="token keyword">int</span> index_from <span class="token operator">=</span> <span class="token punctuation">(</span>cout_cube<span class="token operator">*</span>CUBE_col<span class="token operator">+</span>cube_row<span class="token punctuation">)</span><span class="token operator">*</span>single_filter_num <span class="token operator">+</span> <span class="token punctuation">(</span>ch_cube<span class="token operator">*</span>CUBE_col<span class="token operator">+</span>cube_col<span class="token punctuation">)</span><span class="token operator">*</span>single_filter_channel <span class="token operator">+</span> hk<span class="token operator">*</span>weight_dim<span class="token punctuation">.</span>w<span class="token operator">+</span> wk<span class="token punctuation">;</span>                                </pre></td></tr><tr><td data-num="25"></td><td><pre>                                we_tran5D<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> filters<span class="token punctuation">[</span>index_from<span class="token punctuation">]</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="26"></td><td><pre>                            <span class="token punctuation">}</span>  </pre></td></tr><tr><td data-num="27"></td><td><pre>                        <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="28"></td><td><pre>                    <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="29"></td><td><pre>                <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="30"></td><td><pre>            <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="31"></td><td><pre>        <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="32"></td><td><pre>    <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="33"></td><td><pre><span class="token punctuation">}</span></pre></td></tr></tbody></table></figure><h1 id="输入排布"><a class="anchor" href="#输入排布">#</a> 输入排布</h1>
<p>输入 tensor 的内存排布为 nhwc 输出为 nhwc</p>
<p>昇腾算法的维度详细变换如图下图所示。这里展示了输入 input 从 4D 维度转换到 昇腾 5D 结构，然后再转换到 2D 结构，最后转换到大 Z 小 Z 维度。写这么详细只是为了方便读者理解，而在实际操作中，由于 Input 的变换是在线进行，消耗宝贵的推理时间，所以如华为昇腾书中所说：input 先是从 4D 维度 通过软件算法转换为 昇腾 5D 维度（在模型推理过程中这一步可能不需要，因为中间层的 tensor 已经处于昇腾 5D 维度了），之后从昇腾 5D 维度通过 硬件直接转换到大 Z 小 Z 排布（模型推理过程肯定是边转换变计算，所以不会将整个 tensor 转换为大 Z 小 Z 之后，才进行矩阵运算阶段的。本博客为方便，将整个 tensor 完全转换到大 Z 小 Z，再进行后面计算。）</p>
<p>说完这些，就可以介绍一下昇腾算法极致高效的输入的排布转换过程了！</p>
<p><img loading="lazy" data-src="1719500920323.webp" alt="input"></p>
<h2 id="输入-从input-4d-到input-5d"><a class="anchor" href="#输入-从input-4d-到input-5d">#</a> 输入 从 Input 4D 到 Input 5D</h2>
<p>还是再强调一下，昇腾可以做到整个模型的中间层的 tensor 均保持昇腾 5D 的维度，所以思考一下，可能只有最初输入到模型的 tensor 需要 从 Input 4D 转 到 Input 5D，或者再数据预处理的时候就将数据处理为 5D 排布。</p>
<p><img loading="lazy" data-src="1719500976081.webp" alt="trans6"></p>
<ul>
<li>G 图是最原始的 Input4D 结构，当然，batch 维度 N=2 没有画，只画了一个。它的维度是【25，25，17】</li>
<li>H 图为昇腾 5D 结构图，首先要将通道方向的 17 补齐为 16 的倍数 32，同时每 16 个进行一次拆分，拆成两组。</li>
<li>最后注意一下数据的排布顺序就好了：注意 5D 结构中，K_cube 位于最内层，这些数据是连续的，所以先把 高 h=1, 宽 w=1 位置的 16 个数据排在一起。</li>
<li>紧接着将宽度方向 25 个 K_cube 排在一起，变成 25*16</li>
<li>然后再遍历高的方向。变成 25*25*16</li>
<li>最后是遍历两组，得到昇腾的 5D 结构【2，25，25，16】</li>
</ul>
<p>此处数据搬运较为简单，&lt;!-- 可以参考代码<a target="_blank" rel="noopener" href="https://github.com/ForCheetah/ConvAccelerate">加速算法模拟</a> --&gt;</p>
<h2 id="输入-从input-5d-直接搬到-大z小z"><a class="anchor" href="#输入-从input-5d-直接搬到-大z小z">#</a> 输入 从 Input 5D 直接搬到 大 Z 小 Z</h2>
<p>昇腾通过专门设计的硬件，将 input 从 5D 格式直接搬到 大 Z 小 Z 排布。想要知道怎么搬以及为什么这么搬，还真不得不把其 2D 排布讲明白。  《昇腾 AI 处理器架构与编程》这本书中直接跳过了 2D 排布，导致晦涩难懂。</p>
<h3 id="input-5d-到-input-2d"><a class="anchor" href="#input-5d-到-input-2d">#</a> Input 5D 到 Input 2D</h3>
<p>所以我们直接看 Input2D 与 Weight 2D 的对应情况，如下图所示。</p>
<p><img loading="lazy" data-src="1719501086578.webp" alt="trans5"></p>
<ul>
<li>J 图为 input2D 【25*25，2*3*3*16】   K 图为 Weight2D 【2*3*3*16，34】。再回忆一下 Weight2D 数据每一行和每一列的数据的意义，它的一列数据 2*3*3*16 代表什么呢？  2*3*3*16 代表一整个卷积核，2 代表该卷积核通道方向拆成两块，那么 3*3*16 就是每一块的 高 * 宽 * K_cube。</li>
<li>好巧！Input2D 的一行也是 2*3*3*16！（废话，不一样就没法算了）。既然 weight2D 一列数据的意义一清二楚，那么对应的 Input2D 数据一行的意义也就呼之欲出啦！ Input2D 的一行 就是卷积核在某个滑动窗口位置对应的 input 数据。例如，Input2D 的第一行，就对应于 I 图 3*3 的彩色窗口数据（没有 Pad 的情况下）。</li>
<li>也就可以推知，Input2D 的每一行绿色部分，就是 I 图通道方向拆分的第一块（拆分的绿色部分）；每一行的的蓝色部分，就是 I 图通道防线拆分的第二块（中间深蓝宽度 1，和补齐的浅蓝 15）</li>
<li>那么，为什么 Input2D 有足足 625 行呢？因为滑动窗口纵向滑动 25 次，每次纵向滑动，都包含横向的 25 次，总共 625 次。</li>
</ul>
<p>假如直接计算 Input2D 矩阵乘 Weight2D，卷积计算就得到最终结果啦！这就是普通的 Im2Col 算法，不清楚的小伙伴们还可以去读一下 <a href="https://forcheetah.github.io/2024/05/23/conv1/">Im2Col 算法 NCHW</a> 和 <a href="https://forcheetah.github.io/2024/05/23/conv2/">Im2Col 算法 NHWC</a>。</p>
<p>从 2D 的角度来看，算法是不是很简单啊。</p>
<p>不要高兴的太早，还没完呢。</p>
<h3 id="input-2d-到-大z小z"><a class="anchor" href="#input-2d-到-大z小z">#</a> Input 2D 到 大 Z 小 Z</h3>
<p><img loading="lazy" data-src="1719501234541.webp" alt="trans4"></p>
<p>接下来是将 Input2D 转换到大 Z 小 Z 排布</p>
<p>第一步，将 Input2D【25*25，2*3*3*16】中 25*25 补零为 16 的倍数，即 640，得到【640，2*3*3*16】  ，如图 L。</p>
<p>第二步，将其按照 16*16 的方格进行划分，即得到【40，18】个【16，16】的小块，如图 M。</p>
<p>第三步，将这些小块按照大 Z 小 Z 的顺序进行排布。大 Z 指的是外部按照行优先，将按照 Cube1 到 Cube720 这些块按照 “Z” 字形排布；像 N 图上方排成一行；小 Z 指的是内部也按照行优先，即每个 16*16 的 Cube，先排第一行，然后是第二行... 详见 N 图中的颜色表示。</p>
<p><img loading="lazy" data-src="1719501296597.webp" alt="trans3"></p>
<p>上图来自《昇腾 AI 处理器架构与编程》，矩阵 A 的排布为大 Z 小 Z，矩阵 B 的排布为大 Z 小 N，大家可以再理解一下。</p>
<p>至此，Input 的大 Z 小 Z 排布已经实现，接下来就是 16*16 的矩阵乘了。</p>
<p><img loading="lazy" data-src="1719501324297.webp" alt="trans2"></p>
<ul>
<li>Input 现在是【40，18】个【16，16】小块，如左图，当然，它现在处于大 Z 小 Z 的一维排布。</li>
<li>Weight 现在是 【18，3】个【16，16】小块，如中间图，当然，它现在处于大 Z 小 N 的一维排布。</li>
<li>不知道分块矩阵乘的小伙伴可以再搜索下 《线性代数》中的分块矩阵乘运算。</li>
<li>内部，进行两个 16*16 块的矩阵乘运算，由于 weight 已经按照列优先进行排布，所以矩阵乘的顺序如上图最右边所示。</li>
<li>外部，对【40，18】和【18，3】做矩阵乘运算。</li>
<li>至此，我们得到了【640，18】的矩阵。</li>
<li>然后将上图两图灰色部分对应的多余数据裁掉，就得到了卷积结果【25，25，34】 ，当然，还得遍历一下 batch，得到【2，25，25，34】</li>
</ul>
<h3 id="input5d搬到大z小z"><a class="anchor" href="#input5d搬到大z小z">#</a> Input5D 搬到大 Z 小 Z</h3>
<p>前两小节介绍了 Input5D 变换到 Input 2D，再变换到 大 Z 小 Z 的过程。而在昇腾芯片中，从 Input5D 到 Input2D 由硬件一步实现。</p>
<p>如果前面两小节已经看明白了的话，那么搬运的秘密就呼之欲出了。</p>
<p><img loading="lazy" data-src="1719501381618.webp" alt="trans1"></p>
<ul>
<li>看上图，左图是 Input 的 5D 维度排布【2，25，25，16】，右边是 Input 2D 排布【25*25，2*3*3*16】。中间是个滑动窗口示意图，3*3，因为本文中用的例子就是 3*3 卷积。</li>
<li>回忆一下右边 2D 排布的数据的意义，每一个小格子是通道方向的 16 个数，每一行是滑动窗口每一个位置对应的 2*3*3*16 个数。滑动窗口纵向滑动 25 次，每次要横向滑动 25 次，所以有 625 行数据，再加上补齐的 15 行，才达到了 640 行数据。</li>
<li>那么右图红色 1 的位置是滑动窗口 a 在第一个位置所对应的 16 个数字；红色 2 的位置是滑动窗口 a 横向滑动一次对应的 16 个数字；红色 3 的位置是滑动窗口 a 横向滑动第三次对应的 16 个数字；依次类推，红色 16 的位置是滑动窗口横向滑动第 16 次对应的 16 个数字。这 16 次滑动，滑动窗口的 a 在左图从 1 滑倒 16！</li>
<li>也就是说，右图红色框的 1-16 与左图 1-16 一一对应！</li>
<li>再来回忆一下，左图中 1-16 这 16*16 的数据是连续的吗？是！（不清楚的再回去看 Input 的维度变换）</li>
<li>那么右图中的 1-16 这 16*16 个数据是连续的吗？它是！ 根据大 Z 小 Z 排布，这红色框中 16*16 的数据刚好被分到一个小 Cube 中！</li>
<li>昇腾能够从 Input5D 中一次性拷贝 256 个数据到大 Z 小 Z 排布！</li>
</ul>
<p>&lt;!--</p>
<h1 id="代码模拟"><a class="anchor" href="#代码模拟">#</a> 代码模拟</h1>
<p>当然，我猜测昇腾应该是设计了 16 个 DMA 组成的 DAM 队列，来实现一次 256 个数据的搬运。真的是相当高效了！</p>
<p>我提供了 C 语言代码模拟整个昇腾的卷积运算流程。完整代码可以在 <a target="_blank" rel="noopener" href="https://github.com/ForCheetah/ConvAccelerate">加速算法模拟</a>下载，该工程提供了以下三个测试函数，它们的区别在于不同的输入排布方式。</p>
<figure class="highlight cpp"><figcaption data-lang="C++"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment">//9.  测试 昇腾 卷积算法加速     NCHW 输入， NHWC 输出</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token comment">// TestAscendConvLayer();</span></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token comment">//10.  测试 昇腾 卷积算法加速      NCHW 输入， NCHW 输出</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token comment">// TestAscendConvLayerNCHW();</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token comment">//11. 测试 昇腾卷积算法加速 NHWC      NHWC 输入， NHWC 输出</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token comment">// TestAscendConvLayerNHWC();</span></pre></td></tr></tbody></table></figure><p>还要再提一句，该工程中采用 C 语言函数 memcpy () 来模拟昇腾的批量数据拷贝功能。数据搬运中并不是所有的情况都是 256 个数据内存连续的，所以可以看到代码运行中分两次、三次才能拷贝完 256 个数据的情况。昇腾硬件中设计的 DMA 队列不会出现这种问题。此外，硬件肯定设计为边搬运边计算的工作模式，不会像我工程中完全得到 Input 大 Z 小 Z 排布再进行矩阵运算。</p>
<p>这是隐藏的注释，不会显示。 --&gt;</p>
<p>文章好长啊！画了好多图！</p>
<h1 id="后记"><a class="anchor" href="#后记">#</a> 后记</h1>
<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a target="_blank" rel="noopener" href="https://github.com/ForCheetah/ForCheetah.github.io">github 项目</a> 或随便一个项目下提出 issue，或者<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/guai-dao-ji-de-3-50">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>
<div class="tags"><a href="/tags/Linux/" rel="tag"><i class="ic i-tag"></i>Linux</a><a href="/tags/openBlas/" rel="tag"><i class="ic i-tag"></i>openBlas</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">Edited on</span><time title="Modified: 2025-05-15 19:12:42" itemprop="dateModified" datetime="2025-05-15T19:12:42+08:00">2025-05-15</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.webp" alt="XianMu WeChat Pay"><p>WeChat Pay</p></div><div><img loading="lazy" data-src="/assets/alipay.webp" alt="XianMu Alipay"><p>Alipay</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>XianMu<i class="ic i-at"><em>@</em></i>Пусть этот камень будет более крепким, чем человек</li><li class="link"><strong>Post link: </strong><a href="https://forcheetah.github.io/2024/06/27/conv3/" title="【im2col】AScend conv accelerate">https://forcheetah.github.io/2024/06/27/conv3/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/06/18/deployTVMPython/" rel="prev" itemprop="url" data-background-image="https://forcheetah.github.io/assets/lunbo10.webp" title="【TVM】Python脚本实现模型编译和保存"><span class="type">Previous Post</span><span class="category"><i class="ic i-flag"></i>tvm</span><h3>【TVM】Python脚本实现模型编译和保存</h3></a></div><div class="item right"><a href="/2024/07/05/problem3/" rel="next" itemprop="url" data-background-image="https://forcheetah.github.io/assets/lunbo5.webp" title="SystemC 等待异步事件解决方案"><span class="type">Next Post</span><span class="category"><i class="ic i-flag"></i>问题解决</span><h3>SystemC 等待异步事件解决方案</h3></a></div></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text"> 前置信息</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E6%8E%92%E5%B8%83"><span class="toc-number">2.</span> <span class="toc-text"> 权重排布</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D-%E4%BB%8Ekernel-4d%E5%8F%98%E6%8D%A2%E5%88%B0kernel-2d"><span class="toc-number">2.1.</span> <span class="toc-text"> 权重 从 kernel 4D 变换到 kernel 2D</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D-%E4%BB%8Ekernel2d%E5%8F%98%E6%8D%A2%E5%88%B0%E5%A4%A7z%E5%B0%8Fn"><span class="toc-number">2.2.</span> <span class="toc-text"> 权重 从 kernel2D 变换到大 Z 小 N</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E6%8E%92%E5%B8%83"><span class="toc-number">3.</span> <span class="toc-text"> 输入排布</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5-%E4%BB%8Einput-4d-%E5%88%B0input-5d"><span class="toc-number">3.1.</span> <span class="toc-text"> 输入 从 Input 4D 到 Input 5D</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5-%E4%BB%8Einput-5d-%E7%9B%B4%E6%8E%A5%E6%90%AC%E5%88%B0-%E5%A4%A7z%E5%B0%8Fz"><span class="toc-number">3.2.</span> <span class="toc-text"> 输入 从 Input 5D 直接搬到 大 Z 小 Z</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#input-5d-%E5%88%B0-input-2d"><span class="toc-number">3.2.1.</span> <span class="toc-text"> Input 5D 到 Input 2D</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#input-2d-%E5%88%B0-%E5%A4%A7z%E5%B0%8Fz"><span class="toc-number">3.2.2.</span> <span class="toc-text"> Input 2D 到 大 Z 小 Z</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#input5d%E6%90%AC%E5%88%B0%E5%A4%A7z%E5%B0%8Fz"><span class="toc-number">3.2.3.</span> <span class="toc-text"> Input5D 搬到大 Z 小 Z</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E6%A8%A1%E6%8B%9F"><span class="toc-number">4.</span> <span class="toc-text"> 代码模拟</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%8E%E8%AE%B0"><span class="toc-number">5.</span> <span class="toc-text"> 后记</span></a></li></ol></div><div class="related panel pjax" data-title="Related"><ul><li><a href="/2024/05/23/conv1/" rel="bookmark" title="【Im2Col】卷积加速算法【1】 NCHW">【Im2Col】卷积加速算法【1】 NCHW</a></li><li><a href="/2024/05/23/conv2/" rel="bookmark" title="【Im2Col】卷积加速算法【2】NHWC">【Im2Col】卷积加速算法【2】NHWC</a></li><li class="active"><a href="/2024/06/27/conv3/" rel="bookmark" title="【im2col】AScend conv accelerate">【im2col】AScend conv accelerate</a></li><li><a href="/2024/07/07/conv4/" rel="bookmark" title="【Winograd】卷积加速算法原理及实现">【Winograd】卷积加速算法原理及实现</a></li><li><a href="/2024/12/20/conv5/" rel="bookmark" title="【gemm】Gemm计算加速">【gemm】Gemm计算加速</a></li><li><a href="/2024/12/24/conv6/" rel="bookmark" title="【Gemm】内存对齐">【Gemm】内存对齐</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="XianMu" src="/assets/avatar.webp"><p class="name" itemprop="name">XianMu</p><div class="description" itemprop="description">有自己的博客很帅，但是我很懒，要命！！！</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">41</span><span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">16</span><span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">21</span><span class="name">tags</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/ForCheetah" class="item github" title="https://github.com/ForCheetah"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/guai-dao-ji-de-3-50" class="item zhihu" title="https://www.zhihu.com/people/guai-dao-ji-de-3-50"><i class="ic i-zhihu"></i></a><a href="/huasen.w@foxmail.com" class="item email" title="huasen.w@foxmail.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2024/07/05/problem3/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/06/18/deployTVMPython/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/12/24/conv6/">【Gemm】内存对齐</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E9%87%8F%E5%8C%96/" title="In量化">量化</a></div><span><a href="/2025/01/03/quanti01/">【量化】连续卷积层首尾量化的可行性</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/compile/" title="Incompile">compile</a></div><span><a href="/2025/01/14/aicompile02/">【AI编译】如何进行layer-group</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/" title="In编译器">编译器</a></div><span><a href="/2025/03/20/compile02/">【编译器】使用llvm编译自定义语言【2】转llvm IR</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" title="In常见问题">常见问题</a></div><span><a href="/2024/05/14/cpplib/">C语言工程调用Cpp库解决方案</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/06/27/conv3/">【im2col】AScend conv accelerate</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/05/23/conv1/">【Im2Col】卷积加速算法【1】 NCHW</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/tvm/" title="Intvm">tvm</a></div><span><a href="/2024/10/31/tvm05/">【TVM】通过代码学习编译流程【5】FuseOps</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/NCNN/" title="InNCNN">NCNN</a></div><span><a href="/2025/04/16/ncnn01/">【NCNN】学习ncnn模型转换</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/07/07/conv4/">【Winograd】卷积加速算法原理及实现</a></span></li></ul></div><div class="rpost pjax"><h2>Recent Comments</h2></div></div><div class="status"><div class="copyright">© 2010 -<span itemprop="copyrightYear">2025</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">XianMu @ 暮冬Z羡慕的博客</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="Symbols count total">288k words</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="Reading time total">4:21</span></div><div class="powered-by">Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> &amp; Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config="" type="text/javascript">var LOCAL = {
    ispost: true,
        path: `2024/06/27/conv3/`,
        favicon: {
        show: `(●´3｀●) Here we go again.`,
        hide: `(´Д｀) It's a disaster!`
    },
    search: {
        placeholder: "Search for Posts",
        empty: "We didn't find any results for the search: ${query}",
        stats: "${hits} results found in ${time} ms"
    },
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: {},
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">Article Timeliness Alert</span><br>This is an article published {{publish}} days ago and last updated {{updated}} days ago. Some information may have changed, so please be careful to screen it.</p></div>`,
    quiz: {
        choice: `Multiple Choice`,
        multiple: `Multiple Answer`,
        true_false: `True/False`,
        essay: `Questions`,
        gap_fill: `Gap Filling`,
        mistake: `Wrong Answer`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};
</script><script src="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-6-M/pace/1.2.4/pace.min.js" async=""></script><script src="/js/siteInit.js?v=0.4.2" type="module" fetchpriority="high" defer=""></script></body></html>
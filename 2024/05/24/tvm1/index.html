<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"><link rel="alternate" href="/rss.xml" title="Пусть этот камень будет более крепким, чем человек" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="Пусть этот камень будет более крепким, чем человек" type="application/atom+xml"><link rel="alternate" type="application/json" title="Пусть этот камень будет более крепким, чем человек" href="https://forcheetah.github.io/feed.json"><link rel="preconnect" href="https://lf9-cdn-tos.bytecdntp.com"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&amp;display=swap&amp;subset=latin,latin-ext" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/app.css?v=0.4.2"><link rel="modulepreload" href="/js/chunk-FJ7AJ5BW.js"><link rel="modulepreload" href="/js/chunk-MQTNP6EI.js"><link rel="modulepreload" href="/js/chunk-QAWHJ5B3.js"><link rel="modulepreload" href="/js/index.esm-SU253EAQ.js"><link rel="modulepreload" href="/js/post-SZ2V6ERD.js"><link rel="modulepreload" href="/js/quicklink-GO25OZIT.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo1.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/gamersky.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo10.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo4.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo11.webp" as="image" fetchpriority="high"><link rel="preload" href="https://forcheetah.github.io/assets/lunbo7.webp" as="image" fetchpriority="high"><meta name="keywords" content="accelerate,tvm"><meta name="description" content="有自己的博客很帅，但是我很懒，要命！！！"><link rel="canonical" href="https://forcheetah.github.io/2024/05/24/tvm1/"><title>【TVM】根据例子走通代码库</title><meta name="generator" content="Hexo 7.0.0"></head><body itemscope="" itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">【TVM】根据例子走通代码库</h1><div class="meta"><span class="item" title="Created: 2024-05-24 22:49:36"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">Posted on</span><time itemprop="dateCreated datePublished" datetime="2024-05-24T22:49:36+08:00">2024-05-24</time></span><span class="item" title="Symbols count in article"><span class="icon"><i class="ic i-pen"></i></span><span class="text">Symbols count in article</span><span>7.3k</span><span class="text">words</span></span><span class="item" title="Reading time"><span class="icon"><i class="ic i-clock"></i></span><span class="text">Reading time</span><span>7 mins.</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">暮冬Z羡慕的博客</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><ul><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo1.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/gamersky.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo10.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo4.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo11.webp&quot;);"></li><li class="item" style="background-image: url(&quot;https://forcheetah.github.io/assets/lunbo7.webp&quot;);"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement="" itemscope="" itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">Home</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope="itemscope" itemtype="https://schema.org/ListItem"><a href="/categories/tvm/" itemprop="item" rel="index" title="Intvm"><span itemprop="name">tvm<meta itemprop="position" content="0"></span></a></span></div><article class="post block" itemscope="itemscope" itemtype="http://schema.org/Article" lang="en"><link itemprop="mainEntityOfPage" href="https://forcheetah.github.io/2024/05/24/tvm1/"><span hidden="hidden" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.webp"><meta itemprop="name" content="XianMu"><meta itemprop="description" content="神经网络推理、加速、AI编译。 我必须立刻开始挣扎！, 有自己的博客很帅，但是我很懒，要命！！！"></span><span hidden="hidden" itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="Пусть этот камень будет более крепким, чем человек"></span><div class="body md" itemprop="articleBody"><h1 id="前言"><a class="anchor" href="#前言">#</a> 前言</h1>
<p>最近开始学习 TVM。感觉 TVM 英文文档中 <a target="_blank" rel="noopener" href="https://tvm.apache.org/docs/dev/tutorial/codebase_walkthrough.html">TVM Codebase Walkthrough by Example</a>    一节对于理解 TVM 工程非常有用。本篇文章只是翻译，可以直接跳转查看英文全文。</p>
<blockquote>
<p>这个时代有这么多愿意开源并将技术介绍给我们的行业大牛，真是我们的荣幸，膜拜！<br>
------   大家好啊    我是   暮冬 Z 羡慕</p>
</blockquote>
<h1 id="codebase-structure-overview"><a class="anchor" href="#codebase-structure-overview">#</a> Codebase Structure Overview</h1>
<p>在 TVM 存储库的根目录中，我们有以下子目录，它们共同构成了大部分代码库。</p>
<ul>
<li><strong>src</strong><br>
C++ code for operator compilation and deployment runtimes.<br>
 算子编译 、 runtime 部署 的 C++ 代码</li>
<li><strong>src/relay</strong><br>
Implementation of Relay, a new functional IR for deep learning framework.<br>
Relay IR 的实现      算子的映射关系在 src/relay/op</li>
<li><strong>python</strong><br>
Python frontend that wraps C++ functions and objects implemented in src.<br>
python 前端</li>
<li><strong>src/topi</strong><br>
Compute definitions and backend schedules for standard neural network operators.<br>
 计算标准神经网络算子的定义和后端调度</li>
</ul>
<p>TVM 中 Python 和 C++ 的互操作性不是单向的。尽管在 TVM 中 C++ 完成繁重的内部执行工作，Python 完成用户接口， TVM 中也存在 C++ 调用 Python 的情况：For example, the convolution operator is implemented in Python, and its implementation is invoked from C++ code in Relay.（Relay 中的 C++ 调用 Python 实现的卷积算子）</p>
<h1 id="vector-add-example"><a class="anchor" href="#vector-add-example">#</a> Vector Add Example</h1>
<p>使用 vector add 的例子来查看底层 TVM API.</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>n <span class="token operator">=</span> <span class="token number">1024</span></pre></td></tr><tr><td data-num="2"></td><td><pre>A <span class="token operator">=</span> tvm<span class="token punctuation">.</span>te<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'A'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>B <span class="token operator">=</span> tvm<span class="token punctuation">.</span>te<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'B'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>C <span class="token operator">=</span> tvm<span class="token punctuation">.</span>te<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> <span class="token keyword">lambda</span> i<span class="token punctuation">:</span> A<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"C"</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>这里面 A、B、C 都是  <code>tvm.tensor.Tensor</code>   其 Python 定义位于 <code>python/tvm/te/tensor.py</code> . 支撑的 C++ 定义位于 <code>include/tvm/te/tensor.h</code>  和 <code>src/te/tensor.cc</code>  所有的 Python 类型定义都能找到对应的相同名字的 C++ 定义。</p>
<p>Python 对 C++ 的包装位于  <code>python/tvm/_ffi/</code> 。</p>
<p>一个 Tensor 包含一个 Operation 类，定义于 python/tvm/te/tensor.py，对应的 C++ 实现位于 <code>include/tvm/te/operation.h</code>  和 <code>src/tvm/te/operation</code>  。 <code>Tensor</code>  是  <code>Operation</code>  类的输出。</p>
<p>我们将输出张量 C 对应的操作传递给 <code>tvm.te.create_schedule()</code>  函数 （来自于 <code>python/tvm/te/schedule.py</code> 。）</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>s <span class="token operator">=</span> tvm<span class="token punctuation">.</span>te<span class="token punctuation">.</span>create_schedule<span class="token punctuation">(</span>C<span class="token punctuation">.</span>op<span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>这个函数映射到 C++ 函数 <code>include/tvm/schedule.h</code> 。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>inline Schedule create_schedule<span class="token punctuation">(</span>Array<span class="token operator">&lt;</span>Operation<span class="token operator">&gt;</span> ops<span class="token punctuation">)</span> <span class="token punctuation">{</span></pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token keyword">return</span> Schedule<span class="token punctuation">(</span>ops<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token punctuation">}</span></pre></td></tr></tbody></table></figure><p><code>Schedule</code>  包含 <code>Stage</code>  输出  <code>Operation</code>  的集合。</p>
<p><code>Stage</code>  对应于一个操作 <code>Operation</code> 。上面的 vector add 操作中有两个 placeholder ops 和一个 compute op. 所以 <code>Schedule s</code>  有三个状态  <code>Stage</code> ，每个 <code>Stage</code>  持有以下信息： 循环嵌套结构、每个循环的类型（ <code>Parallel，Vectorized，Unrolled</code> ）、以及在下一个循环嵌套 <code>Stage</code>  中在哪里执行它自己的计算。</p>
<p><code>Schedule</code>  和 <code>Stage</code>  本身定义在 <code>tvm/python/te/schedule.py</code> ，  <code>include/tvm/te/schedule.h</code> ， 和 <code>src/te/schedule/schedule_ops.cc</code> 。</p>
<p>为简单起见，我们使用 <code>tvm.build(...)</code>  处理上方 <code>create_schedule()</code>  函数创建的默认 <code>Schedule s</code>  和 &lt;em&gt;。我们必须添加必要的线程绑定，来使得其能在 GPU 上运行：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>target <span class="token operator">=</span> <span class="token string">"cuda"</span></pre></td></tr><tr><td data-num="2"></td><td><pre>bx<span class="token punctuation">,</span> tx <span class="token operator">=</span> s<span class="token punctuation">[</span>C<span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span>C<span class="token punctuation">.</span>op<span class="token punctuation">.</span>axis<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>s<span class="token punctuation">[</span>C<span class="token punctuation">]</span><span class="token punctuation">.</span>bind<span class="token punctuation">(</span>bx<span class="token punctuation">,</span> tvm<span class="token punctuation">.</span>te<span class="token punctuation">.</span>thread_axis<span class="token punctuation">(</span><span class="token string">"blockIdx.x"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>s<span class="token punctuation">[</span>C<span class="token punctuation">]</span><span class="token punctuation">.</span>bind<span class="token punctuation">(</span>tx<span class="token punctuation">,</span> tvm<span class="token punctuation">.</span>te<span class="token punctuation">.</span>thread_axis<span class="token punctuation">(</span><span class="token string">"threadIdx.x"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>fadd <span class="token operator">=</span> tvm<span class="token punctuation">.</span>build<span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token punctuation">[</span>A<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">]</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p><code>tvm.build(...)</code> ，定义在 <code>python/tvm/driver/build_module.py</code> ， 需要输入一个 <code>Schedule</code> ;  <code>input</code> , <code>output Tensor</code> ; 以及一个 <code>target</code> 。返回一个 <code>tvm.runtime.Module</code> 。</p>
<p>整个 <code>tvm.build(...)</code>  过程可以分成两步：</p>
<ul>
<li>
<p>i. 降级 高级的、初始的循环嵌套结构被转换为 最终的、低级的 IR</p>
</li>
<li>
<p>ii. 代码生成 low level IR 生成目标机器码</p>
</li>
</ul>
<p>降级是通过 <code>tvm.lower()</code>  函数完成的，它定义在 <code>python/tvm/build\_module.py</code> 。第一，指定绑定推理，一个最初的循环嵌套结构就创建好了。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">lower</span><span class="token punctuation">(</span>sch<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="2"></td><td><pre>          args<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>          name<span class="token operator">=</span><span class="token string">"default_function"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>          binds<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="5"></td><td><pre>          simple_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="6"></td><td><pre>   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr><tr><td data-num="7"></td><td><pre>   bounds <span class="token operator">=</span> schedule<span class="token punctuation">.</span>InferBound<span class="token punctuation">(</span>sch<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>   stmt <span class="token operator">=</span> schedule<span class="token punctuation">.</span>ScheduleOps<span class="token punctuation">(</span>sch<span class="token punctuation">,</span> bounds<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr></tbody></table></figure><p>边界推断是推断所有循环边界和中间缓冲区大小的过程。如果你的目标是 CUDA，且你用了 share memory，它需要的最小 size 在此处确定。绑定推理时在 <code>src/te/schedule/bound.cc，src/te/schedule/graph.cc </code>  和  <code>src/te/schedule/message\_passing.cc</code>  中实现的。</p>
<p><code>stmt</code> ， <code>ScheduleOps()</code>  的输出，表示一个初识的循环嵌套结构。如果在 schedule 中已经应用了 <code>reorder</code>  和 <code>split</code>  原语，那么初始的循环嵌套结构已经反映了这些变化。 <code>ScheduleOps()</code>  定义在 <code>rc/te/schedule/schedule_ops.cc</code> 。</p>
<p>接下来应用一些 lowering passes to  <code>stmt</code>  . 这些 passes 在 <code>src/tir/pass</code>  子文件夹下实现。举个例子，如果在你的 <code>schedule</code>  中应用了 <code>vectorize</code>  或者 <code>unroll</code>  原语，他们会被应用到循环 vectorization 和 unrolling passes。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr><tr><td data-num="2"></td><td><pre>stmt <span class="token operator">=</span> ir_pass<span class="token punctuation">.</span>VectorizeLoop<span class="token punctuation">(</span>stmt<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr><tr><td data-num="4"></td><td><pre>stmt <span class="token operator">=</span> ir_pass<span class="token punctuation">.</span>UnrollLoop<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    stmt<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    cfg<span class="token punctuation">.</span>auto_unroll_max_step<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    cfg<span class="token punctuation">.</span>auto_unroll_max_depth<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    cfg<span class="token punctuation">.</span>auto_unroll_max_extent<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    cfg<span class="token punctuation">.</span>unroll_explicit<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr></tbody></table></figure><p>在降级 lowering 结束后， <code>build()</code>  函数生成目标机器代码。如果你的设备是 X86, 这个代码可能包含 SSE 或者 AVX 指令；如果是 CUDA 设备，将包含 PTX 指令。 此外，除了目标特定的机器代码之外，TVM 还生成负责内存管理、内核启动等的主机端代码。</p>
<p><code>build\_module()</code>  函数完成代码生成，定义在 <code>python/tvm/target/codegen.py</code> 。在 C++ 端代码生成定义在 <code>src/target/codegen</code> 。 <code>build\_module()</code>  Python 函数会搜索在 <code>src/target/codegen/codegen.cc</code>  中的 <code>build()</code>  函数。</p>
<p><code>build()</code>  函数 <code>PackedFunc</code>  注册表中为目标设备查找代码生成器，并调用找到的函数。例如， <code>codegen.build\_cuda</code>  函数注册在 <code>src/codegen/build_cuda_on.cc</code> ，就像这样：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>TVM_REGISTER_GLOBAL<span class="token punctuation">(</span><span class="token string">"codegen.build_cuda"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token punctuation">.</span>set_body<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">(</span>TVMArgs args<span class="token punctuation">,</span> TVMRetValue<span class="token operator">*</span> rv<span class="token punctuation">)</span> <span class="token punctuation">{</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token operator">*</span>rv <span class="token operator">=</span> BuildCUDA<span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></tbody></table></figure><p>上方的 <code>BuildCUDA()</code>  函数使用定义在 <code>src/codegen/codegen_cuda.cc</code>  的 <code>CodeGenCUDA</code>  类，从 lowered IR 生成 CUDA kernel source，并使用 NVRTC 编译 kernel。如果你的目标设备使用 LLVM，包括 X86、ARM、NVPTX 和 AMDGPU，代码可由定义在 <code>src/codegen/llvm/codegen_llvm.cc</code>  的 <code>CodeGenLLVM</code>  来生成。 <code>CodeGenLLVM</code>  将 TVM IR 转换成 LLVM IR，运行一些 LLVM 优化 passes，以及生成目标机器码。</p>
<p>在 <code>src/codegen/codegen.cc</code>  中的 <code>Build()</code>  函数会返回一个 <code>runtime::Module</code>  类，它定义在 <code>include/tvm/runtime/module.h</code>  和 <code>src/runtime/module.cc</code> 。一个 <code>Module</code>  类是一个潜在目标 设备的特定 <code>ModuleNode</code>  的容器。</p>
<p>每个后端都实现一个 <code>ModuleNode</code>  的子类，来添加目标特定的 runtime API 调用。 例如，CUDA 后端在 <code>src/runtime/cuda/cuda_module.cc</code>  实现 <code>CUDAModuleNode</code>  类，来管理 CUDA 驱动 API。上方的 <code>BuildCUDA()</code>  函数用 <code>runtime::Module</code>  包装了 <code>CUDAModuleNode</code> ，并包装到 Python 端。LLVM 后端在 <code>src/codegen/llvm/llvm_module.cc</code>  实现了 <code>LLVMModuleNode</code> ，处理了 JIT 执行和编译代码。其他对应各个后端的 <code>ModuleNode</code>  子类可以在 <code>src/runtime</code>  子文件夹找到。<br>
返回的 <code>module</code> ，可以被认作编译函数和设备 API 的组合，可以被 TVM 的 NDArray objects 调用。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>dev <span class="token operator">=</span> tvm<span class="token punctuation">.</span>device<span class="token punctuation">(</span>target<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>a <span class="token operator">=</span> tvm<span class="token punctuation">.</span>nd<span class="token punctuation">.</span>array<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>size<span class="token operator">=</span>n<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>A<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> dev<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>b <span class="token operator">=</span> tvm<span class="token punctuation">.</span>nd<span class="token punctuation">.</span>array<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>size<span class="token operator">=</span>n<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>B<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> dev<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>c <span class="token operator">=</span> tvm<span class="token punctuation">.</span>nd<span class="token punctuation">.</span>array<span class="token punctuation">(</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n<span class="token punctuation">,</span> dtype<span class="token operator">=</span>C<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> dev<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>fadd<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>output <span class="token operator">=</span> c<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>在幕后，TVM 会自动分配设备内存并管理内存传输。为了实现这个目标，每个后端都需要继承在 <code>include/tvm/runtime/device_api.h</code>  定义的 <code>DeviceAPI</code>  类，使用设备特定的 API 重写里面的内存管理方法。例如，CUDA 后端在 <code>src/runtime/cuda/cuda_device_api.cc</code>  使用 <code>cudaMalloc</code> 、 <code>cudaMemcpy</code>  实现了 <code>CUDADeviceAPI</code> .</p>
<p>第一次使用 <code>fadd(a, b, c)</code>  调用编译后的模块时，会调用  <code>ModuleNode</code>  的  <code>GetFunction()</code>  方法来获取可用于内核调用的  <code>PackedFunc</code> 。例如，在 <code>src/runtime/cuda/cuda_module.cc</code>  CUDA 后端实现了 <code>CUDAModuleNode::GetFunction()</code>  函数如下：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>PackedFunc CUDAModuleNode<span class="token punctuation">:</span><span class="token punctuation">:</span>GetFunction<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="2"></td><td><pre>      const std<span class="token punctuation">:</span><span class="token punctuation">:</span>string<span class="token operator">&amp;</span> name<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>      const std<span class="token punctuation">:</span><span class="token punctuation">:</span>shared_ptr<span class="token operator">&lt;</span>ModuleNode<span class="token operator">&gt;</span><span class="token operator">&amp;</span> sptr_to_self<span class="token punctuation">)</span> <span class="token punctuation">{</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  auto it <span class="token operator">=</span> fmap_<span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="5"></td><td><pre>  const FunctionInfo<span class="token operator">&amp;</span> info <span class="token operator">=</span> it<span class="token operator">-</span><span class="token operator">&gt;</span>second<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="6"></td><td><pre>  CUDAWrappedFunc f<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>  f<span class="token punctuation">.</span>Init<span class="token punctuation">(</span>this<span class="token punctuation">,</span> sptr_to_self<span class="token punctuation">,</span> name<span class="token punctuation">,</span> info<span class="token punctuation">.</span>arg_types<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> info<span class="token punctuation">.</span>launch_param_tags<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="8"></td><td><pre>  <span class="token keyword">return</span> PackFuncVoidAddr<span class="token punctuation">(</span>f<span class="token punctuation">,</span> info<span class="token punctuation">.</span>arg_types<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token punctuation">}</span></pre></td></tr></tbody></table></figure><p><code>PackedFunc</code>  的重载函数 <code>operator()</code>  会被调用。从而会调用定义在 <code>src/runtime/cuda/cuda_module.cc</code>  的 <code>CUDAWrappedFunc</code>  的 <code>operator()</code>  函数，最终我们会看到 <code>cuLaunchKernel</code>  驱动会调用：</p>
<figure class="highlight cpp"><figcaption data-lang="C++"><span>p</span></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">CUDAWrappedFunc</span> <span class="token punctuation">{</span></pre></td></tr><tr><td data-num="2"></td><td><pre> <span class="token keyword">public</span><span class="token operator">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  <span class="token keyword">void</span> <span class="token function">Init</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr><tr><td data-num="5"></td><td><pre>  <span class="token keyword">void</span> <span class="token keyword">operator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>TVMArgs args<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="6"></td><td><pre>                  TVMRetValue<span class="token operator">*</span> rv<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="7"></td><td><pre>                  <span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span> void_args<span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">{</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token keyword">int</span> device_id<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token function">CUDA_CALL</span><span class="token punctuation">(</span><span class="token function">cudaGetDevice</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>device_id<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token keyword">if</span> <span class="token punctuation">(</span>fcache_<span class="token punctuation">[</span>device_id<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token keyword">nullptr</span><span class="token punctuation">)</span> <span class="token punctuation">{</span></pre></td></tr><tr><td data-num="11"></td><td><pre>      fcache_<span class="token punctuation">[</span>device_id<span class="token punctuation">]</span> <span class="token operator">=</span> m_<span class="token operator">-&gt;</span><span class="token function">GetFunc</span><span class="token punctuation">(</span>device_id<span class="token punctuation">,</span> func_name_<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    CUstream strm <span class="token operator">=</span> <span class="token generic-function"><span class="token function">static_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span>CUstream<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token class-name">CUDAThreadEntry</span><span class="token double-colon punctuation">::</span><span class="token function">ThreadLocal</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-&gt;</span>stream<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    ThreadWorkLoad wl <span class="token operator">=</span> launch_param_config_<span class="token punctuation">.</span><span class="token function">Extract</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    CUresult result <span class="token operator">=</span> <span class="token function">cuLaunchKernel</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        fcache_<span class="token punctuation">[</span>device_id<span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        wl<span class="token punctuation">.</span><span class="token function">grid_dim</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="18"></td><td><pre>        wl<span class="token punctuation">.</span><span class="token function">grid_dim</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        wl<span class="token punctuation">.</span><span class="token function">grid_dim</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="20"></td><td><pre>        wl<span class="token punctuation">.</span><span class="token function">block_dim</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="21"></td><td><pre>        wl<span class="token punctuation">.</span><span class="token function">block_dim</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="22"></td><td><pre>        wl<span class="token punctuation">.</span><span class="token function">block_dim</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="23"></td><td><pre>        <span class="token number">0</span><span class="token punctuation">,</span> strm<span class="token punctuation">,</span> void_args<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="24"></td><td><pre>  <span class="token punctuation">}</span></pre></td></tr><tr><td data-num="25"></td><td><pre><span class="token punctuation">}</span><span class="token punctuation">;</span></pre></td></tr></tbody></table></figure><p>本文概括了 TVM 如何编译和执行函数。 虽然本文没有详细说明 TOPI 或 Relay，但最终所有神经网络算子都会经历与上述相同的编译过程。</p>
<h1 id="后记"><a class="anchor" href="#后记">#</a> 后记</h1>
<p>本博客目前以及可预期的将来都不会支持评论功能。各位大侠如若有指教和问题，可以在我的 <a target="_blank" rel="noopener" href="https://github.com/ForCheetah/ForCheetah.github.io">github 项目</a> 或随便一个项目下提出 issue，或者<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/guai-dao-ji-de-3-50">知乎</a> 私信，并指明哪一篇博客，我看到一定及时回复，感激不尽！</p>
<div class="tags"><a href="/tags/accelerate/" rel="tag"><i class="ic i-tag"></i>accelerate</a><a href="/tags/conv/" rel="tag"><i class="ic i-tag"></i>conv</a><a href="/tags/tvm/" rel="tag"><i class="ic i-tag"></i>tvm</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">Edited on</span><time title="Modified: 2024-05-29 21:04:02" itemprop="dateModified" datetime="2024-05-29T21:04:02+08:00">2024-05-29</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.webp" alt="XianMu WeChat Pay"><p>WeChat Pay</p></div><div><img loading="lazy" data-src="/assets/alipay.webp" alt="XianMu Alipay"><p>Alipay</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>XianMu<i class="ic i-at"><em>@</em></i>Пусть этот камень будет более крепким, чем человек</li><li class="link"><strong>Post link: </strong><a href="https://forcheetah.github.io/2024/05/24/tvm1/" title="【TVM】根据例子走通代码库">https://forcheetah.github.io/2024/05/24/tvm1/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/05/24/category/" rel="prev" itemprop="url" data-background-image="https://forcheetah.github.io/assets/danger.webp" title="博客汇总目录"><span class="type">Previous Post</span><span class="category"><i class="ic i-flag"></i>目录</span><h3>博客汇总目录</h3></a></div><div class="item right"><a href="/2024/05/26/zatan3D/" rel="next" itemprop="url" data-background-image="https://forcheetah.github.io/assets/lunbo7.webp" title="【3D建模】IS-7攻城锤流纹岩皮肤展示"><span class="type">Next Post</span><span class="category"><i class="ic i-flag"></i>杂谈</span><h3>【3D建模】IS-7攻城锤流纹岩皮肤展示</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text"> 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#codebase-structure-overview"><span class="toc-number">2.</span> <span class="toc-text"> Codebase Structure Overview</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#vector-add-example"><span class="toc-number">3.</span> <span class="toc-text"> Vector Add Example</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%8E%E8%AE%B0"><span class="toc-number">4.</span> <span class="toc-text"> 后记</span></a></li></ol></div><div class="related panel pjax" data-title="Related"><ul><li class="active"><a href="/2024/05/24/tvm1/" rel="bookmark" title="【TVM】根据例子走通代码库">【TVM】根据例子走通代码库</a></li><li><a href="/2024/06/10/deployTVM/" rel="bookmark" title="【TVM】C++部署运行TVM">【TVM】C++部署运行TVM</a></li><li><a href="/2024/06/18/deployTVMPython/" rel="bookmark" title="【TVM】Python脚本实现模型编译和保存">【TVM】Python脚本实现模型编译和保存</a></li><li><a href="/2024/10/10/tvm01/" rel="bookmark" title="【TVM】通过代码学习编译流程【1】必要知识">【TVM】通过代码学习编译流程【1】必要知识</a></li><li><a href="/2024/10/13/tvm02/" rel="bookmark" title="【TVM】通过代码学习编译流程【2】模型转换">【TVM】通过代码学习编译流程【2】模型转换</a></li><li><a href="/2024/10/17/tvm03/" rel="bookmark" title="【TVM】通过代码学习编译流程【3】模型编译">【TVM】通过代码学习编译流程【3】模型编译</a></li><li><a href="/2024/10/21/tvm04/" rel="bookmark" title="【TVM】通过代码学习编译流程【4】BuildRelay">【TVM】通过代码学习编译流程【4】BuildRelay</a></li><li><a href="/2024/10/22/tvm3.5/" rel="bookmark" title="【TVM】通过代码学习类【3.5】Pass">【TVM】通过代码学习类【3.5】Pass</a></li><li><a href="/2024/10/25/tvm06/" rel="bookmark" title="【TVM】通过代码学习编译流程【6】CodeGen">【TVM】通过代码学习编译流程【6】CodeGen</a></li><li><a href="/2024/10/31/tvm05/" rel="bookmark" title="【TVM】通过代码学习编译流程【5】FuseOps">【TVM】通过代码学习编译流程【5】FuseOps</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="XianMu" src="/assets/avatar.webp"><p class="name" itemprop="name">XianMu</p><div class="description" itemprop="description">有自己的博客很帅，但是我很懒，要命！！！</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">40</span><span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">16</span><span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">21</span><span class="name">tags</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/ForCheetah" class="item github" title="https://github.com/ForCheetah"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/guai-dao-ji-de-3-50" class="item zhihu" title="https://www.zhihu.com/people/guai-dao-ji-de-3-50"><i class="ic i-zhihu"></i></a><a href="/huasen.w@foxmail.com" class="item email" title="huasen.w@foxmail.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2024/05/26/zatan3D/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/05/24/category/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/tvm/" title="Intvm">tvm</a></div><span><a href="/2024/10/17/tvm03/">【TVM】通过代码学习编译流程【3】模型编译</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/" title="In编译器">编译器</a></div><span><a href="/2025/03/13/compile01/">【编译器】使用llvm编译自定义语言【1】构建AST</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" title="In计算机科学">计算机科学</a><i class="ic i-angle-right"></i><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/Linux/" title="InLinux">Linux</a></div><span><a href="/2024/05/12/test/">foo</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/05/23/conv1/">【Im2Col】卷积加速算法【1】 NCHW</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/07/07/conv4/">【Winograd】卷积加速算法原理及实现</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/12/20/conv5/">【gemm】Gemm计算加速</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%9D%82%E8%B0%88/" title="In杂谈">杂谈</a></div><span><a href="/2025/01/21/zatanNoval2/">【感想】写作进度报告2</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F/" title="In卷积加速">卷积加速</a></div><span><a href="/2024/06/27/conv3/">【im2col】昇腾卷积加速算法</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%BC%96%E8%AF%91%E5%99%A8/" title="In编译器">编译器</a></div><span><a href="/2025/03/20/compile02/">【编译器】使用llvm编译自定义语言【2】转llvm IR</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%9D%82%E8%B0%88/" title="In杂谈">杂谈</a></div><span><a href="/2024/05/26/zatan3D/">【3D建模】IS-7攻城锤流纹岩皮肤展示</a></span></li></ul></div><div class="rpost pjax"><h2>Recent Comments</h2></div></div><div class="status"><div class="copyright">© 2010 -<span itemprop="copyrightYear">2025</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">XianMu @ 暮冬Z羡慕的博客</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="Symbols count total">276k words</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="Reading time total">4:11</span></div><div class="powered-by">Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> &amp; Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config="" type="text/javascript">var LOCAL = {
    ispost: true,
        path: `2024/05/24/tvm1/`,
        favicon: {
        show: `(●´3｀●) Here we go again.`,
        hide: `(´Д｀) It's a disaster!`
    },
    search: {
        placeholder: "Search for Posts",
        empty: "We didn't find any results for the search: ${query}",
        stats: "${hits} results found in ${time} ms"
    },
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: {},
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">Article Timeliness Alert</span><br>This is an article published {{publish}} days ago and last updated {{updated}} days ago. Some information may have changed, so please be careful to screen it.</p></div>`,
    quiz: {
        choice: `Multiple Choice`,
        multiple: `Multiple Answer`,
        true_false: `True/False`,
        essay: `Questions`,
        gap_fill: `Gap Filling`,
        mistake: `Wrong Answer`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};
</script><script src="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-6-M/pace/1.2.4/pace.min.js" async=""></script><script src="/js/siteInit.js?v=0.4.2" type="module" fetchpriority="high" defer=""></script></body></html>